2022-12-29 09:55:25,991 - Log file for this run: /home/lyl/MAX78000/ai8x-training/logs/2022.12.29-095525/2022.12.29-095525.log
2022-12-29 09:55:27,880 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-12-29 09:55:27,880 - Optimizer Args: {'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False}
2022-12-29 09:55:27,895 - Dataset sizes:
	training=1800
	validation=200
	test=527
2022-12-29 09:55:27,896 - Reading compression schedule from: policies/schedule-garbage.yaml
2022-12-29 09:55:27,898 - 

2022-12-29 09:55:27,899 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:55:28,973 - Epoch: [0][   10/  113]    Overall Loss 1.778598    Objective Loss 1.778598                                        LR 0.000100    Time 0.107389    
2022-12-29 09:55:29,468 - Epoch: [0][   20/  113]    Overall Loss 1.772520    Objective Loss 1.772520                                        LR 0.000100    Time 0.078403    
2022-12-29 09:55:29,968 - Epoch: [0][   30/  113]    Overall Loss 1.769141    Objective Loss 1.769141                                        LR 0.000100    Time 0.068923    
2022-12-29 09:55:30,465 - Epoch: [0][   40/  113]    Overall Loss 1.763171    Objective Loss 1.763171                                        LR 0.000100    Time 0.064104    
2022-12-29 09:55:30,969 - Epoch: [0][   50/  113]    Overall Loss 1.759435    Objective Loss 1.759435                                        LR 0.000100    Time 0.061347    
2022-12-29 09:55:31,466 - Epoch: [0][   60/  113]    Overall Loss 1.755280    Objective Loss 1.755280                                        LR 0.000100    Time 0.059401    
2022-12-29 09:55:31,968 - Epoch: [0][   70/  113]    Overall Loss 1.756265    Objective Loss 1.756265                                        LR 0.000100    Time 0.058082    
2022-12-29 09:55:32,468 - Epoch: [0][   80/  113]    Overall Loss 1.754161    Objective Loss 1.754161                                        LR 0.000100    Time 0.057066    
2022-12-29 09:55:32,969 - Epoch: [0][   90/  113]    Overall Loss 1.753225    Objective Loss 1.753225                                        LR 0.000100    Time 0.056280    
2022-12-29 09:55:33,468 - Epoch: [0][  100/  113]    Overall Loss 1.748320    Objective Loss 1.748320                                        LR 0.000100    Time 0.055643    
2022-12-29 09:55:33,975 - Epoch: [0][  110/  113]    Overall Loss 1.747023    Objective Loss 1.747023                                        LR 0.000100    Time 0.055189    
2022-12-29 09:55:34,118 - Epoch: [0][  113/  113]    Overall Loss 1.746261    Objective Loss 1.746261    Top1 20.833333    Top5 91.666667    LR 0.000100    Time 0.054986    
2022-12-29 09:55:34,167 - --- validate (epoch=0)-----------
2022-12-29 09:55:34,167 - 200 samples (16 per mini-batch)
2022-12-29 09:55:34,647 - Epoch: [0][   10/   13]    Loss 1.691090    Top1 25.625000    Top5 96.250000    
2022-12-29 09:55:34,701 - Epoch: [0][   13/   13]    Loss 1.696931    Top1 26.500000    Top5 96.000000    
2022-12-29 09:55:34,759 - ==> Top1: 26.500    Top5: 96.000    Loss: 1.697

2022-12-29 09:55:34,759 - ==> Confusion:
[[ 0  0  0 29  0  0]
 [ 0  0  0 39  0  0]
 [ 0  0  0 32  0  0]
 [ 0  0  0 53  0  0]
 [ 0  0  0 39  0  0]
 [ 0  0  0  8  0  0]]

2022-12-29 09:55:34,762 - ==> Best [Top1: 26.500   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 0]
2022-12-29 09:55:34,763 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:55:34,777 - 

2022-12-29 09:55:34,777 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:55:35,471 - Epoch: [1][   10/  113]    Overall Loss 1.714092    Objective Loss 1.714092                                        LR 0.000100    Time 0.069294    
2022-12-29 09:55:35,973 - Epoch: [1][   20/  113]    Overall Loss 1.716368    Objective Loss 1.716368                                        LR 0.000100    Time 0.059708    
2022-12-29 09:55:36,473 - Epoch: [1][   30/  113]    Overall Loss 1.729139    Objective Loss 1.729139                                        LR 0.000100    Time 0.056483    
2022-12-29 09:55:36,974 - Epoch: [1][   40/  113]    Overall Loss 1.718002    Objective Loss 1.718002                                        LR 0.000100    Time 0.054851    
2022-12-29 09:55:37,471 - Epoch: [1][   50/  113]    Overall Loss 1.720428    Objective Loss 1.720428                                        LR 0.000100    Time 0.053820    
2022-12-29 09:55:37,971 - Epoch: [1][   60/  113]    Overall Loss 1.726994    Objective Loss 1.726994                                        LR 0.000100    Time 0.053179    
2022-12-29 09:55:38,474 - Epoch: [1][   70/  113]    Overall Loss 1.724557    Objective Loss 1.724557                                        LR 0.000100    Time 0.052756    
2022-12-29 09:55:38,975 - Epoch: [1][   80/  113]    Overall Loss 1.721380    Objective Loss 1.721380                                        LR 0.000100    Time 0.052416    
2022-12-29 09:55:39,476 - Epoch: [1][   90/  113]    Overall Loss 1.721409    Objective Loss 1.721409                                        LR 0.000100    Time 0.052162    
2022-12-29 09:55:39,977 - Epoch: [1][  100/  113]    Overall Loss 1.719151    Objective Loss 1.719151                                        LR 0.000100    Time 0.051942    
2022-12-29 09:55:40,485 - Epoch: [1][  110/  113]    Overall Loss 1.720323    Objective Loss 1.720323                                        LR 0.000100    Time 0.051834    
2022-12-29 09:55:40,632 - Epoch: [1][  113/  113]    Overall Loss 1.719886    Objective Loss 1.719886    Top1 16.666667    Top5 100.000000    LR 0.000100    Time 0.051752    
2022-12-29 09:55:40,694 - --- validate (epoch=1)-----------
2022-12-29 09:55:40,695 - 200 samples (16 per mini-batch)
2022-12-29 09:55:41,163 - Epoch: [1][   10/   13]    Loss 1.699222    Top1 23.750000    Top5 95.000000    
2022-12-29 09:55:41,217 - Epoch: [1][   13/   13]    Loss 1.725026    Top1 24.500000    Top5 93.000000    
2022-12-29 09:55:41,271 - ==> Top1: 24.500    Top5: 93.000    Loss: 1.725

2022-12-29 09:55:41,272 - ==> Confusion:
[[ 0 16  0 14  0  0]
 [ 0 34  0 12  0  0]
 [ 0 16  0 15  0  0]
 [ 0 36  0 15  0  0]
 [ 0 20  0  8  0  0]
 [ 0  8  0  6  0  0]]

2022-12-29 09:55:41,274 - ==> Best [Top1: 26.500   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 0]
2022-12-29 09:55:41,274 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:55:41,296 - 

2022-12-29 09:55:41,297 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:55:41,981 - Epoch: [2][   10/  113]    Overall Loss 1.680552    Objective Loss 1.680552                                        LR 0.000100    Time 0.068308    
2022-12-29 09:55:42,477 - Epoch: [2][   20/  113]    Overall Loss 1.713324    Objective Loss 1.713324                                        LR 0.000100    Time 0.058950    
2022-12-29 09:55:42,976 - Epoch: [2][   30/  113]    Overall Loss 1.720871    Objective Loss 1.720871                                        LR 0.000100    Time 0.055928    
2022-12-29 09:55:43,478 - Epoch: [2][   40/  113]    Overall Loss 1.728519    Objective Loss 1.728519                                        LR 0.000100    Time 0.054467    
2022-12-29 09:55:43,977 - Epoch: [2][   50/  113]    Overall Loss 1.727298    Objective Loss 1.727298                                        LR 0.000100    Time 0.053556    
2022-12-29 09:55:44,479 - Epoch: [2][   60/  113]    Overall Loss 1.727398    Objective Loss 1.727398                                        LR 0.000100    Time 0.052981    
2022-12-29 09:55:44,978 - Epoch: [2][   70/  113]    Overall Loss 1.723799    Objective Loss 1.723799                                        LR 0.000100    Time 0.052535    
2022-12-29 09:55:45,479 - Epoch: [2][   80/  113]    Overall Loss 1.720213    Objective Loss 1.720213                                        LR 0.000100    Time 0.052219    
2022-12-29 09:55:45,979 - Epoch: [2][   90/  113]    Overall Loss 1.718872    Objective Loss 1.718872                                        LR 0.000100    Time 0.051975    
2022-12-29 09:55:46,483 - Epoch: [2][  100/  113]    Overall Loss 1.720235    Objective Loss 1.720235                                        LR 0.000100    Time 0.051814    
2022-12-29 09:55:46,991 - Epoch: [2][  110/  113]    Overall Loss 1.720362    Objective Loss 1.720362                                        LR 0.000100    Time 0.051709    
2022-12-29 09:55:47,134 - Epoch: [2][  113/  113]    Overall Loss 1.719145    Objective Loss 1.719145    Top1 16.666667    Top5 91.666667    LR 0.000100    Time 0.051603    
2022-12-29 09:55:47,187 - --- validate (epoch=2)-----------
2022-12-29 09:55:47,187 - 200 samples (16 per mini-batch)
2022-12-29 09:55:47,662 - Epoch: [2][   10/   13]    Loss 1.725109    Top1 24.375000    Top5 94.375000    
2022-12-29 09:55:47,716 - Epoch: [2][   13/   13]    Loss 1.717212    Top1 24.500000    Top5 95.500000    
2022-12-29 09:55:47,776 - ==> Top1: 24.500    Top5: 95.500    Loss: 1.717

2022-12-29 09:55:47,776 - ==> Confusion:
[[ 0  9  0 10 16  0]
 [ 0 14  0  2 20  0]
 [ 0 13  0  9  9  0]
 [ 0 13  0 10 22  0]
 [ 0 15  0  4 25  0]
 [ 0  2  0  2  5  0]]

2022-12-29 09:55:47,778 - ==> Best [Top1: 26.500   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 0]
2022-12-29 09:55:47,778 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:55:47,797 - 

2022-12-29 09:55:47,797 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:55:48,485 - Epoch: [3][   10/  113]    Overall Loss 1.702703    Objective Loss 1.702703                                        LR 0.000100    Time 0.068730    
2022-12-29 09:55:48,987 - Epoch: [3][   20/  113]    Overall Loss 1.696694    Objective Loss 1.696694                                        LR 0.000100    Time 0.059431    
2022-12-29 09:55:49,489 - Epoch: [3][   30/  113]    Overall Loss 1.702647    Objective Loss 1.702647                                        LR 0.000100    Time 0.056345    
2022-12-29 09:55:49,992 - Epoch: [3][   40/  113]    Overall Loss 1.706783    Objective Loss 1.706783                                        LR 0.000100    Time 0.054817    
2022-12-29 09:55:50,491 - Epoch: [3][   50/  113]    Overall Loss 1.708319    Objective Loss 1.708319                                        LR 0.000100    Time 0.053829    
2022-12-29 09:55:50,997 - Epoch: [3][   60/  113]    Overall Loss 1.705365    Objective Loss 1.705365                                        LR 0.000100    Time 0.053279    
2022-12-29 09:55:51,501 - Epoch: [3][   70/  113]    Overall Loss 1.704739    Objective Loss 1.704739                                        LR 0.000100    Time 0.052861    
2022-12-29 09:55:52,003 - Epoch: [3][   80/  113]    Overall Loss 1.705488    Objective Loss 1.705488                                        LR 0.000100    Time 0.052519    
2022-12-29 09:55:52,505 - Epoch: [3][   90/  113]    Overall Loss 1.708911    Objective Loss 1.708911                                        LR 0.000100    Time 0.052265    
2022-12-29 09:55:53,005 - Epoch: [3][  100/  113]    Overall Loss 1.709889    Objective Loss 1.709889                                        LR 0.000100    Time 0.052029    
2022-12-29 09:55:53,509 - Epoch: [3][  110/  113]    Overall Loss 1.707144    Objective Loss 1.707144                                        LR 0.000100    Time 0.051880    
2022-12-29 09:55:53,653 - Epoch: [3][  113/  113]    Overall Loss 1.706636    Objective Loss 1.706636    Top1 29.166667    Top5 100.000000    LR 0.000100    Time 0.051775    
2022-12-29 09:55:53,696 - --- validate (epoch=3)-----------
2022-12-29 09:55:53,697 - 200 samples (16 per mini-batch)
2022-12-29 09:55:54,167 - Epoch: [3][   10/   13]    Loss 1.730518    Top1 20.625000    Top5 95.625000    
2022-12-29 09:55:54,222 - Epoch: [3][   13/   13]    Loss 1.735425    Top1 20.000000    Top5 95.500000    
2022-12-29 09:55:54,271 - ==> Top1: 20.000    Top5: 95.500    Loss: 1.735

2022-12-29 09:55:54,272 - ==> Confusion:
[[ 0  0  0 42  0  0]
 [ 0  0  0 35  0  0]
 [ 0  0  0 26  0  0]
 [ 0  0  0 40  0  0]
 [ 0  0  0 48  0  0]
 [ 0  0  0  9  0  0]]

2022-12-29 09:55:54,273 - ==> Best [Top1: 26.500   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 0]
2022-12-29 09:55:54,273 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:55:54,287 - 

2022-12-29 09:55:54,287 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:55:54,978 - Epoch: [4][   10/  113]    Overall Loss 1.724378    Objective Loss 1.724378                                        LR 0.000100    Time 0.069048    
2022-12-29 09:55:55,485 - Epoch: [4][   20/  113]    Overall Loss 1.707964    Objective Loss 1.707964                                        LR 0.000100    Time 0.059801    
2022-12-29 09:55:55,985 - Epoch: [4][   30/  113]    Overall Loss 1.714720    Objective Loss 1.714720                                        LR 0.000100    Time 0.056535    
2022-12-29 09:55:56,488 - Epoch: [4][   40/  113]    Overall Loss 1.710490    Objective Loss 1.710490                                        LR 0.000100    Time 0.054968    
2022-12-29 09:55:56,990 - Epoch: [4][   50/  113]    Overall Loss 1.702571    Objective Loss 1.702571                                        LR 0.000100    Time 0.053993    
2022-12-29 09:55:57,493 - Epoch: [4][   60/  113]    Overall Loss 1.704918    Objective Loss 1.704918                                        LR 0.000100    Time 0.053372    
2022-12-29 09:55:57,992 - Epoch: [4][   70/  113]    Overall Loss 1.700452    Objective Loss 1.700452                                        LR 0.000100    Time 0.052875    
2022-12-29 09:55:58,494 - Epoch: [4][   80/  113]    Overall Loss 1.704184    Objective Loss 1.704184                                        LR 0.000100    Time 0.052528    
2022-12-29 09:55:58,993 - Epoch: [4][   90/  113]    Overall Loss 1.705529    Objective Loss 1.705529                                        LR 0.000100    Time 0.052229    
2022-12-29 09:55:59,494 - Epoch: [4][  100/  113]    Overall Loss 1.705578    Objective Loss 1.705578                                        LR 0.000100    Time 0.052015    
2022-12-29 09:56:00,002 - Epoch: [4][  110/  113]    Overall Loss 1.704025    Objective Loss 1.704025                                        LR 0.000100    Time 0.051897    
2022-12-29 09:56:00,148 - Epoch: [4][  113/  113]    Overall Loss 1.704748    Objective Loss 1.704748    Top1 16.666667    Top5 91.666667    LR 0.000100    Time 0.051806    
2022-12-29 09:56:00,192 - --- validate (epoch=4)-----------
2022-12-29 09:56:00,192 - 200 samples (16 per mini-batch)
2022-12-29 09:56:00,679 - Epoch: [4][   10/   13]    Loss 1.669199    Top1 30.625000    Top5 96.250000    
2022-12-29 09:56:00,737 - Epoch: [4][   13/   13]    Loss 1.671627    Top1 29.000000    Top5 96.000000    
2022-12-29 09:56:00,798 - ==> Top1: 29.000    Top5: 96.000    Loss: 1.672

2022-12-29 09:56:00,799 - ==> Confusion:
[[ 0  0  0 33  0  0]
 [ 0  0  0 33  0  0]
 [ 0  0  0 33  0  0]
 [ 0  0  0 58  0  0]
 [ 0  0  0 35  0  0]
 [ 0  0  0  8  0  0]]

2022-12-29 09:56:00,801 - ==> Best [Top1: 29.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 4]
2022-12-29 09:56:00,801 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:56:00,826 - 

2022-12-29 09:56:00,826 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:01,520 - Epoch: [5][   10/  113]    Overall Loss 1.700110    Objective Loss 1.700110                                        LR 0.000100    Time 0.069184    
2022-12-29 09:56:02,022 - Epoch: [5][   20/  113]    Overall Loss 1.699002    Objective Loss 1.699002                                        LR 0.000100    Time 0.059694    
2022-12-29 09:56:02,525 - Epoch: [5][   30/  113]    Overall Loss 1.689368    Objective Loss 1.689368                                        LR 0.000100    Time 0.056552    
2022-12-29 09:56:03,030 - Epoch: [5][   40/  113]    Overall Loss 1.692003    Objective Loss 1.692003                                        LR 0.000100    Time 0.055019    
2022-12-29 09:56:03,534 - Epoch: [5][   50/  113]    Overall Loss 1.703629    Objective Loss 1.703629                                        LR 0.000100    Time 0.054071    
2022-12-29 09:56:04,040 - Epoch: [5][   60/  113]    Overall Loss 1.712719    Objective Loss 1.712719                                        LR 0.000100    Time 0.053480    
2022-12-29 09:56:04,544 - Epoch: [5][   70/  113]    Overall Loss 1.709516    Objective Loss 1.709516                                        LR 0.000100    Time 0.053039    
2022-12-29 09:56:05,052 - Epoch: [5][   80/  113]    Overall Loss 1.705424    Objective Loss 1.705424                                        LR 0.000100    Time 0.052752    
2022-12-29 09:56:05,553 - Epoch: [5][   90/  113]    Overall Loss 1.710426    Objective Loss 1.710426                                        LR 0.000100    Time 0.052452    
2022-12-29 09:56:06,055 - Epoch: [5][  100/  113]    Overall Loss 1.712534    Objective Loss 1.712534                                        LR 0.000100    Time 0.052223    
2022-12-29 09:56:06,562 - Epoch: [5][  110/  113]    Overall Loss 1.712065    Objective Loss 1.712065                                        LR 0.000100    Time 0.052080    
2022-12-29 09:56:06,704 - Epoch: [5][  113/  113]    Overall Loss 1.712801    Objective Loss 1.712801    Top1 16.666667    Top5 87.500000    LR 0.000100    Time 0.051953    
2022-12-29 09:56:06,762 - --- validate (epoch=5)-----------
2022-12-29 09:56:06,763 - 200 samples (16 per mini-batch)
2022-12-29 09:56:07,247 - Epoch: [5][   10/   13]    Loss 1.726695    Top1 20.000000    Top5 93.750000    
2022-12-29 09:56:07,301 - Epoch: [5][   13/   13]    Loss 1.733756    Top1 19.000000    Top5 93.500000    
2022-12-29 09:56:07,343 - ==> Top1: 19.000    Top5: 93.500    Loss: 1.734

2022-12-29 09:56:07,344 - ==> Confusion:
[[ 0  0  0 35  0  0]
 [ 0  0  0 44  0  0]
 [ 0  0  0 36  0  0]
 [ 0  0  0 38  0  0]
 [ 0  0  0 34  0  0]
 [ 0  0  0 13  0  0]]

2022-12-29 09:56:07,346 - ==> Best [Top1: 29.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 4]
2022-12-29 09:56:07,346 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:56:07,364 - 

2022-12-29 09:56:07,365 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:08,060 - Epoch: [6][   10/  113]    Overall Loss 1.669841    Objective Loss 1.669841                                        LR 0.000100    Time 0.069331    
2022-12-29 09:56:08,563 - Epoch: [6][   20/  113]    Overall Loss 1.680511    Objective Loss 1.680511                                        LR 0.000100    Time 0.059779    
2022-12-29 09:56:09,064 - Epoch: [6][   30/  113]    Overall Loss 1.684413    Objective Loss 1.684413                                        LR 0.000100    Time 0.056541    
2022-12-29 09:56:09,568 - Epoch: [6][   40/  113]    Overall Loss 1.691776    Objective Loss 1.691776                                        LR 0.000100    Time 0.054997    
2022-12-29 09:56:10,068 - Epoch: [6][   50/  113]    Overall Loss 1.695767    Objective Loss 1.695767                                        LR 0.000100    Time 0.053995    
2022-12-29 09:56:10,573 - Epoch: [6][   60/  113]    Overall Loss 1.702043    Objective Loss 1.702043                                        LR 0.000100    Time 0.053410    
2022-12-29 09:56:11,079 - Epoch: [6][   70/  113]    Overall Loss 1.697535    Objective Loss 1.697535                                        LR 0.000100    Time 0.053000    
2022-12-29 09:56:11,582 - Epoch: [6][   80/  113]    Overall Loss 1.704960    Objective Loss 1.704960                                        LR 0.000100    Time 0.052651    
2022-12-29 09:56:12,083 - Epoch: [6][   90/  113]    Overall Loss 1.703894    Objective Loss 1.703894                                        LR 0.000100    Time 0.052369    
2022-12-29 09:56:12,585 - Epoch: [6][  100/  113]    Overall Loss 1.707562    Objective Loss 1.707562                                        LR 0.000100    Time 0.052147    
2022-12-29 09:56:13,092 - Epoch: [6][  110/  113]    Overall Loss 1.709434    Objective Loss 1.709434                                        LR 0.000100    Time 0.052006    
2022-12-29 09:56:13,233 - Epoch: [6][  113/  113]    Overall Loss 1.711420    Objective Loss 1.711420    Top1 25.000000    Top5 83.333333    LR 0.000100    Time 0.051869    
2022-12-29 09:56:13,280 - --- validate (epoch=6)-----------
2022-12-29 09:56:13,280 - 200 samples (16 per mini-batch)
2022-12-29 09:56:13,768 - Epoch: [6][   10/   13]    Loss 1.719786    Top1 23.750000    Top5 93.750000    
2022-12-29 09:56:13,821 - Epoch: [6][   13/   13]    Loss 1.703868    Top1 25.000000    Top5 94.000000    
2022-12-29 09:56:13,878 - ==> Top1: 25.000    Top5: 94.000    Loss: 1.704

2022-12-29 09:56:13,879 - ==> Confusion:
[[ 0  3  0 20  9  0]
 [ 0  2  0 19 18  0]
 [ 0  0  0 20  8  0]
 [ 0  4  0 29 18  0]
 [ 0  0  0 19 19  0]
 [ 0  2  0  7  3  0]]

2022-12-29 09:56:13,880 - ==> Best [Top1: 29.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 4]
2022-12-29 09:56:13,881 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:56:13,904 - 

2022-12-29 09:56:13,904 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:14,594 - Epoch: [7][   10/  113]    Overall Loss 1.686849    Objective Loss 1.686849                                        LR 0.000100    Time 0.068947    
2022-12-29 09:56:15,095 - Epoch: [7][   20/  113]    Overall Loss 1.678641    Objective Loss 1.678641                                        LR 0.000100    Time 0.059453    
2022-12-29 09:56:15,592 - Epoch: [7][   30/  113]    Overall Loss 1.692728    Objective Loss 1.692728                                        LR 0.000100    Time 0.056197    
2022-12-29 09:56:16,094 - Epoch: [7][   40/  113]    Overall Loss 1.689089    Objective Loss 1.689089                                        LR 0.000100    Time 0.054675    
2022-12-29 09:56:16,593 - Epoch: [7][   50/  113]    Overall Loss 1.688156    Objective Loss 1.688156                                        LR 0.000100    Time 0.053713    
2022-12-29 09:56:17,093 - Epoch: [7][   60/  113]    Overall Loss 1.685614    Objective Loss 1.685614                                        LR 0.000100    Time 0.053094    
2022-12-29 09:56:17,593 - Epoch: [7][   70/  113]    Overall Loss 1.694336    Objective Loss 1.694336                                        LR 0.000100    Time 0.052643    
2022-12-29 09:56:18,093 - Epoch: [7][   80/  113]    Overall Loss 1.695231    Objective Loss 1.695231                                        LR 0.000100    Time 0.052305    
2022-12-29 09:56:18,592 - Epoch: [7][   90/  113]    Overall Loss 1.697851    Objective Loss 1.697851                                        LR 0.000100    Time 0.052031    
2022-12-29 09:56:19,093 - Epoch: [7][  100/  113]    Overall Loss 1.693113    Objective Loss 1.693113                                        LR 0.000100    Time 0.051830    
2022-12-29 09:56:19,595 - Epoch: [7][  110/  113]    Overall Loss 1.691838    Objective Loss 1.691838                                        LR 0.000100    Time 0.051679    
2022-12-29 09:56:19,739 - Epoch: [7][  113/  113]    Overall Loss 1.690620    Objective Loss 1.690620    Top1 16.666667    Top5 95.833333    LR 0.000100    Time 0.051581    
2022-12-29 09:56:19,787 - --- validate (epoch=7)-----------
2022-12-29 09:56:19,788 - 200 samples (16 per mini-batch)
2022-12-29 09:56:20,280 - Epoch: [7][   10/   13]    Loss 1.710311    Top1 28.125000    Top5 92.500000    
2022-12-29 09:56:20,335 - Epoch: [7][   13/   13]    Loss 1.726815    Top1 26.500000    Top5 92.000000    
2022-12-29 09:56:20,380 - ==> Top1: 26.500    Top5: 92.000    Loss: 1.727

2022-12-29 09:56:20,380 - ==> Confusion:
[[ 6  0  0 21  2  0]
 [ 0  1  0 22  6  0]
 [ 1  1  0 34  4  0]
 [ 0  2  0 33 14  0]
 [ 0  2  0 22 13  0]
 [ 0  1  0 11  4  0]]

2022-12-29 09:56:20,382 - ==> Best [Top1: 29.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 4]
2022-12-29 09:56:20,383 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:56:20,401 - 

2022-12-29 09:56:20,402 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:21,085 - Epoch: [8][   10/  113]    Overall Loss 1.656664    Objective Loss 1.656664                                        LR 0.000100    Time 0.068215    
2022-12-29 09:56:21,585 - Epoch: [8][   20/  113]    Overall Loss 1.662329    Objective Loss 1.662329                                        LR 0.000100    Time 0.059063    
2022-12-29 09:56:22,084 - Epoch: [8][   30/  113]    Overall Loss 1.656787    Objective Loss 1.656787                                        LR 0.000100    Time 0.056016    
2022-12-29 09:56:22,585 - Epoch: [8][   40/  113]    Overall Loss 1.638609    Objective Loss 1.638609                                        LR 0.000100    Time 0.054522    
2022-12-29 09:56:23,085 - Epoch: [8][   50/  113]    Overall Loss 1.634356    Objective Loss 1.634356                                        LR 0.000100    Time 0.053607    
2022-12-29 09:56:23,583 - Epoch: [8][   60/  113]    Overall Loss 1.643104    Objective Loss 1.643104                                        LR 0.000100    Time 0.052965    
2022-12-29 09:56:24,083 - Epoch: [8][   70/  113]    Overall Loss 1.644197    Objective Loss 1.644197                                        LR 0.000100    Time 0.052535    
2022-12-29 09:56:24,584 - Epoch: [8][   80/  113]    Overall Loss 1.650897    Objective Loss 1.650897                                        LR 0.000100    Time 0.052225    
2022-12-29 09:56:25,089 - Epoch: [8][   90/  113]    Overall Loss 1.663078    Objective Loss 1.663078                                        LR 0.000100    Time 0.052022    
2022-12-29 09:56:25,589 - Epoch: [8][  100/  113]    Overall Loss 1.668661    Objective Loss 1.668661                                        LR 0.000100    Time 0.051821    
2022-12-29 09:56:26,092 - Epoch: [8][  110/  113]    Overall Loss 1.667908    Objective Loss 1.667908                                        LR 0.000100    Time 0.051680    
2022-12-29 09:56:26,233 - Epoch: [8][  113/  113]    Overall Loss 1.666580    Objective Loss 1.666580    Top1 16.666667    Top5 91.666667    LR 0.000100    Time 0.051553    
2022-12-29 09:56:26,293 - --- validate (epoch=8)-----------
2022-12-29 09:56:26,294 - 200 samples (16 per mini-batch)
2022-12-29 09:56:26,779 - Epoch: [8][   10/   13]    Loss 1.647445    Top1 33.125000    Top5 95.000000    
2022-12-29 09:56:26,833 - Epoch: [8][   13/   13]    Loss 1.682443    Top1 30.500000    Top5 94.000000    
2022-12-29 09:56:26,875 - ==> Top1: 30.500    Top5: 94.000    Loss: 1.682

2022-12-29 09:56:26,876 - ==> Confusion:
[[ 1 12  0 12  0  0]
 [ 0 28  0 17  0  0]
 [ 0 15  0 18  0  0]
 [ 0 20  0 32  0  0]
 [ 0 20  0 13  0  0]
 [ 0  4  0  8  0  0]]

2022-12-29 09:56:26,877 - ==> Best [Top1: 30.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 8]
2022-12-29 09:56:26,878 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:56:26,900 - 

2022-12-29 09:56:26,900 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:27,585 - Epoch: [9][   10/  113]    Overall Loss 1.699234    Objective Loss 1.699234                                        LR 0.000100    Time 0.068351    
2022-12-29 09:56:28,086 - Epoch: [9][   20/  113]    Overall Loss 1.665673    Objective Loss 1.665673                                        LR 0.000100    Time 0.059213    
2022-12-29 09:56:28,586 - Epoch: [9][   30/  113]    Overall Loss 1.666062    Objective Loss 1.666062                                        LR 0.000100    Time 0.056112    
2022-12-29 09:56:29,086 - Epoch: [9][   40/  113]    Overall Loss 1.650262    Objective Loss 1.650262                                        LR 0.000100    Time 0.054576    
2022-12-29 09:56:29,587 - Epoch: [9][   50/  113]    Overall Loss 1.643287    Objective Loss 1.643287                                        LR 0.000100    Time 0.053668    
2022-12-29 09:56:30,089 - Epoch: [9][   60/  113]    Overall Loss 1.642807    Objective Loss 1.642807                                        LR 0.000100    Time 0.053082    
2022-12-29 09:56:30,590 - Epoch: [9][   70/  113]    Overall Loss 1.649659    Objective Loss 1.649659                                        LR 0.000100    Time 0.052647    
2022-12-29 09:56:31,094 - Epoch: [9][   80/  113]    Overall Loss 1.659691    Objective Loss 1.659691                                        LR 0.000100    Time 0.052363    
2022-12-29 09:56:31,595 - Epoch: [9][   90/  113]    Overall Loss 1.664700    Objective Loss 1.664700                                        LR 0.000100    Time 0.052101    
2022-12-29 09:56:32,093 - Epoch: [9][  100/  113]    Overall Loss 1.671736    Objective Loss 1.671736                                        LR 0.000100    Time 0.051873    
2022-12-29 09:56:32,596 - Epoch: [9][  110/  113]    Overall Loss 1.672264    Objective Loss 1.672264                                        LR 0.000100    Time 0.051724    
2022-12-29 09:56:32,743 - Epoch: [9][  113/  113]    Overall Loss 1.671792    Objective Loss 1.671792    Top1 16.666667    Top5 91.666667    LR 0.000100    Time 0.051645    
2022-12-29 09:56:32,785 - --- validate (epoch=9)-----------
2022-12-29 09:56:32,785 - 200 samples (16 per mini-batch)
2022-12-29 09:56:33,263 - Epoch: [9][   10/   13]    Loss 1.649555    Top1 28.750000    Top5 98.125000    
2022-12-29 09:56:33,319 - Epoch: [9][   13/   13]    Loss 1.650061    Top1 30.500000    Top5 98.000000    
2022-12-29 09:56:33,361 - ==> Top1: 30.500    Top5: 98.000    Loss: 1.650

2022-12-29 09:56:33,361 - ==> Confusion:
[[12  5  0  0 16  0]
 [ 1 12  0  0 24  0]
 [ 2  7  0  5 24  0]
 [ 5  9  0  4 30  0]
 [ 2  5  0  0 33  0]
 [ 1  0  0  1  2  0]]

2022-12-29 09:56:33,363 - ==> Best [Top1: 30.500   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 9]
2022-12-29 09:56:33,363 - Saving checkpoint to: logs/2022.12.29-095525/checkpoint.pth.tar
2022-12-29 09:56:33,412 - 

2022-12-29 09:56:33,412 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:34,230 - Epoch: [10][   10/  113]    Overall Loss 2.693558    Objective Loss 2.693558                                        LR 0.000100    Time 0.081641    
2022-12-29 09:56:34,860 - Epoch: [10][   20/  113]    Overall Loss 2.627983    Objective Loss 2.627983                                        LR 0.000100    Time 0.072284    
2022-12-29 09:56:35,489 - Epoch: [10][   30/  113]    Overall Loss 2.396572    Objective Loss 2.396572                                        LR 0.000100    Time 0.069155    
2022-12-29 09:56:36,118 - Epoch: [10][   40/  113]    Overall Loss 2.319529    Objective Loss 2.319529                                        LR 0.000100    Time 0.067577    
2022-12-29 09:56:36,744 - Epoch: [10][   50/  113]    Overall Loss 2.216913    Objective Loss 2.216913                                        LR 0.000100    Time 0.066568    
2022-12-29 09:56:37,367 - Epoch: [10][   60/  113]    Overall Loss 2.147219    Objective Loss 2.147219                                        LR 0.000100    Time 0.065851    
2022-12-29 09:56:37,995 - Epoch: [10][   70/  113]    Overall Loss 2.066201    Objective Loss 2.066201                                        LR 0.000100    Time 0.065405    
2022-12-29 09:56:38,619 - Epoch: [10][   80/  113]    Overall Loss 2.038938    Objective Loss 2.038938                                        LR 0.000100    Time 0.065027    
2022-12-29 09:56:39,249 - Epoch: [10][   90/  113]    Overall Loss 2.008883    Objective Loss 2.008883                                        LR 0.000100    Time 0.064794    
2022-12-29 09:56:39,871 - Epoch: [10][  100/  113]    Overall Loss 1.980290    Objective Loss 1.980290                                        LR 0.000100    Time 0.064536    
2022-12-29 09:56:40,496 - Epoch: [10][  110/  113]    Overall Loss 1.952373    Objective Loss 1.952373                                        LR 0.000100    Time 0.064344    
2022-12-29 09:56:40,672 - Epoch: [10][  113/  113]    Overall Loss 1.944831    Objective Loss 1.944831    Top1 29.166667    Top5 100.000000    LR 0.000100    Time 0.064185    
2022-12-29 09:56:40,738 - --- validate (epoch=10)-----------
2022-12-29 09:56:40,738 - 200 samples (16 per mini-batch)
2022-12-29 09:56:41,282 - Epoch: [10][   10/   13]    Loss 1.665222    Top1 25.625000    Top5 92.500000    
2022-12-29 09:56:41,368 - Epoch: [10][   13/   13]    Loss 1.702024    Top1 23.500000    Top5 92.000000    
2022-12-29 09:56:41,418 - ==> Top1: 23.500    Top5: 92.000    Loss: 1.702

2022-12-29 09:56:41,419 - ==> Confusion:
[[10  0  2  3 17  0]
 [ 1  0  6  9 30  0]
 [ 1  0 15  3 15  0]
 [ 0  2 12  7 22  0]
 [ 0  2  4  8 15  0]
 [ 1  0  2  5  8  0]]

2022-12-29 09:56:41,421 - ==> Best [Top1: 23.500   Top5: 92.000   Sparsity:0.00   Params: 289216 on epoch: 10]
2022-12-29 09:56:41,421 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:56:41,434 - 

2022-12-29 09:56:41,434 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:42,318 - Epoch: [11][   10/  113]    Overall Loss 1.696450    Objective Loss 1.696450                                        LR 0.000100    Time 0.088260    
2022-12-29 09:56:42,944 - Epoch: [11][   20/  113]    Overall Loss 1.671905    Objective Loss 1.671905                                        LR 0.000100    Time 0.075417    
2022-12-29 09:56:43,568 - Epoch: [11][   30/  113]    Overall Loss 1.678031    Objective Loss 1.678031                                        LR 0.000100    Time 0.071064    
2022-12-29 09:56:44,194 - Epoch: [11][   40/  113]    Overall Loss 1.677959    Objective Loss 1.677959                                        LR 0.000100    Time 0.068941    
2022-12-29 09:56:44,824 - Epoch: [11][   50/  113]    Overall Loss 1.689684    Objective Loss 1.689684                                        LR 0.000100    Time 0.067736    
2022-12-29 09:56:45,456 - Epoch: [11][   60/  113]    Overall Loss 1.685779    Objective Loss 1.685779                                        LR 0.000100    Time 0.066970    
2022-12-29 09:56:46,087 - Epoch: [11][   70/  113]    Overall Loss 1.685923    Objective Loss 1.685923                                        LR 0.000100    Time 0.066412    
2022-12-29 09:56:46,720 - Epoch: [11][   80/  113]    Overall Loss 1.680270    Objective Loss 1.680270                                        LR 0.000100    Time 0.066012    
2022-12-29 09:56:47,349 - Epoch: [11][   90/  113]    Overall Loss 1.679692    Objective Loss 1.679692                                        LR 0.000100    Time 0.065671    
2022-12-29 09:56:47,976 - Epoch: [11][  100/  113]    Overall Loss 1.675197    Objective Loss 1.675197                                        LR 0.000100    Time 0.065360    
2022-12-29 09:56:48,597 - Epoch: [11][  110/  113]    Overall Loss 1.672370    Objective Loss 1.672370                                        LR 0.000100    Time 0.065060    
2022-12-29 09:56:48,768 - Epoch: [11][  113/  113]    Overall Loss 1.672341    Objective Loss 1.672341    Top1 33.333333    Top5 91.666667    LR 0.000100    Time 0.064851    
2022-12-29 09:56:48,814 - --- validate (epoch=11)-----------
2022-12-29 09:56:48,815 - 200 samples (16 per mini-batch)
2022-12-29 09:56:49,359 - Epoch: [11][   10/   13]    Loss 1.626288    Top1 28.750000    Top5 96.875000    
2022-12-29 09:56:49,444 - Epoch: [11][   13/   13]    Loss 1.662378    Top1 27.500000    Top5 94.500000    
2022-12-29 09:56:49,491 - ==> Top1: 27.500    Top5: 94.500    Loss: 1.662

2022-12-29 09:56:49,492 - ==> Confusion:
[[ 8  7  0  6  6  0]
 [ 2 15  1 12 13  0]
 [ 3  9  1 12  6  0]
 [ 2 18  2 17 15  0]
 [ 0  8  2  8 14  0]
 [ 0  4  1  8  0  0]]

2022-12-29 09:56:49,495 - ==> Best [Top1: 27.500   Top5: 94.500   Sparsity:0.00   Params: 289216 on epoch: 11]
2022-12-29 09:56:49,495 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:56:49,523 - 

2022-12-29 09:56:49,523 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:50,344 - Epoch: [12][   10/  113]    Overall Loss 1.629445    Objective Loss 1.629445                                        LR 0.000100    Time 0.081964    
2022-12-29 09:56:50,981 - Epoch: [12][   20/  113]    Overall Loss 1.632712    Objective Loss 1.632712                                        LR 0.000100    Time 0.072793    
2022-12-29 09:56:51,611 - Epoch: [12][   30/  113]    Overall Loss 1.661471    Objective Loss 1.661471                                        LR 0.000100    Time 0.069500    
2022-12-29 09:56:52,232 - Epoch: [12][   40/  113]    Overall Loss 1.655959    Objective Loss 1.655959                                        LR 0.000100    Time 0.067643    
2022-12-29 09:56:52,861 - Epoch: [12][   50/  113]    Overall Loss 1.652076    Objective Loss 1.652076                                        LR 0.000100    Time 0.066688    
2022-12-29 09:56:53,484 - Epoch: [12][   60/  113]    Overall Loss 1.648364    Objective Loss 1.648364                                        LR 0.000100    Time 0.065943    
2022-12-29 09:56:54,109 - Epoch: [12][   70/  113]    Overall Loss 1.651234    Objective Loss 1.651234                                        LR 0.000100    Time 0.065442    
2022-12-29 09:56:54,730 - Epoch: [12][   80/  113]    Overall Loss 1.645620    Objective Loss 1.645620                                        LR 0.000100    Time 0.065024    
2022-12-29 09:56:55,355 - Epoch: [12][   90/  113]    Overall Loss 1.650497    Objective Loss 1.650497                                        LR 0.000100    Time 0.064738    
2022-12-29 09:56:55,975 - Epoch: [12][  100/  113]    Overall Loss 1.643706    Objective Loss 1.643706                                        LR 0.000100    Time 0.064459    
2022-12-29 09:56:56,593 - Epoch: [12][  110/  113]    Overall Loss 1.640246    Objective Loss 1.640246                                        LR 0.000100    Time 0.064217    
2022-12-29 09:56:56,766 - Epoch: [12][  113/  113]    Overall Loss 1.637878    Objective Loss 1.637878    Top1 45.833333    Top5 95.833333    LR 0.000100    Time 0.064038    
2022-12-29 09:56:56,820 - --- validate (epoch=12)-----------
2022-12-29 09:56:56,821 - 200 samples (16 per mini-batch)
2022-12-29 09:56:57,360 - Epoch: [12][   10/   13]    Loss 1.573797    Top1 36.250000    Top5 96.875000    
2022-12-29 09:56:57,444 - Epoch: [12][   13/   13]    Loss 1.550257    Top1 35.500000    Top5 97.000000    
2022-12-29 09:56:57,491 - ==> Top1: 35.500    Top5: 97.000    Loss: 1.550

2022-12-29 09:56:57,492 - ==> Confusion:
[[14  2  0 10  5  0]
 [ 4  6  0 20 12  0]
 [ 0  1  0 21 12  0]
 [ 2  6  1 32 12  0]
 [ 0  3  0 12 19  0]
 [ 0  0  0  3  3  0]]

2022-12-29 09:56:57,493 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:56:57,494 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:56:57,519 - 

2022-12-29 09:56:57,519 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:56:58,342 - Epoch: [13][   10/  113]    Overall Loss 1.636823    Objective Loss 1.636823                                        LR 0.000100    Time 0.082106    
2022-12-29 09:56:58,973 - Epoch: [13][   20/  113]    Overall Loss 1.642625    Objective Loss 1.642625                                        LR 0.000100    Time 0.072579    
2022-12-29 09:56:59,602 - Epoch: [13][   30/  113]    Overall Loss 1.621834    Objective Loss 1.621834                                        LR 0.000100    Time 0.069339    
2022-12-29 09:57:00,231 - Epoch: [13][   40/  113]    Overall Loss 1.599379    Objective Loss 1.599379                                        LR 0.000100    Time 0.067712    
2022-12-29 09:57:00,857 - Epoch: [13][   50/  113]    Overall Loss 1.584417    Objective Loss 1.584417                                        LR 0.000100    Time 0.066681    
2022-12-29 09:57:01,485 - Epoch: [13][   60/  113]    Overall Loss 1.597371    Objective Loss 1.597371                                        LR 0.000100    Time 0.066029    
2022-12-29 09:57:02,109 - Epoch: [13][   70/  113]    Overall Loss 1.600595    Objective Loss 1.600595                                        LR 0.000100    Time 0.065508    
2022-12-29 09:57:02,737 - Epoch: [13][   80/  113]    Overall Loss 1.602678    Objective Loss 1.602678                                        LR 0.000100    Time 0.065156    
2022-12-29 09:57:03,361 - Epoch: [13][   90/  113]    Overall Loss 1.603502    Objective Loss 1.603502                                        LR 0.000100    Time 0.064854    
2022-12-29 09:57:03,987 - Epoch: [13][  100/  113]    Overall Loss 1.605930    Objective Loss 1.605930                                        LR 0.000100    Time 0.064622    
2022-12-29 09:57:04,610 - Epoch: [13][  110/  113]    Overall Loss 1.608943    Objective Loss 1.608943                                        LR 0.000100    Time 0.064405    
2022-12-29 09:57:04,785 - Epoch: [13][  113/  113]    Overall Loss 1.609870    Objective Loss 1.609870    Top1 12.500000    Top5 100.000000    LR 0.000100    Time 0.064240    
2022-12-29 09:57:04,841 - --- validate (epoch=13)-----------
2022-12-29 09:57:04,842 - 200 samples (16 per mini-batch)
2022-12-29 09:57:05,387 - Epoch: [13][   10/   13]    Loss 1.654507    Top1 28.125000    Top5 91.875000    
2022-12-29 09:57:05,472 - Epoch: [13][   13/   13]    Loss 1.638598    Top1 28.000000    Top5 93.000000    
2022-12-29 09:57:05,523 - ==> Top1: 28.000    Top5: 93.000    Loss: 1.639

2022-12-29 09:57:05,523 - ==> Confusion:
[[12  2  0 16  1  0]
 [ 2  1  0 28  2  0]
 [ 4  0  0 27  2  0]
 [ 2  0  0 42  0  0]
 [ 3  1  0 40  1  0]
 [ 3  0  0 11  0  0]]

2022-12-29 09:57:05,526 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:05,527 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:05,540 - 

2022-12-29 09:57:05,540 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:06,363 - Epoch: [14][   10/  113]    Overall Loss 1.628749    Objective Loss 1.628749                                        LR 0.000100    Time 0.082179    
2022-12-29 09:57:06,992 - Epoch: [14][   20/  113]    Overall Loss 1.632777    Objective Loss 1.632777                                        LR 0.000100    Time 0.072495    
2022-12-29 09:57:07,624 - Epoch: [14][   30/  113]    Overall Loss 1.640642    Objective Loss 1.640642                                        LR 0.000100    Time 0.069386    
2022-12-29 09:57:08,254 - Epoch: [14][   40/  113]    Overall Loss 1.631025    Objective Loss 1.631025                                        LR 0.000100    Time 0.067788    
2022-12-29 09:57:08,882 - Epoch: [14][   50/  113]    Overall Loss 1.622986    Objective Loss 1.622986                                        LR 0.000100    Time 0.066788    
2022-12-29 09:57:09,510 - Epoch: [14][   60/  113]    Overall Loss 1.622219    Objective Loss 1.622219                                        LR 0.000100    Time 0.066114    
2022-12-29 09:57:10,137 - Epoch: [14][   70/  113]    Overall Loss 1.626699    Objective Loss 1.626699                                        LR 0.000100    Time 0.065613    
2022-12-29 09:57:10,762 - Epoch: [14][   80/  113]    Overall Loss 1.616644    Objective Loss 1.616644                                        LR 0.000100    Time 0.065223    
2022-12-29 09:57:11,391 - Epoch: [14][   90/  113]    Overall Loss 1.620968    Objective Loss 1.620968                                        LR 0.000100    Time 0.064956    
2022-12-29 09:57:12,018 - Epoch: [14][  100/  113]    Overall Loss 1.620263    Objective Loss 1.620263                                        LR 0.000100    Time 0.064724    
2022-12-29 09:57:12,644 - Epoch: [14][  110/  113]    Overall Loss 1.619263    Objective Loss 1.619263                                        LR 0.000100    Time 0.064527    
2022-12-29 09:57:12,818 - Epoch: [14][  113/  113]    Overall Loss 1.618790    Objective Loss 1.618790    Top1 16.666667    Top5 91.666667    LR 0.000100    Time 0.064358    
2022-12-29 09:57:12,864 - --- validate (epoch=14)-----------
2022-12-29 09:57:12,864 - 200 samples (16 per mini-batch)
2022-12-29 09:57:13,396 - Epoch: [14][   10/   13]    Loss 1.621034    Top1 28.125000    Top5 95.000000    
2022-12-29 09:57:13,481 - Epoch: [14][   13/   13]    Loss 1.623061    Top1 27.500000    Top5 94.500000    
2022-12-29 09:57:13,534 - ==> Top1: 27.500    Top5: 94.500    Loss: 1.623

2022-12-29 09:57:13,535 - ==> Confusion:
[[14  4  0 11  4  0]
 [18  7  1 21  5  0]
 [ 5  3  3 12  1  0]
 [ 5  8  0 30  4  0]
 [12  6  1 13  1  0]
 [ 2  3  0  6  0  0]]

2022-12-29 09:57:13,538 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:13,538 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:13,560 - 

2022-12-29 09:57:13,561 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:14,391 - Epoch: [15][   10/  113]    Overall Loss 1.707550    Objective Loss 1.707550                                        LR 0.000100    Time 0.082939    
2022-12-29 09:57:15,022 - Epoch: [15][   20/  113]    Overall Loss 1.644544    Objective Loss 1.644544                                        LR 0.000100    Time 0.073013    
2022-12-29 09:57:15,653 - Epoch: [15][   30/  113]    Overall Loss 1.624516    Objective Loss 1.624516                                        LR 0.000100    Time 0.069676    
2022-12-29 09:57:16,275 - Epoch: [15][   40/  113]    Overall Loss 1.623438    Objective Loss 1.623438                                        LR 0.000100    Time 0.067787    
2022-12-29 09:57:16,902 - Epoch: [15][   50/  113]    Overall Loss 1.622232    Objective Loss 1.622232                                        LR 0.000100    Time 0.066765    
2022-12-29 09:57:17,528 - Epoch: [15][   60/  113]    Overall Loss 1.614871    Objective Loss 1.614871                                        LR 0.000100    Time 0.066066    
2022-12-29 09:57:18,158 - Epoch: [15][   70/  113]    Overall Loss 1.616139    Objective Loss 1.616139                                        LR 0.000100    Time 0.065621    
2022-12-29 09:57:18,786 - Epoch: [15][   80/  113]    Overall Loss 1.618731    Objective Loss 1.618731                                        LR 0.000100    Time 0.065262    
2022-12-29 09:57:19,420 - Epoch: [15][   90/  113]    Overall Loss 1.621714    Objective Loss 1.621714                                        LR 0.000100    Time 0.065055    
2022-12-29 09:57:20,046 - Epoch: [15][  100/  113]    Overall Loss 1.613261    Objective Loss 1.613261                                        LR 0.000100    Time 0.064806    
2022-12-29 09:57:20,669 - Epoch: [15][  110/  113]    Overall Loss 1.616738    Objective Loss 1.616738                                        LR 0.000100    Time 0.064565    
2022-12-29 09:57:20,844 - Epoch: [15][  113/  113]    Overall Loss 1.612351    Objective Loss 1.612351    Top1 41.666667    Top5 95.833333    LR 0.000100    Time 0.064403    
2022-12-29 09:57:20,907 - --- validate (epoch=15)-----------
2022-12-29 09:57:20,907 - 200 samples (16 per mini-batch)
2022-12-29 09:57:21,452 - Epoch: [15][   10/   13]    Loss 1.578954    Top1 33.750000    Top5 95.625000    
2022-12-29 09:57:21,538 - Epoch: [15][   13/   13]    Loss 1.575286    Top1 33.500000    Top5 95.500000    
2022-12-29 09:57:21,581 - ==> Top1: 33.500    Top5: 95.500    Loss: 1.575

2022-12-29 09:57:21,582 - ==> Confusion:
[[21  5  0  5  9  0]
 [ 4 15  2 19  6  0]
 [ 2  1  1 22  8  0]
 [ 2  9  2 18  9  0]
 [ 1  5  1 13 12  0]
 [ 0  1  0  4  3  0]]

2022-12-29 09:57:21,584 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:21,585 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:21,595 - 

2022-12-29 09:57:21,595 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:22,417 - Epoch: [16][   10/  113]    Overall Loss 1.630108    Objective Loss 1.630108                                        LR 0.000100    Time 0.082130    
2022-12-29 09:57:23,045 - Epoch: [16][   20/  113]    Overall Loss 1.597918    Objective Loss 1.597918                                        LR 0.000100    Time 0.072430    
2022-12-29 09:57:23,672 - Epoch: [16][   30/  113]    Overall Loss 1.606884    Objective Loss 1.606884                                        LR 0.000100    Time 0.069167    
2022-12-29 09:57:24,299 - Epoch: [16][   40/  113]    Overall Loss 1.589720    Objective Loss 1.589720                                        LR 0.000100    Time 0.067545    
2022-12-29 09:57:24,926 - Epoch: [16][   50/  113]    Overall Loss 1.582414    Objective Loss 1.582414                                        LR 0.000100    Time 0.066550    
2022-12-29 09:57:25,554 - Epoch: [16][   60/  113]    Overall Loss 1.600443    Objective Loss 1.600443                                        LR 0.000100    Time 0.065927    
2022-12-29 09:57:26,177 - Epoch: [16][   70/  113]    Overall Loss 1.607632    Objective Loss 1.607632                                        LR 0.000100    Time 0.065405    
2022-12-29 09:57:26,803 - Epoch: [16][   80/  113]    Overall Loss 1.604719    Objective Loss 1.604719                                        LR 0.000100    Time 0.065038    
2022-12-29 09:57:27,433 - Epoch: [16][   90/  113]    Overall Loss 1.604044    Objective Loss 1.604044                                        LR 0.000100    Time 0.064812    
2022-12-29 09:57:28,059 - Epoch: [16][  100/  113]    Overall Loss 1.605714    Objective Loss 1.605714                                        LR 0.000100    Time 0.064581    
2022-12-29 09:57:28,683 - Epoch: [16][  110/  113]    Overall Loss 1.605987    Objective Loss 1.605987                                        LR 0.000100    Time 0.064382    
2022-12-29 09:57:28,856 - Epoch: [16][  113/  113]    Overall Loss 1.605842    Objective Loss 1.605842    Top1 37.500000    Top5 100.000000    LR 0.000100    Time 0.064200    
2022-12-29 09:57:28,906 - --- validate (epoch=16)-----------
2022-12-29 09:57:28,906 - 200 samples (16 per mini-batch)
2022-12-29 09:57:29,451 - Epoch: [16][   10/   13]    Loss 1.675716    Top1 28.125000    Top5 94.375000    
2022-12-29 09:57:29,538 - Epoch: [16][   13/   13]    Loss 1.670617    Top1 28.500000    Top5 94.000000    
2022-12-29 09:57:29,588 - ==> Top1: 28.500    Top5: 94.000    Loss: 1.671

2022-12-29 09:57:29,588 - ==> Confusion:
[[ 6  1  2  7  5  0]
 [ 4  6  2 17 10  0]
 [ 6  6  4  9  3  0]
 [ 3  8  3 26  9  0]
 [ 4 13  3 16 15  0]
 [ 3  4  0  2  3  0]]

2022-12-29 09:57:29,590 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:29,590 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:29,613 - 

2022-12-29 09:57:29,614 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:30,430 - Epoch: [17][   10/  113]    Overall Loss 1.607882    Objective Loss 1.607882                                        LR 0.000100    Time 0.081448    
2022-12-29 09:57:31,058 - Epoch: [17][   20/  113]    Overall Loss 1.652847    Objective Loss 1.652847                                        LR 0.000100    Time 0.072074    
2022-12-29 09:57:31,686 - Epoch: [17][   30/  113]    Overall Loss 1.650526    Objective Loss 1.650526                                        LR 0.000100    Time 0.068970    
2022-12-29 09:57:32,312 - Epoch: [17][   40/  113]    Overall Loss 1.623800    Objective Loss 1.623800                                        LR 0.000100    Time 0.067383    
2022-12-29 09:57:32,941 - Epoch: [17][   50/  113]    Overall Loss 1.604715    Objective Loss 1.604715                                        LR 0.000100    Time 0.066465    
2022-12-29 09:57:33,569 - Epoch: [17][   60/  113]    Overall Loss 1.602459    Objective Loss 1.602459                                        LR 0.000100    Time 0.065857    
2022-12-29 09:57:34,199 - Epoch: [17][   70/  113]    Overall Loss 1.612720    Objective Loss 1.612720                                        LR 0.000100    Time 0.065436    
2022-12-29 09:57:34,827 - Epoch: [17][   80/  113]    Overall Loss 1.624402    Objective Loss 1.624402                                        LR 0.000100    Time 0.065098    
2022-12-29 09:57:35,459 - Epoch: [17][   90/  113]    Overall Loss 1.625597    Objective Loss 1.625597                                        LR 0.000100    Time 0.064891    
2022-12-29 09:57:36,084 - Epoch: [17][  100/  113]    Overall Loss 1.620801    Objective Loss 1.620801                                        LR 0.000100    Time 0.064648    
2022-12-29 09:57:36,709 - Epoch: [17][  110/  113]    Overall Loss 1.621632    Objective Loss 1.621632                                        LR 0.000100    Time 0.064446    
2022-12-29 09:57:36,880 - Epoch: [17][  113/  113]    Overall Loss 1.620300    Objective Loss 1.620300    Top1 37.500000    Top5 95.833333    LR 0.000100    Time 0.064240    
2022-12-29 09:57:36,943 - --- validate (epoch=17)-----------
2022-12-29 09:57:36,943 - 200 samples (16 per mini-batch)
2022-12-29 09:57:37,481 - Epoch: [17][   10/   13]    Loss 1.608628    Top1 30.625000    Top5 93.750000    
2022-12-29 09:57:37,566 - Epoch: [17][   13/   13]    Loss 1.607346    Top1 34.000000    Top5 94.000000    
2022-12-29 09:57:37,627 - ==> Top1: 34.000    Top5: 94.000    Loss: 1.607

2022-12-29 09:57:37,628 - ==> Confusion:
[[14  4  2 14  2  0]
 [ 6 10  1 21  2  0]
 [ 3  7  1 12  0  0]
 [ 5  4  0 42  4  0]
 [ 4  5  2 25  1  0]
 [ 3  1  0  5  0  0]]

2022-12-29 09:57:37,631 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:37,632 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:37,654 - 

2022-12-29 09:57:37,654 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:38,467 - Epoch: [18][   10/  113]    Overall Loss 1.595174    Objective Loss 1.595174                                        LR 0.000100    Time 0.081190    
2022-12-29 09:57:39,093 - Epoch: [18][   20/  113]    Overall Loss 1.611106    Objective Loss 1.611106                                        LR 0.000100    Time 0.071862    
2022-12-29 09:57:39,725 - Epoch: [18][   30/  113]    Overall Loss 1.601925    Objective Loss 1.601925                                        LR 0.000100    Time 0.068952    
2022-12-29 09:57:40,350 - Epoch: [18][   40/  113]    Overall Loss 1.586720    Objective Loss 1.586720                                        LR 0.000100    Time 0.067322    
2022-12-29 09:57:40,978 - Epoch: [18][   50/  113]    Overall Loss 1.595289    Objective Loss 1.595289                                        LR 0.000100    Time 0.066419    
2022-12-29 09:57:41,608 - Epoch: [18][   60/  113]    Overall Loss 1.589425    Objective Loss 1.589425                                        LR 0.000100    Time 0.065834    
2022-12-29 09:57:42,234 - Epoch: [18][   70/  113]    Overall Loss 1.593842    Objective Loss 1.593842                                        LR 0.000100    Time 0.065372    
2022-12-29 09:57:42,859 - Epoch: [18][   80/  113]    Overall Loss 1.588674    Objective Loss 1.588674                                        LR 0.000100    Time 0.065006    
2022-12-29 09:57:43,488 - Epoch: [18][   90/  113]    Overall Loss 1.584067    Objective Loss 1.584067                                        LR 0.000100    Time 0.064765    
2022-12-29 09:57:44,114 - Epoch: [18][  100/  113]    Overall Loss 1.586499    Objective Loss 1.586499                                        LR 0.000100    Time 0.064543    
2022-12-29 09:57:44,739 - Epoch: [18][  110/  113]    Overall Loss 1.590937    Objective Loss 1.590937                                        LR 0.000100    Time 0.064354    
2022-12-29 09:57:44,909 - Epoch: [18][  113/  113]    Overall Loss 1.591041    Objective Loss 1.591041    Top1 29.166667    Top5 91.666667    LR 0.000100    Time 0.064143    
2022-12-29 09:57:44,969 - --- validate (epoch=18)-----------
2022-12-29 09:57:44,969 - 200 samples (16 per mini-batch)
2022-12-29 09:57:45,516 - Epoch: [18][   10/   13]    Loss 1.588139    Top1 30.000000    Top5 95.625000    
2022-12-29 09:57:45,602 - Epoch: [18][   13/   13]    Loss 1.608197    Top1 29.500000    Top5 95.500000    
2022-12-29 09:57:45,649 - ==> Top1: 29.500    Top5: 95.500    Loss: 1.608

2022-12-29 09:57:45,649 - ==> Confusion:
[[15  9  0 13  6  0]
 [ 5 20  0 13  4  0]
 [ 1 14  3 13  1  0]
 [ 1 20  0 14  4  0]
 [ 2 13  0 13  7  0]
 [ 2  3  0  4  0  0]]

2022-12-29 09:57:45,651 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:45,651 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:45,670 - 

2022-12-29 09:57:45,671 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:46,506 - Epoch: [19][   10/  113]    Overall Loss 1.622153    Objective Loss 1.622153                                        LR 0.000100    Time 0.083262    
2022-12-29 09:57:47,137 - Epoch: [19][   20/  113]    Overall Loss 1.625867    Objective Loss 1.625867                                        LR 0.000100    Time 0.073178    
2022-12-29 09:57:47,769 - Epoch: [19][   30/  113]    Overall Loss 1.639287    Objective Loss 1.639287                                        LR 0.000100    Time 0.069833    
2022-12-29 09:57:48,397 - Epoch: [19][   40/  113]    Overall Loss 1.631787    Objective Loss 1.631787                                        LR 0.000100    Time 0.068063    
2022-12-29 09:57:49,026 - Epoch: [19][   50/  113]    Overall Loss 1.612298    Objective Loss 1.612298                                        LR 0.000100    Time 0.067010    
2022-12-29 09:57:49,658 - Epoch: [19][   60/  113]    Overall Loss 1.608319    Objective Loss 1.608319                                        LR 0.000100    Time 0.066370    
2022-12-29 09:57:50,286 - Epoch: [19][   70/  113]    Overall Loss 1.611252    Objective Loss 1.611252                                        LR 0.000100    Time 0.065857    
2022-12-29 09:57:50,916 - Epoch: [19][   80/  113]    Overall Loss 1.608440    Objective Loss 1.608440                                        LR 0.000100    Time 0.065489    
2022-12-29 09:57:51,548 - Epoch: [19][   90/  113]    Overall Loss 1.608483    Objective Loss 1.608483                                        LR 0.000100    Time 0.065231    
2022-12-29 09:57:52,173 - Epoch: [19][  100/  113]    Overall Loss 1.606823    Objective Loss 1.606823                                        LR 0.000100    Time 0.064959    
2022-12-29 09:57:52,802 - Epoch: [19][  110/  113]    Overall Loss 1.606753    Objective Loss 1.606753                                        LR 0.000100    Time 0.064764    
2022-12-29 09:57:52,973 - Epoch: [19][  113/  113]    Overall Loss 1.608716    Objective Loss 1.608716    Top1 37.500000    Top5 87.500000    LR 0.000100    Time 0.064558    
2022-12-29 09:57:53,024 - --- validate (epoch=19)-----------
2022-12-29 09:57:53,024 - 200 samples (16 per mini-batch)
2022-12-29 09:57:53,566 - Epoch: [19][   10/   13]    Loss 1.617443    Top1 33.125000    Top5 95.000000    
2022-12-29 09:57:53,650 - Epoch: [19][   13/   13]    Loss 1.655310    Top1 32.000000    Top5 94.500000    
2022-12-29 09:57:53,694 - ==> Top1: 32.000    Top5: 94.500    Loss: 1.655

2022-12-29 09:57:53,694 - ==> Confusion:
[[15  1  1 14  3  0]
 [ 7  1  0  3 22  0]
 [ 4  1  2  6 16  0]
 [ 3  0  3 19 21  0]
 [ 6  6  2  5 27  0]
 [ 4  0  0  4  4  0]]

2022-12-29 09:57:53,696 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:57:53,696 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:57:53,715 - 

2022-12-29 09:57:53,715 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:57:54,545 - Epoch: [20][   10/  113]    Overall Loss 1.616728    Objective Loss 1.616728                                        LR 0.000100    Time 0.082788    
2022-12-29 09:57:55,177 - Epoch: [20][   20/  113]    Overall Loss 1.606958    Objective Loss 1.606958                                        LR 0.000100    Time 0.072980    
2022-12-29 09:57:55,809 - Epoch: [20][   30/  113]    Overall Loss 1.624424    Objective Loss 1.624424                                        LR 0.000100    Time 0.069703    
2022-12-29 09:57:56,440 - Epoch: [20][   40/  113]    Overall Loss 1.630075    Objective Loss 1.630075                                        LR 0.000100    Time 0.068039    
2022-12-29 09:57:57,072 - Epoch: [20][   50/  113]    Overall Loss 1.614431    Objective Loss 1.614431                                        LR 0.000100    Time 0.067054    
2022-12-29 09:57:57,701 - Epoch: [20][   60/  113]    Overall Loss 1.611347    Objective Loss 1.611347                                        LR 0.000100    Time 0.066355    
2022-12-29 09:57:58,329 - Epoch: [20][   70/  113]    Overall Loss 1.613132    Objective Loss 1.613132                                        LR 0.000100    Time 0.065851    
2022-12-29 09:57:58,959 - Epoch: [20][   80/  113]    Overall Loss 1.616307    Objective Loss 1.616307                                        LR 0.000100    Time 0.065487    
2022-12-29 09:57:59,591 - Epoch: [20][   90/  113]    Overall Loss 1.612295    Objective Loss 1.612295                                        LR 0.000100    Time 0.065221    
2022-12-29 09:58:00,221 - Epoch: [20][  100/  113]    Overall Loss 1.617890    Objective Loss 1.617890                                        LR 0.000100    Time 0.065002    
2022-12-29 09:58:00,844 - Epoch: [20][  110/  113]    Overall Loss 1.618735    Objective Loss 1.618735                                        LR 0.000100    Time 0.064748    
2022-12-29 09:58:01,021 - Epoch: [20][  113/  113]    Overall Loss 1.619085    Objective Loss 1.619085    Top1 33.333333    Top5 87.500000    LR 0.000100    Time 0.064587    
2022-12-29 09:58:01,071 - --- validate (epoch=20)-----------
2022-12-29 09:58:01,071 - 200 samples (16 per mini-batch)
2022-12-29 09:58:01,619 - Epoch: [20][   10/   13]    Loss 1.682253    Top1 26.875000    Top5 91.875000    
2022-12-29 09:58:01,703 - Epoch: [20][   13/   13]    Loss 1.681389    Top1 25.500000    Top5 91.500000    
2022-12-29 09:58:01,762 - ==> Top1: 25.500    Top5: 91.500    Loss: 1.681

2022-12-29 09:58:01,763 - ==> Confusion:
[[ 9  0  0 16  1  0]
 [ 1  0  0 43  1  0]
 [ 2  1  2 20  0  0]
 [ 3  2  0 37  0  0]
 [ 2  2  1 37  3  0]
 [ 3  1  0 13  0  0]]

2022-12-29 09:58:01,766 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:01,766 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:01,790 - 

2022-12-29 09:58:01,790 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:02,614 - Epoch: [21][   10/  113]    Overall Loss 1.649548    Objective Loss 1.649548                                        LR 0.000100    Time 0.082188    
2022-12-29 09:58:03,245 - Epoch: [21][   20/  113]    Overall Loss 1.670963    Objective Loss 1.670963                                        LR 0.000100    Time 0.072629    
2022-12-29 09:58:03,875 - Epoch: [21][   30/  113]    Overall Loss 1.642923    Objective Loss 1.642923                                        LR 0.000100    Time 0.069413    
2022-12-29 09:58:04,500 - Epoch: [21][   40/  113]    Overall Loss 1.614736    Objective Loss 1.614736                                        LR 0.000100    Time 0.067661    
2022-12-29 09:58:05,124 - Epoch: [21][   50/  113]    Overall Loss 1.623895    Objective Loss 1.623895                                        LR 0.000100    Time 0.066594    
2022-12-29 09:58:05,753 - Epoch: [21][   60/  113]    Overall Loss 1.618388    Objective Loss 1.618388                                        LR 0.000100    Time 0.065976    
2022-12-29 09:58:06,380 - Epoch: [21][   70/  113]    Overall Loss 1.624223    Objective Loss 1.624223                                        LR 0.000100    Time 0.065501    
2022-12-29 09:58:07,005 - Epoch: [21][   80/  113]    Overall Loss 1.629017    Objective Loss 1.629017                                        LR 0.000100    Time 0.065114    
2022-12-29 09:58:07,633 - Epoch: [21][   90/  113]    Overall Loss 1.621301    Objective Loss 1.621301                                        LR 0.000100    Time 0.064852    
2022-12-29 09:58:08,260 - Epoch: [21][  100/  113]    Overall Loss 1.618778    Objective Loss 1.618778                                        LR 0.000100    Time 0.064632    
2022-12-29 09:58:08,887 - Epoch: [21][  110/  113]    Overall Loss 1.620382    Objective Loss 1.620382                                        LR 0.000100    Time 0.064456    
2022-12-29 09:58:09,060 - Epoch: [21][  113/  113]    Overall Loss 1.614316    Objective Loss 1.614316    Top1 50.000000    Top5 100.000000    LR 0.000100    Time 0.064275    
2022-12-29 09:58:09,106 - --- validate (epoch=21)-----------
2022-12-29 09:58:09,106 - 200 samples (16 per mini-batch)
2022-12-29 09:58:09,641 - Epoch: [21][   10/   13]    Loss 1.653215    Top1 25.625000    Top5 91.875000    
2022-12-29 09:58:09,725 - Epoch: [21][   13/   13]    Loss 1.630728    Top1 25.000000    Top5 93.500000    
2022-12-29 09:58:09,780 - ==> Top1: 25.000    Top5: 93.500    Loss: 1.631

2022-12-29 09:58:09,780 - ==> Confusion:
[[ 9  0  0 14  7  0]
 [ 2  0  1  8 27  0]
 [ 3  1  3 13 14  0]
 [ 6  0  5 17 21  0]
 [ 3  1  2  8 21  0]
 [ 3  0  0  5  6  0]]

2022-12-29 09:58:09,782 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:09,782 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:09,794 - 

2022-12-29 09:58:09,795 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:10,611 - Epoch: [22][   10/  113]    Overall Loss 1.615424    Objective Loss 1.615424                                        LR 0.000100    Time 0.081513    
2022-12-29 09:58:11,243 - Epoch: [22][   20/  113]    Overall Loss 1.597261    Objective Loss 1.597261                                        LR 0.000100    Time 0.072336    
2022-12-29 09:58:11,869 - Epoch: [22][   30/  113]    Overall Loss 1.598945    Objective Loss 1.598945                                        LR 0.000100    Time 0.069077    
2022-12-29 09:58:12,498 - Epoch: [22][   40/  113]    Overall Loss 1.598676    Objective Loss 1.598676                                        LR 0.000100    Time 0.067520    
2022-12-29 09:58:13,124 - Epoch: [22][   50/  113]    Overall Loss 1.594027    Objective Loss 1.594027                                        LR 0.000100    Time 0.066523    
2022-12-29 09:58:13,748 - Epoch: [22][   60/  113]    Overall Loss 1.593185    Objective Loss 1.593185                                        LR 0.000100    Time 0.065830    
2022-12-29 09:58:14,375 - Epoch: [22][   70/  113]    Overall Loss 1.592595    Objective Loss 1.592595                                        LR 0.000100    Time 0.065373    
2022-12-29 09:58:15,002 - Epoch: [22][   80/  113]    Overall Loss 1.599805    Objective Loss 1.599805                                        LR 0.000100    Time 0.065036    
2022-12-29 09:58:15,630 - Epoch: [22][   90/  113]    Overall Loss 1.594914    Objective Loss 1.594914                                        LR 0.000100    Time 0.064780    
2022-12-29 09:58:16,256 - Epoch: [22][  100/  113]    Overall Loss 1.594319    Objective Loss 1.594319                                        LR 0.000100    Time 0.064562    
2022-12-29 09:58:16,883 - Epoch: [22][  110/  113]    Overall Loss 1.599377    Objective Loss 1.599377                                        LR 0.000100    Time 0.064389    
2022-12-29 09:58:17,057 - Epoch: [22][  113/  113]    Overall Loss 1.602385    Objective Loss 1.602385    Top1 20.833333    Top5 87.500000    LR 0.000100    Time 0.064212    
2022-12-29 09:58:17,118 - --- validate (epoch=22)-----------
2022-12-29 09:58:17,119 - 200 samples (16 per mini-batch)
2022-12-29 09:58:17,655 - Epoch: [22][   10/   13]    Loss 1.594899    Top1 35.000000    Top5 96.250000    
2022-12-29 09:58:17,738 - Epoch: [22][   13/   13]    Loss 1.597922    Top1 33.000000    Top5 96.000000    
2022-12-29 09:58:17,791 - ==> Top1: 33.000    Top5: 96.000    Loss: 1.598

2022-12-29 09:58:17,792 - ==> Confusion:
[[ 9  6  0  3  3  0]
 [ 4 25  0  7  9  0]
 [ 3 11  2  7 10  0]
 [ 7 17  0 17  9  0]
 [ 4 13  2 10 13  0]
 [ 2  1  0  6  0  0]]

2022-12-29 09:58:17,794 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:17,794 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:17,820 - 

2022-12-29 09:58:17,820 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:18,651 - Epoch: [23][   10/  113]    Overall Loss 1.615621    Objective Loss 1.615621                                        LR 0.000100    Time 0.083039    
2022-12-29 09:58:19,278 - Epoch: [23][   20/  113]    Overall Loss 1.602135    Objective Loss 1.602135                                        LR 0.000100    Time 0.072840    
2022-12-29 09:58:19,915 - Epoch: [23][   30/  113]    Overall Loss 1.606923    Objective Loss 1.606923                                        LR 0.000100    Time 0.069772    
2022-12-29 09:58:20,538 - Epoch: [23][   40/  113]    Overall Loss 1.601739    Objective Loss 1.601739                                        LR 0.000100    Time 0.067901    
2022-12-29 09:58:21,169 - Epoch: [23][   50/  113]    Overall Loss 1.600635    Objective Loss 1.600635                                        LR 0.000100    Time 0.066916    
2022-12-29 09:58:21,794 - Epoch: [23][   60/  113]    Overall Loss 1.603592    Objective Loss 1.603592                                        LR 0.000100    Time 0.066175    
2022-12-29 09:58:22,421 - Epoch: [23][   70/  113]    Overall Loss 1.597661    Objective Loss 1.597661                                        LR 0.000100    Time 0.065680    
2022-12-29 09:58:23,051 - Epoch: [23][   80/  113]    Overall Loss 1.597664    Objective Loss 1.597664                                        LR 0.000100    Time 0.065337    
2022-12-29 09:58:23,682 - Epoch: [23][   90/  113]    Overall Loss 1.599195    Objective Loss 1.599195                                        LR 0.000100    Time 0.065077    
2022-12-29 09:58:24,308 - Epoch: [23][  100/  113]    Overall Loss 1.595063    Objective Loss 1.595063                                        LR 0.000100    Time 0.064826    
2022-12-29 09:58:24,929 - Epoch: [23][  110/  113]    Overall Loss 1.588256    Objective Loss 1.588256                                        LR 0.000100    Time 0.064573    
2022-12-29 09:58:25,098 - Epoch: [23][  113/  113]    Overall Loss 1.587916    Objective Loss 1.587916    Top1 29.166667    Top5 95.833333    LR 0.000100    Time 0.064352    
2022-12-29 09:58:25,159 - --- validate (epoch=23)-----------
2022-12-29 09:58:25,159 - 200 samples (16 per mini-batch)
2022-12-29 09:58:25,700 - Epoch: [23][   10/   13]    Loss 1.589362    Top1 35.000000    Top5 95.625000    
2022-12-29 09:58:25,786 - Epoch: [23][   13/   13]    Loss 1.589050    Top1 33.500000    Top5 95.000000    
2022-12-29 09:58:25,839 - ==> Top1: 33.500    Top5: 95.000    Loss: 1.589

2022-12-29 09:58:25,840 - ==> Confusion:
[[18  0  0  7  2  0]
 [16  9  2 21  6  0]
 [ 5  3  6  7  0  0]
 [11  6  3 29  1  0]
 [ 7  2  2 17  5  0]
 [ 2  5  3  3  2  0]]

2022-12-29 09:58:25,843 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:25,843 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:25,857 - 

2022-12-29 09:58:25,858 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:26,667 - Epoch: [24][   10/  113]    Overall Loss 1.528209    Objective Loss 1.528209                                        LR 0.000100    Time 0.080832    
2022-12-29 09:58:27,300 - Epoch: [24][   20/  113]    Overall Loss 1.513028    Objective Loss 1.513028                                        LR 0.000100    Time 0.072009    
2022-12-29 09:58:27,933 - Epoch: [24][   30/  113]    Overall Loss 1.551929    Objective Loss 1.551929                                        LR 0.000100    Time 0.069099    
2022-12-29 09:58:28,562 - Epoch: [24][   40/  113]    Overall Loss 1.550697    Objective Loss 1.550697                                        LR 0.000100    Time 0.067531    
2022-12-29 09:58:29,189 - Epoch: [24][   50/  113]    Overall Loss 1.556051    Objective Loss 1.556051                                        LR 0.000100    Time 0.066552    
2022-12-29 09:58:29,816 - Epoch: [24][   60/  113]    Overall Loss 1.553340    Objective Loss 1.553340                                        LR 0.000100    Time 0.065904    
2022-12-29 09:58:30,443 - Epoch: [24][   70/  113]    Overall Loss 1.559264    Objective Loss 1.559264                                        LR 0.000100    Time 0.065442    
2022-12-29 09:58:31,074 - Epoch: [24][   80/  113]    Overall Loss 1.562105    Objective Loss 1.562105                                        LR 0.000100    Time 0.065142    
2022-12-29 09:58:31,704 - Epoch: [24][   90/  113]    Overall Loss 1.567736    Objective Loss 1.567736                                        LR 0.000100    Time 0.064898    
2022-12-29 09:58:32,331 - Epoch: [24][  100/  113]    Overall Loss 1.571137    Objective Loss 1.571137                                        LR 0.000100    Time 0.064672    
2022-12-29 09:58:32,958 - Epoch: [24][  110/  113]    Overall Loss 1.570213    Objective Loss 1.570213                                        LR 0.000100    Time 0.064496    
2022-12-29 09:58:33,130 - Epoch: [24][  113/  113]    Overall Loss 1.573115    Objective Loss 1.573115    Top1 45.833333    Top5 87.500000    LR 0.000100    Time 0.064301    
2022-12-29 09:58:33,191 - --- validate (epoch=24)-----------
2022-12-29 09:58:33,191 - 200 samples (16 per mini-batch)
2022-12-29 09:58:33,739 - Epoch: [24][   10/   13]    Loss 1.672043    Top1 27.500000    Top5 91.875000    
2022-12-29 09:58:33,824 - Epoch: [24][   13/   13]    Loss 1.677601    Top1 27.500000    Top5 92.000000    
2022-12-29 09:58:33,872 - ==> Top1: 27.500    Top5: 92.000    Loss: 1.678

2022-12-29 09:58:33,873 - ==> Confusion:
[[11  1  0 15  1  0]
 [ 6  7  0 25  2  0]
 [ 6  5  0 23  0  0]
 [ 2  1  0 34  3  0]
 [ 5  7  0 27  3  0]
 [ 3  6  0  6  1  0]]

2022-12-29 09:58:33,875 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:33,875 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:33,899 - 

2022-12-29 09:58:33,900 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:34,736 - Epoch: [25][   10/  113]    Overall Loss 1.608099    Objective Loss 1.608099                                        LR 0.000100    Time 0.083546    
2022-12-29 09:58:35,365 - Epoch: [25][   20/  113]    Overall Loss 1.577655    Objective Loss 1.577655                                        LR 0.000100    Time 0.073181    
2022-12-29 09:58:36,003 - Epoch: [25][   30/  113]    Overall Loss 1.592898    Objective Loss 1.592898                                        LR 0.000100    Time 0.070032    
2022-12-29 09:58:36,632 - Epoch: [25][   40/  113]    Overall Loss 1.586724    Objective Loss 1.586724                                        LR 0.000100    Time 0.068252    
2022-12-29 09:58:37,256 - Epoch: [25][   50/  113]    Overall Loss 1.589036    Objective Loss 1.589036                                        LR 0.000100    Time 0.067075    
2022-12-29 09:58:37,884 - Epoch: [25][   60/  113]    Overall Loss 1.593341    Objective Loss 1.593341                                        LR 0.000100    Time 0.066343    
2022-12-29 09:58:38,510 - Epoch: [25][   70/  113]    Overall Loss 1.596446    Objective Loss 1.596446                                        LR 0.000100    Time 0.065808    
2022-12-29 09:58:39,135 - Epoch: [25][   80/  113]    Overall Loss 1.590834    Objective Loss 1.590834                                        LR 0.000100    Time 0.065385    
2022-12-29 09:58:39,765 - Epoch: [25][   90/  113]    Overall Loss 1.593785    Objective Loss 1.593785                                        LR 0.000100    Time 0.065115    
2022-12-29 09:58:40,389 - Epoch: [25][  100/  113]    Overall Loss 1.592373    Objective Loss 1.592373                                        LR 0.000100    Time 0.064840    
2022-12-29 09:58:41,019 - Epoch: [25][  110/  113]    Overall Loss 1.595119    Objective Loss 1.595119                                        LR 0.000100    Time 0.064666    
2022-12-29 09:58:41,191 - Epoch: [25][  113/  113]    Overall Loss 1.597985    Objective Loss 1.597985    Top1 25.000000    Top5 87.500000    LR 0.000100    Time 0.064473    
2022-12-29 09:58:41,243 - --- validate (epoch=25)-----------
2022-12-29 09:58:41,243 - 200 samples (16 per mini-batch)
2022-12-29 09:58:41,827 - Epoch: [25][   10/   13]    Loss 1.664171    Top1 29.375000    Top5 94.375000    
2022-12-29 09:58:41,911 - Epoch: [25][   13/   13]    Loss 1.651739    Top1 30.500000    Top5 94.500000    
2022-12-29 09:58:41,970 - ==> Top1: 30.500    Top5: 94.500    Loss: 1.652

2022-12-29 09:58:41,971 - ==> Confusion:
[[16  6  1  3  7  0]
 [ 7  8  0  2 17  0]
 [ 3  3  3  5 13  0]
 [ 9  6  8 10 22  0]
 [ 9  2  1  2 24  0]
 [ 2  1  2  1  7  0]]

2022-12-29 09:58:41,973 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:41,973 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:41,995 - 

2022-12-29 09:58:41,996 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:42,828 - Epoch: [26][   10/  113]    Overall Loss 1.619691    Objective Loss 1.619691                                        LR 0.000100    Time 0.083177    
2022-12-29 09:58:43,460 - Epoch: [26][   20/  113]    Overall Loss 1.593837    Objective Loss 1.593837                                        LR 0.000100    Time 0.073123    
2022-12-29 09:58:44,091 - Epoch: [26][   30/  113]    Overall Loss 1.615678    Objective Loss 1.615678                                        LR 0.000100    Time 0.069777    
2022-12-29 09:58:44,717 - Epoch: [26][   40/  113]    Overall Loss 1.604111    Objective Loss 1.604111                                        LR 0.000100    Time 0.067979    
2022-12-29 09:58:45,337 - Epoch: [26][   50/  113]    Overall Loss 1.595140    Objective Loss 1.595140                                        LR 0.000100    Time 0.066768    
2022-12-29 09:58:45,962 - Epoch: [26][   60/  113]    Overall Loss 1.606849    Objective Loss 1.606849                                        LR 0.000100    Time 0.066037    
2022-12-29 09:58:46,586 - Epoch: [26][   70/  113]    Overall Loss 1.611995    Objective Loss 1.611995                                        LR 0.000100    Time 0.065521    
2022-12-29 09:58:47,207 - Epoch: [26][   80/  113]    Overall Loss 1.613443    Objective Loss 1.613443                                        LR 0.000100    Time 0.065085    
2022-12-29 09:58:47,839 - Epoch: [26][   90/  113]    Overall Loss 1.609817    Objective Loss 1.609817                                        LR 0.000100    Time 0.064865    
2022-12-29 09:58:48,467 - Epoch: [26][  100/  113]    Overall Loss 1.609896    Objective Loss 1.609896                                        LR 0.000100    Time 0.064661    
2022-12-29 09:58:49,085 - Epoch: [26][  110/  113]    Overall Loss 1.605414    Objective Loss 1.605414                                        LR 0.000100    Time 0.064392    
2022-12-29 09:58:49,257 - Epoch: [26][  113/  113]    Overall Loss 1.608200    Objective Loss 1.608200    Top1 25.000000    Top5 91.666667    LR 0.000100    Time 0.064203    
2022-12-29 09:58:49,300 - --- validate (epoch=26)-----------
2022-12-29 09:58:49,300 - 200 samples (16 per mini-batch)
2022-12-29 09:58:49,851 - Epoch: [26][   10/   13]    Loss 1.616388    Top1 30.000000    Top5 94.375000    
2022-12-29 09:58:49,935 - Epoch: [26][   13/   13]    Loss 1.627925    Top1 29.500000    Top5 95.000000    
2022-12-29 09:58:50,000 - ==> Top1: 29.500    Top5: 95.000    Loss: 1.628

2022-12-29 09:58:50,000 - ==> Confusion:
[[20  2  0  6  2  0]
 [12 12  0 10  4  0]
 [ 6  8  0 17  3  0]
 [10 10  0 23  8  0]
 [12  8  0 10  4  0]
 [ 5  3  0  3  2  0]]

2022-12-29 09:58:50,003 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:50,003 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:50,017 - 

2022-12-29 09:58:50,018 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:50,842 - Epoch: [27][   10/  113]    Overall Loss 1.637477    Objective Loss 1.637477                                        LR 0.000100    Time 0.082277    
2022-12-29 09:58:51,478 - Epoch: [27][   20/  113]    Overall Loss 1.583603    Objective Loss 1.583603                                        LR 0.000100    Time 0.072940    
2022-12-29 09:58:52,105 - Epoch: [27][   30/  113]    Overall Loss 1.578106    Objective Loss 1.578106                                        LR 0.000100    Time 0.069509    
2022-12-29 09:58:52,730 - Epoch: [27][   40/  113]    Overall Loss 1.561796    Objective Loss 1.561796                                        LR 0.000100    Time 0.067750    
2022-12-29 09:58:53,355 - Epoch: [27][   50/  113]    Overall Loss 1.564735    Objective Loss 1.564735                                        LR 0.000100    Time 0.066677    
2022-12-29 09:58:53,979 - Epoch: [27][   60/  113]    Overall Loss 1.582582    Objective Loss 1.582582                                        LR 0.000100    Time 0.065959    
2022-12-29 09:58:54,608 - Epoch: [27][   70/  113]    Overall Loss 1.584024    Objective Loss 1.584024                                        LR 0.000100    Time 0.065523    
2022-12-29 09:58:55,234 - Epoch: [27][   80/  113]    Overall Loss 1.592563    Objective Loss 1.592563                                        LR 0.000100    Time 0.065151    
2022-12-29 09:58:55,863 - Epoch: [27][   90/  113]    Overall Loss 1.593328    Objective Loss 1.593328                                        LR 0.000100    Time 0.064891    
2022-12-29 09:58:56,489 - Epoch: [27][  100/  113]    Overall Loss 1.593016    Objective Loss 1.593016                                        LR 0.000100    Time 0.064661    
2022-12-29 09:58:57,115 - Epoch: [27][  110/  113]    Overall Loss 1.585749    Objective Loss 1.585749                                        LR 0.000100    Time 0.064464    
2022-12-29 09:58:57,287 - Epoch: [27][  113/  113]    Overall Loss 1.585372    Objective Loss 1.585372    Top1 41.666667    Top5 100.000000    LR 0.000100    Time 0.064273    
2022-12-29 09:58:57,339 - --- validate (epoch=27)-----------
2022-12-29 09:58:57,339 - 200 samples (16 per mini-batch)
2022-12-29 09:58:57,885 - Epoch: [27][   10/   13]    Loss 1.559864    Top1 33.125000    Top5 98.750000    
2022-12-29 09:58:57,968 - Epoch: [27][   13/   13]    Loss 1.609358    Top1 33.000000    Top5 97.000000    
2022-12-29 09:58:58,021 - ==> Top1: 33.000    Top5: 97.000    Loss: 1.609

2022-12-29 09:58:58,022 - ==> Confusion:
[[11  3  0 13  1  0]
 [ 6  8  4 24 10  0]
 [ 1  3  8 17  1  0]
 [ 6  0  2 32  6  0]
 [ 3  3  3 18  7  0]
 [ 1  1  3  4  1  0]]

2022-12-29 09:58:58,023 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:58:58,023 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:58:58,043 - 

2022-12-29 09:58:58,043 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:58:58,877 - Epoch: [28][   10/  113]    Overall Loss 1.605805    Objective Loss 1.605805                                        LR 0.000100    Time 0.083151    
2022-12-29 09:58:59,505 - Epoch: [28][   20/  113]    Overall Loss 1.581970    Objective Loss 1.581970                                        LR 0.000100    Time 0.072958    
2022-12-29 09:59:00,142 - Epoch: [28][   30/  113]    Overall Loss 1.603450    Objective Loss 1.603450                                        LR 0.000100    Time 0.069858    
2022-12-29 09:59:00,767 - Epoch: [28][   40/  113]    Overall Loss 1.593492    Objective Loss 1.593492                                        LR 0.000100    Time 0.068019    
2022-12-29 09:59:01,400 - Epoch: [28][   50/  113]    Overall Loss 1.589685    Objective Loss 1.589685                                        LR 0.000100    Time 0.067065    
2022-12-29 09:59:02,029 - Epoch: [28][   60/  113]    Overall Loss 1.582369    Objective Loss 1.582369                                        LR 0.000100    Time 0.066352    
2022-12-29 09:59:02,655 - Epoch: [28][   70/  113]    Overall Loss 1.592749    Objective Loss 1.592749                                        LR 0.000100    Time 0.065815    
2022-12-29 09:59:03,282 - Epoch: [28][   80/  113]    Overall Loss 1.598246    Objective Loss 1.598246                                        LR 0.000100    Time 0.065421    
2022-12-29 09:59:03,917 - Epoch: [28][   90/  113]    Overall Loss 1.600077    Objective Loss 1.600077                                        LR 0.000100    Time 0.065205    
2022-12-29 09:59:04,550 - Epoch: [28][  100/  113]    Overall Loss 1.596911    Objective Loss 1.596911                                        LR 0.000100    Time 0.065011    
2022-12-29 09:59:05,176 - Epoch: [28][  110/  113]    Overall Loss 1.595899    Objective Loss 1.595899                                        LR 0.000100    Time 0.064784    
2022-12-29 09:59:05,350 - Epoch: [28][  113/  113]    Overall Loss 1.593485    Objective Loss 1.593485    Top1 12.500000    Top5 100.000000    LR 0.000100    Time 0.064606    
2022-12-29 09:59:05,408 - --- validate (epoch=28)-----------
2022-12-29 09:59:05,409 - 200 samples (16 per mini-batch)
2022-12-29 09:59:05,947 - Epoch: [28][   10/   13]    Loss 1.583426    Top1 35.000000    Top5 93.750000    
2022-12-29 09:59:06,033 - Epoch: [28][   13/   13]    Loss 1.599799    Top1 35.000000    Top5 94.500000    
2022-12-29 09:59:06,079 - ==> Top1: 35.000    Top5: 94.500    Loss: 1.600

2022-12-29 09:59:06,080 - ==> Confusion:
[[15  4  0  9  6  0]
 [ 3  3  0 11 17  0]
 [ 3  3  1 17 14  0]
 [ 1  0  2 33 16  0]
 [ 1  1  1 10 18  0]
 [ 2  2  0  1  6  0]]

2022-12-29 09:59:06,082 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:06,083 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:06,097 - 

2022-12-29 09:59:06,097 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:06,914 - Epoch: [29][   10/  113]    Overall Loss 1.502083    Objective Loss 1.502083                                        LR 0.000100    Time 0.081526    
2022-12-29 09:59:07,548 - Epoch: [29][   20/  113]    Overall Loss 1.545789    Objective Loss 1.545789                                        LR 0.000100    Time 0.072447    
2022-12-29 09:59:08,186 - Epoch: [29][   30/  113]    Overall Loss 1.575907    Objective Loss 1.575907                                        LR 0.000100    Time 0.069543    
2022-12-29 09:59:08,817 - Epoch: [29][   40/  113]    Overall Loss 1.559190    Objective Loss 1.559190                                        LR 0.000100    Time 0.067932    
2022-12-29 09:59:09,448 - Epoch: [29][   50/  113]    Overall Loss 1.561766    Objective Loss 1.561766                                        LR 0.000100    Time 0.066958    
2022-12-29 09:59:10,077 - Epoch: [29][   60/  113]    Overall Loss 1.576276    Objective Loss 1.576276                                        LR 0.000100    Time 0.066260    
2022-12-29 09:59:10,701 - Epoch: [29][   70/  113]    Overall Loss 1.578106    Objective Loss 1.578106                                        LR 0.000100    Time 0.065712    
2022-12-29 09:59:11,334 - Epoch: [29][   80/  113]    Overall Loss 1.576900    Objective Loss 1.576900                                        LR 0.000100    Time 0.065400    
2022-12-29 09:59:11,967 - Epoch: [29][   90/  113]    Overall Loss 1.574975    Objective Loss 1.574975                                        LR 0.000100    Time 0.065158    
2022-12-29 09:59:12,596 - Epoch: [29][  100/  113]    Overall Loss 1.570670    Objective Loss 1.570670                                        LR 0.000100    Time 0.064936    
2022-12-29 09:59:13,230 - Epoch: [29][  110/  113]    Overall Loss 1.572718    Objective Loss 1.572718                                        LR 0.000100    Time 0.064784    
2022-12-29 09:59:13,403 - Epoch: [29][  113/  113]    Overall Loss 1.576555    Objective Loss 1.576555    Top1 25.000000    Top5 87.500000    LR 0.000100    Time 0.064592    
2022-12-29 09:59:13,462 - --- validate (epoch=29)-----------
2022-12-29 09:59:13,462 - 200 samples (16 per mini-batch)
2022-12-29 09:59:14,014 - Epoch: [29][   10/   13]    Loss 1.651019    Top1 33.125000    Top5 96.250000    
2022-12-29 09:59:14,100 - Epoch: [29][   13/   13]    Loss 1.674284    Top1 31.500000    Top5 96.000000    
2022-12-29 09:59:14,149 - ==> Top1: 31.500    Top5: 96.000    Loss: 1.674

2022-12-29 09:59:14,150 - ==> Confusion:
[[15  9  2  8  8  0]
 [ 4 12  3 13  4  0]
 [ 2 10  6 11  4  0]
 [ 1  9  1 19 12  0]
 [11  9  2  6 11  0]
 [ 3  2  0  2  1  0]]

2022-12-29 09:59:14,151 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:14,151 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:14,170 - 

2022-12-29 09:59:14,171 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:14,989 - Epoch: [30][   10/  113]    Overall Loss 1.563880    Objective Loss 1.563880                                        LR 0.000100    Time 0.081682    
2022-12-29 09:59:15,616 - Epoch: [30][   20/  113]    Overall Loss 1.561151    Objective Loss 1.561151                                        LR 0.000100    Time 0.072125    
2022-12-29 09:59:16,245 - Epoch: [30][   30/  113]    Overall Loss 1.575385    Objective Loss 1.575385                                        LR 0.000100    Time 0.069054    
2022-12-29 09:59:16,872 - Epoch: [30][   40/  113]    Overall Loss 1.569582    Objective Loss 1.569582                                        LR 0.000100    Time 0.067440    
2022-12-29 09:59:17,497 - Epoch: [30][   50/  113]    Overall Loss 1.563943    Objective Loss 1.563943                                        LR 0.000100    Time 0.066456    
2022-12-29 09:59:18,125 - Epoch: [30][   60/  113]    Overall Loss 1.567629    Objective Loss 1.567629                                        LR 0.000100    Time 0.065832    
2022-12-29 09:59:18,751 - Epoch: [30][   70/  113]    Overall Loss 1.570627    Objective Loss 1.570627                                        LR 0.000100    Time 0.065371    
2022-12-29 09:59:19,379 - Epoch: [30][   80/  113]    Overall Loss 1.574225    Objective Loss 1.574225                                        LR 0.000100    Time 0.065042    
2022-12-29 09:59:20,011 - Epoch: [30][   90/  113]    Overall Loss 1.563760    Objective Loss 1.563760                                        LR 0.000100    Time 0.064825    
2022-12-29 09:59:20,640 - Epoch: [30][  100/  113]    Overall Loss 1.567888    Objective Loss 1.567888                                        LR 0.000100    Time 0.064636    
2022-12-29 09:59:21,267 - Epoch: [30][  110/  113]    Overall Loss 1.570670    Objective Loss 1.570670                                        LR 0.000100    Time 0.064455    
2022-12-29 09:59:21,441 - Epoch: [30][  113/  113]    Overall Loss 1.568033    Objective Loss 1.568033    Top1 20.833333    Top5 100.000000    LR 0.000100    Time 0.064281    
2022-12-29 09:59:21,495 - --- validate (epoch=30)-----------
2022-12-29 09:59:21,496 - 200 samples (16 per mini-batch)
2022-12-29 09:59:22,046 - Epoch: [30][   10/   13]    Loss 1.533066    Top1 30.625000    Top5 98.750000    
2022-12-29 09:59:22,132 - Epoch: [30][   13/   13]    Loss 1.561424    Top1 30.000000    Top5 98.500000    
2022-12-29 09:59:22,183 - ==> Top1: 30.000    Top5: 98.500    Loss: 1.561

2022-12-29 09:59:22,184 - ==> Confusion:
[[16  3  1  5  8  0]
 [ 9 11  1  5 21  0]
 [ 4  2  5 11  4  0]
 [ 8  4 10 20 13  0]
 [ 6  4  5 10  8  0]
 [ 0  3  1  0  2  0]]

2022-12-29 09:59:22,186 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:22,186 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:22,199 - 

2022-12-29 09:59:22,199 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:23,025 - Epoch: [31][   10/  113]    Overall Loss 1.609072    Objective Loss 1.609072                                        LR 0.000100    Time 0.082446    
2022-12-29 09:59:23,654 - Epoch: [31][   20/  113]    Overall Loss 1.613772    Objective Loss 1.613772                                        LR 0.000100    Time 0.072640    
2022-12-29 09:59:24,288 - Epoch: [31][   30/  113]    Overall Loss 1.619806    Objective Loss 1.619806                                        LR 0.000100    Time 0.069551    
2022-12-29 09:59:24,918 - Epoch: [31][   40/  113]    Overall Loss 1.630620    Objective Loss 1.630620                                        LR 0.000100    Time 0.067910    
2022-12-29 09:59:25,541 - Epoch: [31][   50/  113]    Overall Loss 1.614839    Objective Loss 1.614839                                        LR 0.000100    Time 0.066767    
2022-12-29 09:59:26,170 - Epoch: [31][   60/  113]    Overall Loss 1.623858    Objective Loss 1.623858                                        LR 0.000100    Time 0.066124    
2022-12-29 09:59:26,795 - Epoch: [31][   70/  113]    Overall Loss 1.612605    Objective Loss 1.612605                                        LR 0.000100    Time 0.065592    
2022-12-29 09:59:27,428 - Epoch: [31][   80/  113]    Overall Loss 1.618657    Objective Loss 1.618657                                        LR 0.000100    Time 0.065301    
2022-12-29 09:59:28,062 - Epoch: [31][   90/  113]    Overall Loss 1.608596    Objective Loss 1.608596                                        LR 0.000100    Time 0.065091    
2022-12-29 09:59:28,691 - Epoch: [31][  100/  113]    Overall Loss 1.606592    Objective Loss 1.606592                                        LR 0.000100    Time 0.064862    
2022-12-29 09:59:29,317 - Epoch: [31][  110/  113]    Overall Loss 1.605201    Objective Loss 1.605201                                        LR 0.000100    Time 0.064651    
2022-12-29 09:59:29,487 - Epoch: [31][  113/  113]    Overall Loss 1.604822    Objective Loss 1.604822    Top1 29.166667    Top5 95.833333    LR 0.000100    Time 0.064444    
2022-12-29 09:59:29,535 - --- validate (epoch=31)-----------
2022-12-29 09:59:29,536 - 200 samples (16 per mini-batch)
2022-12-29 09:59:30,080 - Epoch: [31][   10/   13]    Loss 1.569988    Top1 30.000000    Top5 93.750000    
2022-12-29 09:59:30,167 - Epoch: [31][   13/   13]    Loss 1.582823    Top1 30.500000    Top5 93.500000    
2022-12-29 09:59:30,214 - ==> Top1: 30.500    Top5: 93.500    Loss: 1.583

2022-12-29 09:59:30,215 - ==> Confusion:
[[16  1  0  4 10  0]
 [ 5  3  0 14 25  0]
 [ 2  1  1  7 11  0]
 [ 0  2  7 17 18  0]
 [ 3  0  2 13 24  0]
 [ 1  1  0  1 11  0]]

2022-12-29 09:59:30,216 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:30,217 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:30,235 - 

2022-12-29 09:59:30,236 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:31,067 - Epoch: [32][   10/  113]    Overall Loss 1.553986    Objective Loss 1.553986                                        LR 0.000100    Time 0.082988    
2022-12-29 09:59:31,700 - Epoch: [32][   20/  113]    Overall Loss 1.583324    Objective Loss 1.583324                                        LR 0.000100    Time 0.073089    
2022-12-29 09:59:32,333 - Epoch: [32][   30/  113]    Overall Loss 1.564475    Objective Loss 1.564475                                        LR 0.000100    Time 0.069808    
2022-12-29 09:59:32,966 - Epoch: [32][   40/  113]    Overall Loss 1.553845    Objective Loss 1.553845                                        LR 0.000100    Time 0.068184    
2022-12-29 09:59:33,596 - Epoch: [32][   50/  113]    Overall Loss 1.576724    Objective Loss 1.576724                                        LR 0.000100    Time 0.067127    
2022-12-29 09:59:34,226 - Epoch: [32][   60/  113]    Overall Loss 1.574981    Objective Loss 1.574981                                        LR 0.000100    Time 0.066429    
2022-12-29 09:59:34,858 - Epoch: [32][   70/  113]    Overall Loss 1.586071    Objective Loss 1.586071                                        LR 0.000100    Time 0.065970    
2022-12-29 09:59:35,485 - Epoch: [32][   80/  113]    Overall Loss 1.585690    Objective Loss 1.585690                                        LR 0.000100    Time 0.065551    
2022-12-29 09:59:36,116 - Epoch: [32][   90/  113]    Overall Loss 1.580510    Objective Loss 1.580510                                        LR 0.000100    Time 0.065279    
2022-12-29 09:59:36,744 - Epoch: [32][  100/  113]    Overall Loss 1.576293    Objective Loss 1.576293                                        LR 0.000100    Time 0.065020    
2022-12-29 09:59:37,367 - Epoch: [32][  110/  113]    Overall Loss 1.580422    Objective Loss 1.580422                                        LR 0.000100    Time 0.064770    
2022-12-29 09:59:37,538 - Epoch: [32][  113/  113]    Overall Loss 1.579252    Objective Loss 1.579252    Top1 33.333333    Top5 95.833333    LR 0.000100    Time 0.064558    
2022-12-29 09:59:37,599 - --- validate (epoch=32)-----------
2022-12-29 09:59:37,600 - 200 samples (16 per mini-batch)
2022-12-29 09:59:38,147 - Epoch: [32][   10/   13]    Loss 1.626096    Top1 29.375000    Top5 94.375000    
2022-12-29 09:59:38,231 - Epoch: [32][   13/   13]    Loss 1.623359    Top1 30.000000    Top5 94.000000    
2022-12-29 09:59:38,286 - ==> Top1: 30.000    Top5: 94.000    Loss: 1.623

2022-12-29 09:59:38,287 - ==> Confusion:
[[ 8  6  0 14  4  0]
 [ 2  8  1 19  1  0]
 [ 2 10  3 20  4  0]
 [ 2  5  2 33  5  0]
 [ 1 10  0 18  8  0]
 [ 2  3  0  5  4  0]]

2022-12-29 09:59:38,288 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:38,289 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:38,304 - 

2022-12-29 09:59:38,305 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:39,138 - Epoch: [33][   10/  113]    Overall Loss 1.549414    Objective Loss 1.549414                                        LR 0.000100    Time 0.083252    
2022-12-29 09:59:39,769 - Epoch: [33][   20/  113]    Overall Loss 1.578583    Objective Loss 1.578583                                        LR 0.000100    Time 0.073128    
2022-12-29 09:59:40,401 - Epoch: [33][   30/  113]    Overall Loss 1.555643    Objective Loss 1.555643                                        LR 0.000100    Time 0.069800    
2022-12-29 09:59:41,029 - Epoch: [33][   40/  113]    Overall Loss 1.555703    Objective Loss 1.555703                                        LR 0.000100    Time 0.068057    
2022-12-29 09:59:41,664 - Epoch: [33][   50/  113]    Overall Loss 1.562760    Objective Loss 1.562760                                        LR 0.000100    Time 0.067126    
2022-12-29 09:59:42,289 - Epoch: [33][   60/  113]    Overall Loss 1.559487    Objective Loss 1.559487                                        LR 0.000100    Time 0.066355    
2022-12-29 09:59:42,916 - Epoch: [33][   70/  113]    Overall Loss 1.575580    Objective Loss 1.575580                                        LR 0.000100    Time 0.065824    
2022-12-29 09:59:43,542 - Epoch: [33][   80/  113]    Overall Loss 1.579466    Objective Loss 1.579466                                        LR 0.000100    Time 0.065413    
2022-12-29 09:59:44,168 - Epoch: [33][   90/  113]    Overall Loss 1.582116    Objective Loss 1.582116                                        LR 0.000100    Time 0.065092    
2022-12-29 09:59:44,796 - Epoch: [33][  100/  113]    Overall Loss 1.582573    Objective Loss 1.582573                                        LR 0.000100    Time 0.064860    
2022-12-29 09:59:45,420 - Epoch: [33][  110/  113]    Overall Loss 1.578279    Objective Loss 1.578279                                        LR 0.000100    Time 0.064629    
2022-12-29 09:59:45,592 - Epoch: [33][  113/  113]    Overall Loss 1.576439    Objective Loss 1.576439    Top1 25.000000    Top5 100.000000    LR 0.000100    Time 0.064437    
2022-12-29 09:59:45,652 - --- validate (epoch=33)-----------
2022-12-29 09:59:45,652 - 200 samples (16 per mini-batch)
2022-12-29 09:59:46,198 - Epoch: [33][   10/   13]    Loss 1.735614    Top1 28.125000    Top5 93.750000    
2022-12-29 09:59:46,284 - Epoch: [33][   13/   13]    Loss 1.706797    Top1 29.500000    Top5 94.500000    
2022-12-29 09:59:46,329 - ==> Top1: 29.500    Top5: 94.500    Loss: 1.707

2022-12-29 09:59:46,330 - ==> Confusion:
[[10  2  3 15  2  0]
 [ 4  7  3 15  2  0]
 [ 4  6  6 15  4  0]
 [ 3  3  4 27  4  0]
 [ 2 17  6 17  9  0]
 [ 1  2  2  3  2  0]]

2022-12-29 09:59:46,333 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:46,333 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:46,350 - 

2022-12-29 09:59:46,350 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:47,185 - Epoch: [34][   10/  113]    Overall Loss 1.548999    Objective Loss 1.548999                                        LR 0.000100    Time 0.083327    
2022-12-29 09:59:47,816 - Epoch: [34][   20/  113]    Overall Loss 1.585414    Objective Loss 1.585414                                        LR 0.000100    Time 0.073197    
2022-12-29 09:59:48,444 - Epoch: [34][   30/  113]    Overall Loss 1.591930    Objective Loss 1.591930                                        LR 0.000100    Time 0.069716    
2022-12-29 09:59:49,069 - Epoch: [34][   40/  113]    Overall Loss 1.585685    Objective Loss 1.585685                                        LR 0.000100    Time 0.067903    
2022-12-29 09:59:49,693 - Epoch: [34][   50/  113]    Overall Loss 1.572471    Objective Loss 1.572471                                        LR 0.000100    Time 0.066782    
2022-12-29 09:59:50,325 - Epoch: [34][   60/  113]    Overall Loss 1.575613    Objective Loss 1.575613                                        LR 0.000100    Time 0.066184    
2022-12-29 09:59:50,962 - Epoch: [34][   70/  113]    Overall Loss 1.563618    Objective Loss 1.563618                                        LR 0.000100    Time 0.065826    
2022-12-29 09:59:51,589 - Epoch: [34][   80/  113]    Overall Loss 1.570023    Objective Loss 1.570023                                        LR 0.000100    Time 0.065428    
2022-12-29 09:59:52,220 - Epoch: [34][   90/  113]    Overall Loss 1.566916    Objective Loss 1.566916                                        LR 0.000100    Time 0.065163    
2022-12-29 09:59:52,852 - Epoch: [34][  100/  113]    Overall Loss 1.568811    Objective Loss 1.568811                                        LR 0.000100    Time 0.064965    
2022-12-29 09:59:53,477 - Epoch: [34][  110/  113]    Overall Loss 1.581723    Objective Loss 1.581723                                        LR 0.000100    Time 0.064731    
2022-12-29 09:59:53,649 - Epoch: [34][  113/  113]    Overall Loss 1.580972    Objective Loss 1.580972    Top1 37.500000    Top5 100.000000    LR 0.000100    Time 0.064533    
2022-12-29 09:59:53,704 - --- validate (epoch=34)-----------
2022-12-29 09:59:53,704 - 200 samples (16 per mini-batch)
2022-12-29 09:59:54,250 - Epoch: [34][   10/   13]    Loss 1.521977    Top1 35.000000    Top5 97.500000    
2022-12-29 09:59:54,335 - Epoch: [34][   13/   13]    Loss 1.559006    Top1 34.500000    Top5 96.000000    
2022-12-29 09:59:54,394 - ==> Top1: 34.500    Top5: 96.000    Loss: 1.559

2022-12-29 09:59:54,394 - ==> Confusion:
[[12  5  0  9  3  0]
 [ 5 11  2  7  7  0]
 [ 2 11  0 18  2  0]
 [ 3  7  4 39  4  0]
 [ 2  7  1 17  7  0]
 [ 2  8  1  2  2  0]]

2022-12-29 09:59:54,396 - ==> Best [Top1: 35.500   Top5: 97.000   Sparsity:0.00   Params: 289216 on epoch: 12]
2022-12-29 09:59:54,397 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 09:59:54,412 - 

2022-12-29 09:59:54,412 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 09:59:55,233 - Epoch: [35][   10/  113]    Overall Loss 1.540721    Objective Loss 1.540721                                        LR 0.000100    Time 0.082029    
2022-12-29 09:59:55,862 - Epoch: [35][   20/  113]    Overall Loss 1.530220    Objective Loss 1.530220                                        LR 0.000100    Time 0.072419    
2022-12-29 09:59:56,491 - Epoch: [35][   30/  113]    Overall Loss 1.540972    Objective Loss 1.540972                                        LR 0.000100    Time 0.069239    
2022-12-29 09:59:57,117 - Epoch: [35][   40/  113]    Overall Loss 1.584013    Objective Loss 1.584013                                        LR 0.000100    Time 0.067577    
2022-12-29 09:59:57,743 - Epoch: [35][   50/  113]    Overall Loss 1.583172    Objective Loss 1.583172                                        LR 0.000100    Time 0.066561    
2022-12-29 09:59:58,369 - Epoch: [35][   60/  113]    Overall Loss 1.589267    Objective Loss 1.589267                                        LR 0.000100    Time 0.065894    
2022-12-29 09:59:58,995 - Epoch: [35][   70/  113]    Overall Loss 1.597216    Objective Loss 1.597216                                        LR 0.000100    Time 0.065418    
2022-12-29 09:59:59,619 - Epoch: [35][   80/  113]    Overall Loss 1.592065    Objective Loss 1.592065                                        LR 0.000100    Time 0.065037    
2022-12-29 10:00:00,247 - Epoch: [35][   90/  113]    Overall Loss 1.579310    Objective Loss 1.579310                                        LR 0.000100    Time 0.064788    
2022-12-29 10:00:00,875 - Epoch: [35][  100/  113]    Overall Loss 1.569705    Objective Loss 1.569705                                        LR 0.000100    Time 0.064581    
2022-12-29 10:00:01,497 - Epoch: [35][  110/  113]    Overall Loss 1.569623    Objective Loss 1.569623                                        LR 0.000100    Time 0.064361    
2022-12-29 10:00:01,673 - Epoch: [35][  113/  113]    Overall Loss 1.567709    Objective Loss 1.567709    Top1 33.333333    Top5 95.833333    LR 0.000100    Time 0.064202    
2022-12-29 10:00:01,734 - --- validate (epoch=35)-----------
2022-12-29 10:00:01,734 - 200 samples (16 per mini-batch)
2022-12-29 10:00:02,278 - Epoch: [35][   10/   13]    Loss 1.571304    Top1 39.375000    Top5 95.000000    
2022-12-29 10:00:02,365 - Epoch: [35][   13/   13]    Loss 1.606961    Top1 37.500000    Top5 94.000000    
2022-12-29 10:00:02,412 - ==> Top1: 37.500    Top5: 94.000    Loss: 1.607

2022-12-29 10:00:02,412 - ==> Confusion:
[[12  3  2 11  4  0]
 [ 4 19  4 12  6  0]
 [ 2  3  8 12  4  0]
 [ 1  4  5 25  3  0]
 [ 3  7  6 15 11  0]
 [ 1  8  2  2  1  0]]

2022-12-29 10:00:02,414 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:02,414 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:02,440 - 

2022-12-29 10:00:02,440 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:03,257 - Epoch: [36][   10/  113]    Overall Loss 1.525882    Objective Loss 1.525882                                        LR 0.000100    Time 0.081463    
2022-12-29 10:00:03,882 - Epoch: [36][   20/  113]    Overall Loss 1.556743    Objective Loss 1.556743                                        LR 0.000100    Time 0.071979    
2022-12-29 10:00:04,515 - Epoch: [36][   30/  113]    Overall Loss 1.570115    Objective Loss 1.570115                                        LR 0.000100    Time 0.069064    
2022-12-29 10:00:05,145 - Epoch: [36][   40/  113]    Overall Loss 1.551991    Objective Loss 1.551991                                        LR 0.000100    Time 0.067530    
2022-12-29 10:00:05,774 - Epoch: [36][   50/  113]    Overall Loss 1.560086    Objective Loss 1.560086                                        LR 0.000100    Time 0.066595    
2022-12-29 10:00:06,404 - Epoch: [36][   60/  113]    Overall Loss 1.558388    Objective Loss 1.558388                                        LR 0.000100    Time 0.065995    
2022-12-29 10:00:07,026 - Epoch: [36][   70/  113]    Overall Loss 1.567447    Objective Loss 1.567447                                        LR 0.000100    Time 0.065442    
2022-12-29 10:00:07,646 - Epoch: [36][   80/  113]    Overall Loss 1.573539    Objective Loss 1.573539                                        LR 0.000100    Time 0.065006    
2022-12-29 10:00:08,269 - Epoch: [36][   90/  113]    Overall Loss 1.569982    Objective Loss 1.569982                                        LR 0.000100    Time 0.064698    
2022-12-29 10:00:08,892 - Epoch: [36][  100/  113]    Overall Loss 1.565620    Objective Loss 1.565620                                        LR 0.000100    Time 0.064452    
2022-12-29 10:00:09,512 - Epoch: [36][  110/  113]    Overall Loss 1.561290    Objective Loss 1.561290                                        LR 0.000100    Time 0.064224    
2022-12-29 10:00:09,681 - Epoch: [36][  113/  113]    Overall Loss 1.564513    Objective Loss 1.564513    Top1 16.666667    Top5 100.000000    LR 0.000100    Time 0.064017    
2022-12-29 10:00:09,732 - --- validate (epoch=36)-----------
2022-12-29 10:00:09,732 - 200 samples (16 per mini-batch)
2022-12-29 10:00:10,272 - Epoch: [36][   10/   13]    Loss 1.622897    Top1 30.625000    Top5 95.000000    
2022-12-29 10:00:10,357 - Epoch: [36][   13/   13]    Loss 1.632234    Top1 29.500000    Top5 93.000000    
2022-12-29 10:00:10,408 - ==> Top1: 29.500    Top5: 93.000    Loss: 1.632

2022-12-29 10:00:10,409 - ==> Confusion:
[[11  6  0  6  5  0]
 [ 6 20  0  8 12  0]
 [ 1  6  0 21  1  0]
 [ 5 12  0 24  6  0]
 [ 9  8  0 13  4  0]
 [ 5  5  0  5  1  0]]

2022-12-29 10:00:10,412 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:10,413 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:10,435 - 

2022-12-29 10:00:10,435 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:11,257 - Epoch: [37][   10/  113]    Overall Loss 1.424757    Objective Loss 1.424757                                        LR 0.000100    Time 0.082067    
2022-12-29 10:00:11,890 - Epoch: [37][   20/  113]    Overall Loss 1.483497    Objective Loss 1.483497                                        LR 0.000100    Time 0.072641    
2022-12-29 10:00:12,520 - Epoch: [37][   30/  113]    Overall Loss 1.518028    Objective Loss 1.518028                                        LR 0.000100    Time 0.069428    
2022-12-29 10:00:13,148 - Epoch: [37][   40/  113]    Overall Loss 1.544750    Objective Loss 1.544750                                        LR 0.000100    Time 0.067741    
2022-12-29 10:00:13,774 - Epoch: [37][   50/  113]    Overall Loss 1.558603    Objective Loss 1.558603                                        LR 0.000100    Time 0.066722    
2022-12-29 10:00:14,401 - Epoch: [37][   60/  113]    Overall Loss 1.556130    Objective Loss 1.556130                                        LR 0.000100    Time 0.066033    
2022-12-29 10:00:15,032 - Epoch: [37][   70/  113]    Overall Loss 1.562989    Objective Loss 1.562989                                        LR 0.000100    Time 0.065605    
2022-12-29 10:00:15,659 - Epoch: [37][   80/  113]    Overall Loss 1.564110    Objective Loss 1.564110                                        LR 0.000100    Time 0.065241    
2022-12-29 10:00:16,287 - Epoch: [37][   90/  113]    Overall Loss 1.566928    Objective Loss 1.566928                                        LR 0.000100    Time 0.064964    
2022-12-29 10:00:16,917 - Epoch: [37][  100/  113]    Overall Loss 1.564625    Objective Loss 1.564625                                        LR 0.000100    Time 0.064763    
2022-12-29 10:00:17,535 - Epoch: [37][  110/  113]    Overall Loss 1.559214    Objective Loss 1.559214                                        LR 0.000100    Time 0.064493    
2022-12-29 10:00:17,711 - Epoch: [37][  113/  113]    Overall Loss 1.560075    Objective Loss 1.560075    Top1 25.000000    Top5 100.000000    LR 0.000100    Time 0.064337    
2022-12-29 10:00:17,766 - --- validate (epoch=37)-----------
2022-12-29 10:00:17,766 - 200 samples (16 per mini-batch)
2022-12-29 10:00:18,301 - Epoch: [37][   10/   13]    Loss 1.653345    Top1 30.625000    Top5 92.500000    
2022-12-29 10:00:18,386 - Epoch: [37][   13/   13]    Loss 1.659042    Top1 31.000000    Top5 92.500000    
2022-12-29 10:00:18,450 - ==> Top1: 31.000    Top5: 92.500    Loss: 1.659

2022-12-29 10:00:18,451 - ==> Confusion:
[[13  5  0 10  4  0]
 [ 3  8  0 21  5  0]
 [ 5  6  5 23  0  0]
 [ 2  5  0 32  3  0]
 [ 4 10  1 16  4  0]
 [ 2  5  0  6  2  0]]

2022-12-29 10:00:18,453 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:18,453 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:18,474 - 

2022-12-29 10:00:18,475 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:19,304 - Epoch: [38][   10/  113]    Overall Loss 1.602927    Objective Loss 1.602927                                        LR 0.000100    Time 0.082835    
2022-12-29 10:00:19,937 - Epoch: [38][   20/  113]    Overall Loss 1.598817    Objective Loss 1.598817                                        LR 0.000100    Time 0.073060    
2022-12-29 10:00:20,575 - Epoch: [38][   30/  113]    Overall Loss 1.567680    Objective Loss 1.567680                                        LR 0.000100    Time 0.069955    
2022-12-29 10:00:21,207 - Epoch: [38][   40/  113]    Overall Loss 1.569419    Objective Loss 1.569419                                        LR 0.000100    Time 0.068256    
2022-12-29 10:00:21,832 - Epoch: [38][   50/  113]    Overall Loss 1.566839    Objective Loss 1.566839                                        LR 0.000100    Time 0.067100    
2022-12-29 10:00:22,467 - Epoch: [38][   60/  113]    Overall Loss 1.574726    Objective Loss 1.574726                                        LR 0.000100    Time 0.066485    
2022-12-29 10:00:23,097 - Epoch: [38][   70/  113]    Overall Loss 1.581510    Objective Loss 1.581510                                        LR 0.000100    Time 0.065981    
2022-12-29 10:00:23,722 - Epoch: [38][   80/  113]    Overall Loss 1.579761    Objective Loss 1.579761                                        LR 0.000100    Time 0.065533    
2022-12-29 10:00:24,348 - Epoch: [38][   90/  113]    Overall Loss 1.573704    Objective Loss 1.573704                                        LR 0.000100    Time 0.065202    
2022-12-29 10:00:24,979 - Epoch: [38][  100/  113]    Overall Loss 1.569769    Objective Loss 1.569769                                        LR 0.000100    Time 0.064988    
2022-12-29 10:00:25,604 - Epoch: [38][  110/  113]    Overall Loss 1.575430    Objective Loss 1.575430                                        LR 0.000100    Time 0.064764    
2022-12-29 10:00:25,775 - Epoch: [38][  113/  113]    Overall Loss 1.577923    Objective Loss 1.577923    Top1 12.500000    Top5 95.833333    LR 0.000100    Time 0.064556    
2022-12-29 10:00:25,835 - --- validate (epoch=38)-----------
2022-12-29 10:00:25,835 - 200 samples (16 per mini-batch)
2022-12-29 10:00:26,376 - Epoch: [38][   10/   13]    Loss 1.651492    Top1 26.250000    Top5 93.750000    
2022-12-29 10:00:26,459 - Epoch: [38][   13/   13]    Loss 1.622211    Top1 29.000000    Top5 94.500000    
2022-12-29 10:00:26,508 - ==> Top1: 29.000    Top5: 94.500    Loss: 1.622

2022-12-29 10:00:26,509 - ==> Confusion:
[[10 10  1  3  6  0]
 [ 3 19  3  5 13  0]
 [ 3  3  5  5 12  0]
 [ 2  5  1  8 30  0]
 [ 3 11  2  6 16  0]
 [ 4  5  0  1  5  0]]

2022-12-29 10:00:26,510 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:26,510 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:26,529 - 

2022-12-29 10:00:26,530 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:27,357 - Epoch: [39][   10/  113]    Overall Loss 1.483698    Objective Loss 1.483698                                        LR 0.000100    Time 0.082628    
2022-12-29 10:00:27,993 - Epoch: [39][   20/  113]    Overall Loss 1.530225    Objective Loss 1.530225                                        LR 0.000100    Time 0.073069    
2022-12-29 10:00:28,629 - Epoch: [39][   30/  113]    Overall Loss 1.551508    Objective Loss 1.551508                                        LR 0.000100    Time 0.069925    
2022-12-29 10:00:29,257 - Epoch: [39][   40/  113]    Overall Loss 1.545344    Objective Loss 1.545344                                        LR 0.000100    Time 0.068119    
2022-12-29 10:00:29,881 - Epoch: [39][   50/  113]    Overall Loss 1.553816    Objective Loss 1.553816                                        LR 0.000100    Time 0.066975    
2022-12-29 10:00:30,506 - Epoch: [39][   60/  113]    Overall Loss 1.560154    Objective Loss 1.560154                                        LR 0.000100    Time 0.066215    
2022-12-29 10:00:31,137 - Epoch: [39][   70/  113]    Overall Loss 1.555779    Objective Loss 1.555779                                        LR 0.000100    Time 0.065760    
2022-12-29 10:00:31,760 - Epoch: [39][   80/  113]    Overall Loss 1.542452    Objective Loss 1.542452                                        LR 0.000100    Time 0.065324    
2022-12-29 10:00:32,389 - Epoch: [39][   90/  113]    Overall Loss 1.550582    Objective Loss 1.550582                                        LR 0.000100    Time 0.065050    
2022-12-29 10:00:33,016 - Epoch: [39][  100/  113]    Overall Loss 1.552582    Objective Loss 1.552582                                        LR 0.000100    Time 0.064811    
2022-12-29 10:00:33,638 - Epoch: [39][  110/  113]    Overall Loss 1.550139    Objective Loss 1.550139                                        LR 0.000100    Time 0.064572    
2022-12-29 10:00:33,811 - Epoch: [39][  113/  113]    Overall Loss 1.549538    Objective Loss 1.549538    Top1 37.500000    Top5 91.666667    LR 0.000100    Time 0.064381    
2022-12-29 10:00:33,857 - --- validate (epoch=39)-----------
2022-12-29 10:00:33,858 - 200 samples (16 per mini-batch)
2022-12-29 10:00:34,406 - Epoch: [39][   10/   13]    Loss 1.735279    Top1 27.500000    Top5 93.750000    
2022-12-29 10:00:34,493 - Epoch: [39][   13/   13]    Loss 1.688186    Top1 30.000000    Top5 94.500000    
2022-12-29 10:00:34,550 - ==> Top1: 30.000    Top5: 94.500    Loss: 1.688

2022-12-29 10:00:34,550 - ==> Confusion:
[[14  2  0 14  2  0]
 [ 3  9  2 20  8  0]
 [ 1  5  3 21  8  0]
 [ 0  3  1 30  5  0]
 [ 1  4  2 21  4  0]
 [ 4  2  3  6  2  0]]

2022-12-29 10:00:34,554 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:34,554 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:34,576 - 

2022-12-29 10:00:34,577 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:35,405 - Epoch: [40][   10/  113]    Overall Loss 1.684253    Objective Loss 1.684253                                        LR 0.000100    Time 0.082663    
2022-12-29 10:00:36,037 - Epoch: [40][   20/  113]    Overall Loss 1.613097    Objective Loss 1.613097                                        LR 0.000100    Time 0.072952    
2022-12-29 10:00:36,670 - Epoch: [40][   30/  113]    Overall Loss 1.602390    Objective Loss 1.602390                                        LR 0.000100    Time 0.069705    
2022-12-29 10:00:37,295 - Epoch: [40][   40/  113]    Overall Loss 1.578899    Objective Loss 1.578899                                        LR 0.000100    Time 0.067887    
2022-12-29 10:00:37,920 - Epoch: [40][   50/  113]    Overall Loss 1.569107    Objective Loss 1.569107                                        LR 0.000100    Time 0.066801    
2022-12-29 10:00:38,551 - Epoch: [40][   60/  113]    Overall Loss 1.567292    Objective Loss 1.567292                                        LR 0.000100    Time 0.066169    
2022-12-29 10:00:39,187 - Epoch: [40][   70/  113]    Overall Loss 1.563348    Objective Loss 1.563348                                        LR 0.000100    Time 0.065803    
2022-12-29 10:00:39,812 - Epoch: [40][   80/  113]    Overall Loss 1.559813    Objective Loss 1.559813                                        LR 0.000100    Time 0.065380    
2022-12-29 10:00:40,448 - Epoch: [40][   90/  113]    Overall Loss 1.564073    Objective Loss 1.564073                                        LR 0.000100    Time 0.065179    
2022-12-29 10:00:41,079 - Epoch: [40][  100/  113]    Overall Loss 1.561787    Objective Loss 1.561787                                        LR 0.000100    Time 0.064963    
2022-12-29 10:00:41,706 - Epoch: [40][  110/  113]    Overall Loss 1.557298    Objective Loss 1.557298                                        LR 0.000100    Time 0.064755    
2022-12-29 10:00:41,879 - Epoch: [40][  113/  113]    Overall Loss 1.559530    Objective Loss 1.559530    Top1 33.333333    Top5 100.000000    LR 0.000100    Time 0.064559    
2022-12-29 10:00:41,915 - --- validate (epoch=40)-----------
2022-12-29 10:00:41,915 - 200 samples (16 per mini-batch)
2022-12-29 10:00:42,455 - Epoch: [40][   10/   13]    Loss 1.574353    Top1 35.000000    Top5 93.125000    
2022-12-29 10:00:42,541 - Epoch: [40][   13/   13]    Loss 1.588352    Top1 34.000000    Top5 93.000000    
2022-12-29 10:00:42,584 - ==> Top1: 34.000    Top5: 93.000    Loss: 1.588

2022-12-29 10:00:42,584 - ==> Confusion:
[[16  4  0  2  4  0]
 [ 6 22  0  3 10  0]
 [ 3 11  0  9  5  0]
 [14  9  0 19  5  0]
 [ 7 18  1  9 11  0]
 [ 2  7  1  0  2  0]]

2022-12-29 10:00:42,587 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:42,587 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:42,599 - 

2022-12-29 10:00:42,599 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:43,445 - Epoch: [41][   10/  113]    Overall Loss 1.563386    Objective Loss 1.563386                                        LR 0.000100    Time 0.084473    
2022-12-29 10:00:44,082 - Epoch: [41][   20/  113]    Overall Loss 1.561012    Objective Loss 1.561012                                        LR 0.000100    Time 0.074061    
2022-12-29 10:00:44,718 - Epoch: [41][   30/  113]    Overall Loss 1.560859    Objective Loss 1.560859                                        LR 0.000100    Time 0.070579    
2022-12-29 10:00:45,346 - Epoch: [41][   40/  113]    Overall Loss 1.538964    Objective Loss 1.538964                                        LR 0.000100    Time 0.068610    
2022-12-29 10:00:45,970 - Epoch: [41][   50/  113]    Overall Loss 1.529077    Objective Loss 1.529077                                        LR 0.000100    Time 0.067364    
2022-12-29 10:00:46,599 - Epoch: [41][   60/  113]    Overall Loss 1.538465    Objective Loss 1.538465                                        LR 0.000100    Time 0.066612    
2022-12-29 10:00:47,229 - Epoch: [41][   70/  113]    Overall Loss 1.537065    Objective Loss 1.537065                                        LR 0.000100    Time 0.066088    
2022-12-29 10:00:47,854 - Epoch: [41][   80/  113]    Overall Loss 1.537310    Objective Loss 1.537310                                        LR 0.000100    Time 0.065639    
2022-12-29 10:00:48,484 - Epoch: [41][   90/  113]    Overall Loss 1.555178    Objective Loss 1.555178                                        LR 0.000100    Time 0.065335    
2022-12-29 10:00:49,112 - Epoch: [41][  100/  113]    Overall Loss 1.559038    Objective Loss 1.559038                                        LR 0.000100    Time 0.065079    
2022-12-29 10:00:49,729 - Epoch: [41][  110/  113]    Overall Loss 1.566591    Objective Loss 1.566591                                        LR 0.000100    Time 0.064771    
2022-12-29 10:00:49,905 - Epoch: [41][  113/  113]    Overall Loss 1.564796    Objective Loss 1.564796    Top1 41.666667    Top5 100.000000    LR 0.000100    Time 0.064603    
2022-12-29 10:00:49,965 - --- validate (epoch=41)-----------
2022-12-29 10:00:49,965 - 200 samples (16 per mini-batch)
2022-12-29 10:00:50,512 - Epoch: [41][   10/   13]    Loss 1.561562    Top1 32.500000    Top5 94.375000    
2022-12-29 10:00:50,597 - Epoch: [41][   13/   13]    Loss 1.598509    Top1 31.000000    Top5 94.000000    
2022-12-29 10:00:50,644 - ==> Top1: 31.000    Top5: 94.000    Loss: 1.599

2022-12-29 10:00:50,644 - ==> Confusion:
[[ 9  7  0 15  4  0]
 [ 2 12  2 18  8  0]
 [ 0  7  1 18  2  0]
 [ 0  5  1 34  2  0]
 [ 1  9  0 22  6  0]
 [ 1  8  0  5  1  0]]

2022-12-29 10:00:50,646 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:50,646 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:50,673 - 

2022-12-29 10:00:50,674 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:51,515 - Epoch: [42][   10/  113]    Overall Loss 1.515677    Objective Loss 1.515677                                        LR 0.000100    Time 0.083901    
2022-12-29 10:00:52,144 - Epoch: [42][   20/  113]    Overall Loss 1.541715    Objective Loss 1.541715                                        LR 0.000100    Time 0.073409    
2022-12-29 10:00:52,780 - Epoch: [42][   30/  113]    Overall Loss 1.548653    Objective Loss 1.548653                                        LR 0.000100    Time 0.070100    
2022-12-29 10:00:53,409 - Epoch: [42][   40/  113]    Overall Loss 1.539286    Objective Loss 1.539286                                        LR 0.000100    Time 0.068299    
2022-12-29 10:00:54,033 - Epoch: [42][   50/  113]    Overall Loss 1.547961    Objective Loss 1.547961                                        LR 0.000100    Time 0.067117    
2022-12-29 10:00:54,665 - Epoch: [42][   60/  113]    Overall Loss 1.544499    Objective Loss 1.544499                                        LR 0.000100    Time 0.066455    
2022-12-29 10:00:55,289 - Epoch: [42][   70/  113]    Overall Loss 1.554946    Objective Loss 1.554946                                        LR 0.000100    Time 0.065871    
2022-12-29 10:00:55,920 - Epoch: [42][   80/  113]    Overall Loss 1.559999    Objective Loss 1.559999                                        LR 0.000100    Time 0.065509    
2022-12-29 10:00:56,550 - Epoch: [42][   90/  113]    Overall Loss 1.565057    Objective Loss 1.565057                                        LR 0.000100    Time 0.065227    
2022-12-29 10:00:57,179 - Epoch: [42][  100/  113]    Overall Loss 1.575055    Objective Loss 1.575055                                        LR 0.000100    Time 0.064990    
2022-12-29 10:00:57,806 - Epoch: [42][  110/  113]    Overall Loss 1.574184    Objective Loss 1.574184                                        LR 0.000100    Time 0.064777    
2022-12-29 10:00:57,978 - Epoch: [42][  113/  113]    Overall Loss 1.575254    Objective Loss 1.575254    Top1 29.166667    Top5 87.500000    LR 0.000100    Time 0.064580    
2022-12-29 10:00:58,033 - --- validate (epoch=42)-----------
2022-12-29 10:00:58,034 - 200 samples (16 per mini-batch)
2022-12-29 10:00:58,574 - Epoch: [42][   10/   13]    Loss 1.582844    Top1 30.000000    Top5 95.625000    
2022-12-29 10:00:58,658 - Epoch: [42][   13/   13]    Loss 1.574723    Top1 31.500000    Top5 96.500000    
2022-12-29 10:00:58,721 - ==> Top1: 31.500    Top5: 96.500    Loss: 1.575

2022-12-29 10:00:58,722 - ==> Confusion:
[[14  3  0  6 10  0]
 [ 7 11  0  1 13  0]
 [ 3  8  6  6 13  0]
 [ 4 12  3 15 19  0]
 [ 3  9  1  5 17  0]
 [ 4  4  0  1  2  0]]

2022-12-29 10:00:58,725 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:00:58,725 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:00:58,739 - 

2022-12-29 10:00:58,739 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:00:59,576 - Epoch: [43][   10/  113]    Overall Loss 1.553470    Objective Loss 1.553470                                        LR 0.000100    Time 0.083607    
2022-12-29 10:01:00,211 - Epoch: [43][   20/  113]    Overall Loss 1.552297    Objective Loss 1.552297                                        LR 0.000100    Time 0.073500    
2022-12-29 10:01:00,846 - Epoch: [43][   30/  113]    Overall Loss 1.562096    Objective Loss 1.562096                                        LR 0.000100    Time 0.070171    
2022-12-29 10:01:01,480 - Epoch: [43][   40/  113]    Overall Loss 1.553887    Objective Loss 1.553887                                        LR 0.000100    Time 0.068442    
2022-12-29 10:01:02,112 - Epoch: [43][   50/  113]    Overall Loss 1.551672    Objective Loss 1.551672                                        LR 0.000100    Time 0.067395    
2022-12-29 10:01:02,743 - Epoch: [43][   60/  113]    Overall Loss 1.543417    Objective Loss 1.543417                                        LR 0.000100    Time 0.066669    
2022-12-29 10:01:03,375 - Epoch: [43][   70/  113]    Overall Loss 1.563253    Objective Loss 1.563253                                        LR 0.000100    Time 0.066166    
2022-12-29 10:01:04,005 - Epoch: [43][   80/  113]    Overall Loss 1.564809    Objective Loss 1.564809                                        LR 0.000100    Time 0.065765    
2022-12-29 10:01:04,637 - Epoch: [43][   90/  113]    Overall Loss 1.556664    Objective Loss 1.556664                                        LR 0.000100    Time 0.065477    
2022-12-29 10:01:05,269 - Epoch: [43][  100/  113]    Overall Loss 1.551879    Objective Loss 1.551879                                        LR 0.000100    Time 0.065242    
2022-12-29 10:01:05,893 - Epoch: [43][  110/  113]    Overall Loss 1.552329    Objective Loss 1.552329                                        LR 0.000100    Time 0.064982    
2022-12-29 10:01:06,068 - Epoch: [43][  113/  113]    Overall Loss 1.554575    Objective Loss 1.554575    Top1 33.333333    Top5 87.500000    LR 0.000100    Time 0.064800    
2022-12-29 10:01:06,114 - --- validate (epoch=43)-----------
2022-12-29 10:01:06,115 - 200 samples (16 per mini-batch)
2022-12-29 10:01:06,652 - Epoch: [43][   10/   13]    Loss 1.593721    Top1 30.625000    Top5 93.750000    
2022-12-29 10:01:06,735 - Epoch: [43][   13/   13]    Loss 1.576686    Top1 32.000000    Top5 93.500000    
2022-12-29 10:01:06,801 - ==> Top1: 32.000    Top5: 93.500    Loss: 1.577

2022-12-29 10:01:06,802 - ==> Confusion:
[[16  7  0  5  7  0]
 [ 4 20  1  5  5  0]
 [ 4 15  6  7  3  0]
 [ 5 18  1 16  7  0]
 [ 3 25  1  1  6  0]
 [ 5  5  0  0  2  0]]

2022-12-29 10:01:06,804 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:06,805 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:06,817 - 

2022-12-29 10:01:06,817 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:07,647 - Epoch: [44][   10/  113]    Overall Loss 1.523588    Objective Loss 1.523588                                        LR 0.000100    Time 0.082938    
2022-12-29 10:01:08,277 - Epoch: [44][   20/  113]    Overall Loss 1.564739    Objective Loss 1.564739                                        LR 0.000100    Time 0.072920    
2022-12-29 10:01:08,905 - Epoch: [44][   30/  113]    Overall Loss 1.585189    Objective Loss 1.585189                                        LR 0.000100    Time 0.069543    
2022-12-29 10:01:09,533 - Epoch: [44][   40/  113]    Overall Loss 1.579788    Objective Loss 1.579788                                        LR 0.000100    Time 0.067836    
2022-12-29 10:01:10,158 - Epoch: [44][   50/  113]    Overall Loss 1.559685    Objective Loss 1.559685                                        LR 0.000100    Time 0.066752    
2022-12-29 10:01:10,788 - Epoch: [44][   60/  113]    Overall Loss 1.547724    Objective Loss 1.547724                                        LR 0.000100    Time 0.066121    
2022-12-29 10:01:11,425 - Epoch: [44][   70/  113]    Overall Loss 1.551728    Objective Loss 1.551728                                        LR 0.000100    Time 0.065775    
2022-12-29 10:01:12,052 - Epoch: [44][   80/  113]    Overall Loss 1.553689    Objective Loss 1.553689                                        LR 0.000100    Time 0.065388    
2022-12-29 10:01:12,682 - Epoch: [44][   90/  113]    Overall Loss 1.550592    Objective Loss 1.550592                                        LR 0.000100    Time 0.065114    
2022-12-29 10:01:13,309 - Epoch: [44][  100/  113]    Overall Loss 1.545235    Objective Loss 1.545235                                        LR 0.000100    Time 0.064864    
2022-12-29 10:01:13,934 - Epoch: [44][  110/  113]    Overall Loss 1.546327    Objective Loss 1.546327                                        LR 0.000100    Time 0.064650    
2022-12-29 10:01:14,108 - Epoch: [44][  113/  113]    Overall Loss 1.550093    Objective Loss 1.550093    Top1 33.333333    Top5 87.500000    LR 0.000100    Time 0.064468    
2022-12-29 10:01:14,163 - --- validate (epoch=44)-----------
2022-12-29 10:01:14,164 - 200 samples (16 per mini-batch)
2022-12-29 10:01:14,707 - Epoch: [44][   10/   13]    Loss 1.653986    Top1 29.375000    Top5 93.750000    
2022-12-29 10:01:14,792 - Epoch: [44][   13/   13]    Loss 1.659261    Top1 30.000000    Top5 93.500000    
2022-12-29 10:01:14,845 - ==> Top1: 30.000    Top5: 93.500    Loss: 1.659

2022-12-29 10:01:14,845 - ==> Confusion:
[[13  9  0  5  6  0]
 [ 4 14  1 15  9  0]
 [ 4 10  0 10  4  0]
 [ 3  8  2 22  5  0]
 [ 5 15  4  7 11  0]
 [ 0  5  0  7  2  0]]

2022-12-29 10:01:14,848 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:14,849 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:14,867 - 

2022-12-29 10:01:14,868 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:15,694 - Epoch: [45][   10/  113]    Overall Loss 1.595359    Objective Loss 1.595359                                        LR 0.000100    Time 0.082432    
2022-12-29 10:01:16,328 - Epoch: [45][   20/  113]    Overall Loss 1.544275    Objective Loss 1.544275                                        LR 0.000100    Time 0.072882    
2022-12-29 10:01:16,963 - Epoch: [45][   30/  113]    Overall Loss 1.564716    Objective Loss 1.564716                                        LR 0.000100    Time 0.069745    
2022-12-29 10:01:17,594 - Epoch: [45][   40/  113]    Overall Loss 1.562996    Objective Loss 1.562996                                        LR 0.000100    Time 0.068073    
2022-12-29 10:01:18,224 - Epoch: [45][   50/  113]    Overall Loss 1.553366    Objective Loss 1.553366                                        LR 0.000100    Time 0.067036    
2022-12-29 10:01:18,851 - Epoch: [45][   60/  113]    Overall Loss 1.546911    Objective Loss 1.546911                                        LR 0.000100    Time 0.066309    
2022-12-29 10:01:19,480 - Epoch: [45][   70/  113]    Overall Loss 1.548615    Objective Loss 1.548615                                        LR 0.000100    Time 0.065818    
2022-12-29 10:01:20,106 - Epoch: [45][   80/  113]    Overall Loss 1.552775    Objective Loss 1.552775                                        LR 0.000100    Time 0.065405    
2022-12-29 10:01:20,733 - Epoch: [45][   90/  113]    Overall Loss 1.566951    Objective Loss 1.566951                                        LR 0.000100    Time 0.065101    
2022-12-29 10:01:21,365 - Epoch: [45][  100/  113]    Overall Loss 1.560487    Objective Loss 1.560487                                        LR 0.000100    Time 0.064907    
2022-12-29 10:01:21,987 - Epoch: [45][  110/  113]    Overall Loss 1.567743    Objective Loss 1.567743                                        LR 0.000100    Time 0.064660    
2022-12-29 10:01:22,160 - Epoch: [45][  113/  113]    Overall Loss 1.572930    Objective Loss 1.572930    Top1 33.333333    Top5 87.500000    LR 0.000100    Time 0.064466    
2022-12-29 10:01:22,220 - --- validate (epoch=45)-----------
2022-12-29 10:01:22,220 - 200 samples (16 per mini-batch)
2022-12-29 10:01:22,771 - Epoch: [45][   10/   13]    Loss 1.585407    Top1 34.375000    Top5 95.625000    
2022-12-29 10:01:22,855 - Epoch: [45][   13/   13]    Loss 1.539170    Top1 35.500000    Top5 96.500000    
2022-12-29 10:01:22,916 - ==> Top1: 35.500    Top5: 96.500    Loss: 1.539

2022-12-29 10:01:22,917 - ==> Confusion:
[[ 8  2  2  5 14  0]
 [ 1  7 12  7 20  0]
 [ 0  2 10  9  6  0]
 [ 1  2  7 27  8  0]
 [ 0  8  7  7 19  0]
 [ 1  0  3  2  3  0]]

2022-12-29 10:01:22,919 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:22,919 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:22,941 - 

2022-12-29 10:01:22,942 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:23,775 - Epoch: [46][   10/  113]    Overall Loss 1.557590    Objective Loss 1.557590                                        LR 0.000100    Time 0.083259    
2022-12-29 10:01:24,409 - Epoch: [46][   20/  113]    Overall Loss 1.587816    Objective Loss 1.587816                                        LR 0.000100    Time 0.073285    
2022-12-29 10:01:25,045 - Epoch: [46][   30/  113]    Overall Loss 1.617956    Objective Loss 1.617956                                        LR 0.000100    Time 0.070043    
2022-12-29 10:01:25,677 - Epoch: [46][   40/  113]    Overall Loss 1.592839    Objective Loss 1.592839                                        LR 0.000100    Time 0.068327    
2022-12-29 10:01:26,306 - Epoch: [46][   50/  113]    Overall Loss 1.598231    Objective Loss 1.598231                                        LR 0.000100    Time 0.067232    
2022-12-29 10:01:26,938 - Epoch: [46][   60/  113]    Overall Loss 1.602245    Objective Loss 1.602245                                        LR 0.000100    Time 0.066552    
2022-12-29 10:01:27,572 - Epoch: [46][   70/  113]    Overall Loss 1.590495    Objective Loss 1.590495                                        LR 0.000100    Time 0.066092    
2022-12-29 10:01:28,202 - Epoch: [46][   80/  113]    Overall Loss 1.580862    Objective Loss 1.580862                                        LR 0.000100    Time 0.065694    
2022-12-29 10:01:28,834 - Epoch: [46][   90/  113]    Overall Loss 1.584919    Objective Loss 1.584919                                        LR 0.000100    Time 0.065413    
2022-12-29 10:01:29,467 - Epoch: [46][  100/  113]    Overall Loss 1.578667    Objective Loss 1.578667                                        LR 0.000100    Time 0.065199    
2022-12-29 10:01:30,093 - Epoch: [46][  110/  113]    Overall Loss 1.576764    Objective Loss 1.576764                                        LR 0.000100    Time 0.064955    
2022-12-29 10:01:30,265 - Epoch: [46][  113/  113]    Overall Loss 1.579207    Objective Loss 1.579207    Top1 20.833333    Top5 95.833333    LR 0.000100    Time 0.064751    
2022-12-29 10:01:30,323 - --- validate (epoch=46)-----------
2022-12-29 10:01:30,324 - 200 samples (16 per mini-batch)
2022-12-29 10:01:30,871 - Epoch: [46][   10/   13]    Loss 1.605554    Top1 29.375000    Top5 96.250000    
2022-12-29 10:01:30,957 - Epoch: [46][   13/   13]    Loss 1.619261    Top1 31.000000    Top5 96.500000    
2022-12-29 10:01:31,017 - ==> Top1: 31.000    Top5: 96.500    Loss: 1.619

2022-12-29 10:01:31,018 - ==> Confusion:
[[18  4  0 12  2  0]
 [10 10  2 20  1  0]
 [ 2 10  1 21  0  0]
 [ 3  5  3 27  0  0]
 [ 8 10  3 13  6  0]
 [ 2  0  2  5  0  0]]

2022-12-29 10:01:31,020 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:31,020 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:31,036 - 

2022-12-29 10:01:31,036 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:31,864 - Epoch: [47][   10/  113]    Overall Loss 1.613286    Objective Loss 1.613286                                        LR 0.000100    Time 0.082660    
2022-12-29 10:01:32,499 - Epoch: [47][   20/  113]    Overall Loss 1.617756    Objective Loss 1.617756                                        LR 0.000100    Time 0.073081    
2022-12-29 10:01:33,133 - Epoch: [47][   30/  113]    Overall Loss 1.585742    Objective Loss 1.585742                                        LR 0.000100    Time 0.069839    
2022-12-29 10:01:33,766 - Epoch: [47][   40/  113]    Overall Loss 1.560885    Objective Loss 1.560885                                        LR 0.000100    Time 0.068195    
2022-12-29 10:01:34,394 - Epoch: [47][   50/  113]    Overall Loss 1.549880    Objective Loss 1.549880                                        LR 0.000100    Time 0.067102    
2022-12-29 10:01:35,022 - Epoch: [47][   60/  113]    Overall Loss 1.547755    Objective Loss 1.547755                                        LR 0.000100    Time 0.066371    
2022-12-29 10:01:35,654 - Epoch: [47][   70/  113]    Overall Loss 1.554109    Objective Loss 1.554109                                        LR 0.000100    Time 0.065918    
2022-12-29 10:01:36,280 - Epoch: [47][   80/  113]    Overall Loss 1.548363    Objective Loss 1.548363                                        LR 0.000100    Time 0.065499    
2022-12-29 10:01:36,916 - Epoch: [47][   90/  113]    Overall Loss 1.553636    Objective Loss 1.553636                                        LR 0.000100    Time 0.065276    
2022-12-29 10:01:37,546 - Epoch: [47][  100/  113]    Overall Loss 1.548456    Objective Loss 1.548456                                        LR 0.000100    Time 0.065046    
2022-12-29 10:01:38,167 - Epoch: [47][  110/  113]    Overall Loss 1.540900    Objective Loss 1.540900                                        LR 0.000100    Time 0.064773    
2022-12-29 10:01:38,340 - Epoch: [47][  113/  113]    Overall Loss 1.546878    Objective Loss 1.546878    Top1 37.500000    Top5 91.666667    LR 0.000100    Time 0.064581    
2022-12-29 10:01:38,389 - --- validate (epoch=47)-----------
2022-12-29 10:01:38,390 - 200 samples (16 per mini-batch)
2022-12-29 10:01:38,936 - Epoch: [47][   10/   13]    Loss 1.550193    Top1 35.000000    Top5 95.625000    
2022-12-29 10:01:39,024 - Epoch: [47][   13/   13]    Loss 1.534816    Top1 34.500000    Top5 96.500000    
2022-12-29 10:01:39,068 - ==> Top1: 34.500    Top5: 96.500    Loss: 1.535

2022-12-29 10:01:39,069 - ==> Confusion:
[[14  4  0  5  9  0]
 [ 6 10  4  5 10  0]
 [ 5 12  6  7 11  0]
 [ 6  8  2 26  7  0]
 [ 2  8  4  8 13  0]
 [ 2  3  0  1  2  0]]

2022-12-29 10:01:39,071 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:39,071 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:39,086 - 

2022-12-29 10:01:39,087 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:39,918 - Epoch: [48][   10/  113]    Overall Loss 1.629167    Objective Loss 1.629167                                        LR 0.000100    Time 0.083054    
2022-12-29 10:01:40,551 - Epoch: [48][   20/  113]    Overall Loss 1.572488    Objective Loss 1.572488                                        LR 0.000100    Time 0.073142    
2022-12-29 10:01:41,191 - Epoch: [48][   30/  113]    Overall Loss 1.556830    Objective Loss 1.556830                                        LR 0.000100    Time 0.070079    
2022-12-29 10:01:41,814 - Epoch: [48][   40/  113]    Overall Loss 1.551813    Objective Loss 1.551813                                        LR 0.000100    Time 0.068121    
2022-12-29 10:01:42,442 - Epoch: [48][   50/  113]    Overall Loss 1.544769    Objective Loss 1.544769                                        LR 0.000100    Time 0.067058    
2022-12-29 10:01:43,065 - Epoch: [48][   60/  113]    Overall Loss 1.554612    Objective Loss 1.554612                                        LR 0.000100    Time 0.066249    
2022-12-29 10:01:43,692 - Epoch: [48][   70/  113]    Overall Loss 1.544457    Objective Loss 1.544457                                        LR 0.000100    Time 0.065742    
2022-12-29 10:01:44,316 - Epoch: [48][   80/  113]    Overall Loss 1.549956    Objective Loss 1.549956                                        LR 0.000100    Time 0.065312    
2022-12-29 10:01:44,945 - Epoch: [48][   90/  113]    Overall Loss 1.550537    Objective Loss 1.550537                                        LR 0.000100    Time 0.065039    
2022-12-29 10:01:45,573 - Epoch: [48][  100/  113]    Overall Loss 1.550581    Objective Loss 1.550581                                        LR 0.000100    Time 0.064811    
2022-12-29 10:01:46,197 - Epoch: [48][  110/  113]    Overall Loss 1.544205    Objective Loss 1.544205                                        LR 0.000100    Time 0.064586    
2022-12-29 10:01:46,369 - Epoch: [48][  113/  113]    Overall Loss 1.546633    Objective Loss 1.546633    Top1 29.166667    Top5 91.666667    LR 0.000100    Time 0.064395    
2022-12-29 10:01:46,412 - --- validate (epoch=48)-----------
2022-12-29 10:01:46,413 - 200 samples (16 per mini-batch)
2022-12-29 10:01:46,951 - Epoch: [48][   10/   13]    Loss 1.558875    Top1 38.125000    Top5 93.750000    
2022-12-29 10:01:47,035 - Epoch: [48][   13/   13]    Loss 1.565497    Top1 37.500000    Top5 93.500000    
2022-12-29 10:01:47,100 - ==> Top1: 37.500    Top5: 93.500    Loss: 1.565

2022-12-29 10:01:47,101 - ==> Confusion:
[[21  3  0  7  3  0]
 [20 10  1  7  5  0]
 [ 8  3  2 14  0  0]
 [ 9  3  0 37  2  0]
 [12  2  3 13  5  0]
 [ 5  3  0  2  0  0]]

2022-12-29 10:01:47,103 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:47,103 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:47,116 - 

2022-12-29 10:01:47,116 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:47,934 - Epoch: [49][   10/  113]    Overall Loss 1.614264    Objective Loss 1.614264                                        LR 0.000100    Time 0.081719    
2022-12-29 10:01:48,569 - Epoch: [49][   20/  113]    Overall Loss 1.553211    Objective Loss 1.553211                                        LR 0.000100    Time 0.072580    
2022-12-29 10:01:49,212 - Epoch: [49][   30/  113]    Overall Loss 1.542508    Objective Loss 1.542508                                        LR 0.000100    Time 0.069806    
2022-12-29 10:01:49,840 - Epoch: [49][   40/  113]    Overall Loss 1.547173    Objective Loss 1.547173                                        LR 0.000100    Time 0.068040    
2022-12-29 10:01:50,467 - Epoch: [49][   50/  113]    Overall Loss 1.573513    Objective Loss 1.573513                                        LR 0.000100    Time 0.066959    
2022-12-29 10:01:51,100 - Epoch: [49][   60/  113]    Overall Loss 1.562842    Objective Loss 1.562842                                        LR 0.000100    Time 0.066335    
2022-12-29 10:01:51,729 - Epoch: [49][   70/  113]    Overall Loss 1.564314    Objective Loss 1.564314                                        LR 0.000100    Time 0.065839    
2022-12-29 10:01:52,357 - Epoch: [49][   80/  113]    Overall Loss 1.560390    Objective Loss 1.560390                                        LR 0.000100    Time 0.065455    
2022-12-29 10:01:52,986 - Epoch: [49][   90/  113]    Overall Loss 1.569894    Objective Loss 1.569894                                        LR 0.000100    Time 0.065169    
2022-12-29 10:01:53,616 - Epoch: [49][  100/  113]    Overall Loss 1.570298    Objective Loss 1.570298                                        LR 0.000100    Time 0.064947    
2022-12-29 10:01:54,234 - Epoch: [49][  110/  113]    Overall Loss 1.567912    Objective Loss 1.567912                                        LR 0.000100    Time 0.064652    
2022-12-29 10:01:54,412 - Epoch: [49][  113/  113]    Overall Loss 1.566868    Objective Loss 1.566868    Top1 50.000000    Top5 100.000000    LR 0.000100    Time 0.064507    
2022-12-29 10:01:54,459 - --- validate (epoch=49)-----------
2022-12-29 10:01:54,459 - 200 samples (16 per mini-batch)
2022-12-29 10:01:54,992 - Epoch: [49][   10/   13]    Loss 1.559019    Top1 31.250000    Top5 97.500000    
2022-12-29 10:01:55,077 - Epoch: [49][   13/   13]    Loss 1.534442    Top1 32.000000    Top5 97.500000    
2022-12-29 10:01:55,132 - ==> Top1: 32.000    Top5: 97.500    Loss: 1.534

2022-12-29 10:01:55,133 - ==> Confusion:
[[18  5  0  3  4  0]
 [ 5 15  0  5  5  0]
 [ 4 18  3 18  3  0]
 [ 3 13  0 25  7  0]
 [ 7 21  0  7  3  0]
 [ 2  1  1  2  2  0]]

2022-12-29 10:01:55,134 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:01:55,134 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:01:55,156 - 

2022-12-29 10:01:55,156 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:01:55,987 - Epoch: [50][   10/  113]    Overall Loss 1.558503    Objective Loss 1.558503                                        LR 0.000100    Time 0.082980    
2022-12-29 10:01:56,617 - Epoch: [50][   20/  113]    Overall Loss 1.531679    Objective Loss 1.531679                                        LR 0.000100    Time 0.072949    
2022-12-29 10:01:57,248 - Epoch: [50][   30/  113]    Overall Loss 1.542509    Objective Loss 1.542509                                        LR 0.000100    Time 0.069641    
2022-12-29 10:01:57,878 - Epoch: [50][   40/  113]    Overall Loss 1.533989    Objective Loss 1.533989                                        LR 0.000100    Time 0.067974    
2022-12-29 10:01:58,505 - Epoch: [50][   50/  113]    Overall Loss 1.542539    Objective Loss 1.542539                                        LR 0.000100    Time 0.066909    
2022-12-29 10:01:59,137 - Epoch: [50][   60/  113]    Overall Loss 1.549070    Objective Loss 1.549070                                        LR 0.000100    Time 0.066292    
2022-12-29 10:01:59,767 - Epoch: [50][   70/  113]    Overall Loss 1.561849    Objective Loss 1.561849                                        LR 0.000100    Time 0.065818    
2022-12-29 10:02:00,396 - Epoch: [50][   80/  113]    Overall Loss 1.566448    Objective Loss 1.566448                                        LR 0.000100    Time 0.065445    
2022-12-29 10:02:01,026 - Epoch: [50][   90/  113]    Overall Loss 1.563341    Objective Loss 1.563341                                        LR 0.000100    Time 0.065163    
2022-12-29 10:02:01,653 - Epoch: [50][  100/  113]    Overall Loss 1.557508    Objective Loss 1.557508                                        LR 0.000100    Time 0.064919    
2022-12-29 10:02:02,273 - Epoch: [50][  110/  113]    Overall Loss 1.557181    Objective Loss 1.557181                                        LR 0.000100    Time 0.064650    
2022-12-29 10:02:02,450 - Epoch: [50][  113/  113]    Overall Loss 1.559765    Objective Loss 1.559765    Top1 45.833333    Top5 95.833333    LR 0.000100    Time 0.064490    
2022-12-29 10:02:02,496 - --- validate (epoch=50)-----------
2022-12-29 10:02:02,497 - 200 samples (16 per mini-batch)
2022-12-29 10:02:03,053 - Epoch: [50][   10/   13]    Loss 1.575103    Top1 33.750000    Top5 96.875000    
2022-12-29 10:02:03,136 - Epoch: [50][   13/   13]    Loss 1.636643    Top1 31.500000    Top5 96.000000    
2022-12-29 10:02:03,187 - ==> Top1: 31.500    Top5: 96.000    Loss: 1.637

2022-12-29 10:02:03,187 - ==> Confusion:
[[13  5  4  4  3  0]
 [ 3 14  6  4  7  0]
 [ 4 14 13  8  2  0]
 [ 3  9  6 13  3  0]
 [ 5 18  7  8 10  0]
 [ 1  4  4  2  3  0]]

2022-12-29 10:02:03,190 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:02:03,190 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:03,213 - 

2022-12-29 10:02:03,213 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:04,046 - Epoch: [51][   10/  113]    Overall Loss 1.521665    Objective Loss 1.521665                                        LR 0.000100    Time 0.083132    
2022-12-29 10:02:04,679 - Epoch: [51][   20/  113]    Overall Loss 1.502604    Objective Loss 1.502604                                        LR 0.000100    Time 0.073218    
2022-12-29 10:02:05,309 - Epoch: [51][   30/  113]    Overall Loss 1.524537    Objective Loss 1.524537                                        LR 0.000100    Time 0.069801    
2022-12-29 10:02:05,937 - Epoch: [51][   40/  113]    Overall Loss 1.534100    Objective Loss 1.534100                                        LR 0.000100    Time 0.068027    
2022-12-29 10:02:06,564 - Epoch: [51][   50/  113]    Overall Loss 1.548523    Objective Loss 1.548523                                        LR 0.000100    Time 0.066947    
2022-12-29 10:02:07,193 - Epoch: [51][   60/  113]    Overall Loss 1.550939    Objective Loss 1.550939                                        LR 0.000100    Time 0.066270    
2022-12-29 10:02:07,822 - Epoch: [51][   70/  113]    Overall Loss 1.551872    Objective Loss 1.551872                                        LR 0.000100    Time 0.065778    
2022-12-29 10:02:08,451 - Epoch: [51][   80/  113]    Overall Loss 1.554871    Objective Loss 1.554871                                        LR 0.000100    Time 0.065412    
2022-12-29 10:02:09,078 - Epoch: [51][   90/  113]    Overall Loss 1.556345    Objective Loss 1.556345                                        LR 0.000100    Time 0.065111    
2022-12-29 10:02:09,708 - Epoch: [51][  100/  113]    Overall Loss 1.543990    Objective Loss 1.543990                                        LR 0.000100    Time 0.064890    
2022-12-29 10:02:10,332 - Epoch: [51][  110/  113]    Overall Loss 1.543544    Objective Loss 1.543544                                        LR 0.000100    Time 0.064665    
2022-12-29 10:02:10,504 - Epoch: [51][  113/  113]    Overall Loss 1.549253    Objective Loss 1.549253    Top1 29.166667    Top5 91.666667    LR 0.000100    Time 0.064469    
2022-12-29 10:02:10,564 - --- validate (epoch=51)-----------
2022-12-29 10:02:10,565 - 200 samples (16 per mini-batch)
2022-12-29 10:02:11,118 - Epoch: [51][   10/   13]    Loss 1.553915    Top1 36.250000    Top5 97.500000    
2022-12-29 10:02:11,202 - Epoch: [51][   13/   13]    Loss 1.559383    Top1 35.500000    Top5 96.500000    
2022-12-29 10:02:11,249 - ==> Top1: 35.500    Top5: 96.500    Loss: 1.559

2022-12-29 10:02:11,249 - ==> Confusion:
[[15  1  0  4  4  0]
 [12 11  0  5  6  0]
 [ 3  8  1 11 14  0]
 [10  5  0 19 11  0]
 [12  6  0  7 25  0]
 [ 3  3  1  0  3  0]]

2022-12-29 10:02:11,251 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:02:11,252 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:11,274 - 

2022-12-29 10:02:11,274 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:12,113 - Epoch: [52][   10/  113]    Overall Loss 1.578833    Objective Loss 1.578833                                        LR 0.000100    Time 0.083714    
2022-12-29 10:02:12,743 - Epoch: [52][   20/  113]    Overall Loss 1.613790    Objective Loss 1.613790                                        LR 0.000100    Time 0.073362    
2022-12-29 10:02:13,375 - Epoch: [52][   30/  113]    Overall Loss 1.590216    Objective Loss 1.590216                                        LR 0.000100    Time 0.069939    
2022-12-29 10:02:14,000 - Epoch: [52][   40/  113]    Overall Loss 1.565599    Objective Loss 1.565599                                        LR 0.000100    Time 0.068069    
2022-12-29 10:02:14,625 - Epoch: [52][   50/  113]    Overall Loss 1.563884    Objective Loss 1.563884                                        LR 0.000100    Time 0.066949    
2022-12-29 10:02:15,259 - Epoch: [52][   60/  113]    Overall Loss 1.564162    Objective Loss 1.564162                                        LR 0.000100    Time 0.066351    
2022-12-29 10:02:15,886 - Epoch: [52][   70/  113]    Overall Loss 1.562506    Objective Loss 1.562506                                        LR 0.000100    Time 0.065820    
2022-12-29 10:02:16,514 - Epoch: [52][   80/  113]    Overall Loss 1.557025    Objective Loss 1.557025                                        LR 0.000100    Time 0.065448    
2022-12-29 10:02:17,141 - Epoch: [52][   90/  113]    Overall Loss 1.549022    Objective Loss 1.549022                                        LR 0.000100    Time 0.065136    
2022-12-29 10:02:17,773 - Epoch: [52][  100/  113]    Overall Loss 1.541917    Objective Loss 1.541917                                        LR 0.000100    Time 0.064931    
2022-12-29 10:02:18,399 - Epoch: [52][  110/  113]    Overall Loss 1.534163    Objective Loss 1.534163                                        LR 0.000100    Time 0.064721    
2022-12-29 10:02:18,572 - Epoch: [52][  113/  113]    Overall Loss 1.538491    Objective Loss 1.538491    Top1 20.833333    Top5 95.833333    LR 0.000100    Time 0.064527    
2022-12-29 10:02:18,630 - --- validate (epoch=52)-----------
2022-12-29 10:02:18,630 - 200 samples (16 per mini-batch)
2022-12-29 10:02:19,173 - Epoch: [52][   10/   13]    Loss 1.539794    Top1 32.500000    Top5 96.875000    
2022-12-29 10:02:19,256 - Epoch: [52][   13/   13]    Loss 1.507225    Top1 35.000000    Top5 97.500000    
2022-12-29 10:02:19,303 - ==> Top1: 35.000    Top5: 97.500    Loss: 1.507

2022-12-29 10:02:19,303 - ==> Confusion:
[[13  7  0  0  8  0]
 [ 2 23  0  6  8  0]
 [ 1 15  9  4  5  0]
 [ 3 12  3 19  6  0]
 [ 4 31  2  4  6  0]
 [ 1  4  0  1  3  0]]

2022-12-29 10:02:19,306 - ==> Best [Top1: 37.500   Top5: 94.000   Sparsity:0.00   Params: 289216 on epoch: 35]
2022-12-29 10:02:19,306 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:19,329 - 

2022-12-29 10:02:19,330 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:20,176 - Epoch: [53][   10/  113]    Overall Loss 1.575918    Objective Loss 1.575918                                        LR 0.000100    Time 0.084510    
2022-12-29 10:02:20,804 - Epoch: [53][   20/  113]    Overall Loss 1.539521    Objective Loss 1.539521                                        LR 0.000100    Time 0.073643    
2022-12-29 10:02:21,434 - Epoch: [53][   30/  113]    Overall Loss 1.552495    Objective Loss 1.552495                                        LR 0.000100    Time 0.070069    
2022-12-29 10:02:22,060 - Epoch: [53][   40/  113]    Overall Loss 1.539786    Objective Loss 1.539786                                        LR 0.000100    Time 0.068200    
2022-12-29 10:02:22,687 - Epoch: [53][   50/  113]    Overall Loss 1.542907    Objective Loss 1.542907                                        LR 0.000100    Time 0.067090    
2022-12-29 10:02:23,312 - Epoch: [53][   60/  113]    Overall Loss 1.536923    Objective Loss 1.536923                                        LR 0.000100    Time 0.066313    
2022-12-29 10:02:23,943 - Epoch: [53][   70/  113]    Overall Loss 1.549950    Objective Loss 1.549950                                        LR 0.000100    Time 0.065849    
2022-12-29 10:02:24,574 - Epoch: [53][   80/  113]    Overall Loss 1.545627    Objective Loss 1.545627                                        LR 0.000100    Time 0.065503    
2022-12-29 10:02:25,206 - Epoch: [53][   90/  113]    Overall Loss 1.545137    Objective Loss 1.545137                                        LR 0.000100    Time 0.065239    
2022-12-29 10:02:25,836 - Epoch: [53][  100/  113]    Overall Loss 1.545523    Objective Loss 1.545523                                        LR 0.000100    Time 0.065007    
2022-12-29 10:02:26,466 - Epoch: [53][  110/  113]    Overall Loss 1.542505    Objective Loss 1.542505                                        LR 0.000100    Time 0.064821    
2022-12-29 10:02:26,640 - Epoch: [53][  113/  113]    Overall Loss 1.545724    Objective Loss 1.545724    Top1 33.333333    Top5 87.500000    LR 0.000100    Time 0.064644    
2022-12-29 10:02:26,693 - --- validate (epoch=53)-----------
2022-12-29 10:02:26,693 - 200 samples (16 per mini-batch)
2022-12-29 10:02:27,240 - Epoch: [53][   10/   13]    Loss 1.497584    Top1 40.000000    Top5 96.250000    
2022-12-29 10:02:27,327 - Epoch: [53][   13/   13]    Loss 1.444345    Top1 42.500000    Top5 96.500000    
2022-12-29 10:02:27,378 - ==> Top1: 42.500    Top5: 96.500    Loss: 1.444

2022-12-29 10:02:27,379 - ==> Confusion:
[[10  3  0 11  3  0]
 [ 3 18  0 11 13  0]
 [ 1  1  2 13  6  0]
 [ 1  4  1 43 10  0]
 [ 1  5  1 14 12  0]
 [ 0  5  0  5  3  0]]

2022-12-29 10:02:27,381 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:02:27,381 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:27,407 - 

2022-12-29 10:02:27,407 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:28,242 - Epoch: [54][   10/  113]    Overall Loss 1.543006    Objective Loss 1.543006                                        LR 0.000100    Time 0.083270    
2022-12-29 10:02:28,875 - Epoch: [54][   20/  113]    Overall Loss 1.532142    Objective Loss 1.532142                                        LR 0.000100    Time 0.073285    
2022-12-29 10:02:29,510 - Epoch: [54][   30/  113]    Overall Loss 1.528939    Objective Loss 1.528939                                        LR 0.000100    Time 0.070008    
2022-12-29 10:02:30,137 - Epoch: [54][   40/  113]    Overall Loss 1.538990    Objective Loss 1.538990                                        LR 0.000100    Time 0.068175    
2022-12-29 10:02:30,757 - Epoch: [54][   50/  113]    Overall Loss 1.531339    Objective Loss 1.531339                                        LR 0.000100    Time 0.066921    
2022-12-29 10:02:31,390 - Epoch: [54][   60/  113]    Overall Loss 1.531225    Objective Loss 1.531225                                        LR 0.000100    Time 0.066314    
2022-12-29 10:02:32,022 - Epoch: [54][   70/  113]    Overall Loss 1.534543    Objective Loss 1.534543                                        LR 0.000100    Time 0.065865    
2022-12-29 10:02:32,646 - Epoch: [54][   80/  113]    Overall Loss 1.528083    Objective Loss 1.528083                                        LR 0.000100    Time 0.065421    
2022-12-29 10:02:33,280 - Epoch: [54][   90/  113]    Overall Loss 1.538913    Objective Loss 1.538913                                        LR 0.000100    Time 0.065190    
2022-12-29 10:02:33,914 - Epoch: [54][  100/  113]    Overall Loss 1.537710    Objective Loss 1.537710                                        LR 0.000100    Time 0.065011    
2022-12-29 10:02:34,532 - Epoch: [54][  110/  113]    Overall Loss 1.532785    Objective Loss 1.532785                                        LR 0.000100    Time 0.064709    
2022-12-29 10:02:34,708 - Epoch: [54][  113/  113]    Overall Loss 1.536798    Objective Loss 1.536798    Top1 29.166667    Top5 95.833333    LR 0.000100    Time 0.064549    
2022-12-29 10:02:34,768 - --- validate (epoch=54)-----------
2022-12-29 10:02:34,768 - 200 samples (16 per mini-batch)
2022-12-29 10:02:35,330 - Epoch: [54][   10/   13]    Loss 1.456107    Top1 36.250000    Top5 98.125000    
2022-12-29 10:02:35,414 - Epoch: [54][   13/   13]    Loss 1.489576    Top1 37.500000    Top5 97.000000    
2022-12-29 10:02:35,459 - ==> Top1: 37.500    Top5: 97.000    Loss: 1.490

2022-12-29 10:02:35,460 - ==> Confusion:
[[10  5  0 10  3  0]
 [ 2 14  2 17  9  0]
 [ 1  6  5 18  8  0]
 [ 4  3  1 32  5  0]
 [ 2  6  0 17 14  0]
 [ 0  0  0  5  1  0]]

2022-12-29 10:02:35,463 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:02:35,463 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:35,477 - 

2022-12-29 10:02:35,477 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:36,303 - Epoch: [55][   10/  113]    Overall Loss 1.516243    Objective Loss 1.516243                                        LR 0.000100    Time 0.082488    
2022-12-29 10:02:36,936 - Epoch: [55][   20/  113]    Overall Loss 1.528110    Objective Loss 1.528110                                        LR 0.000100    Time 0.072882    
2022-12-29 10:02:37,567 - Epoch: [55][   30/  113]    Overall Loss 1.550114    Objective Loss 1.550114                                        LR 0.000100    Time 0.069604    
2022-12-29 10:02:38,197 - Epoch: [55][   40/  113]    Overall Loss 1.538164    Objective Loss 1.538164                                        LR 0.000100    Time 0.067941    
2022-12-29 10:02:38,828 - Epoch: [55][   50/  113]    Overall Loss 1.550253    Objective Loss 1.550253                                        LR 0.000100    Time 0.066961    
2022-12-29 10:02:39,459 - Epoch: [55][   60/  113]    Overall Loss 1.568526    Objective Loss 1.568526                                        LR 0.000100    Time 0.066312    
2022-12-29 10:02:40,089 - Epoch: [55][   70/  113]    Overall Loss 1.575125    Objective Loss 1.575125                                        LR 0.000100    Time 0.065827    
2022-12-29 10:02:40,715 - Epoch: [55][   80/  113]    Overall Loss 1.566731    Objective Loss 1.566731                                        LR 0.000100    Time 0.065423    
2022-12-29 10:02:41,348 - Epoch: [55][   90/  113]    Overall Loss 1.556167    Objective Loss 1.556167                                        LR 0.000100    Time 0.065174    
2022-12-29 10:02:41,974 - Epoch: [55][  100/  113]    Overall Loss 1.558943    Objective Loss 1.558943                                        LR 0.000100    Time 0.064916    
2022-12-29 10:02:42,602 - Epoch: [55][  110/  113]    Overall Loss 1.557494    Objective Loss 1.557494                                        LR 0.000100    Time 0.064716    
2022-12-29 10:02:42,772 - Epoch: [55][  113/  113]    Overall Loss 1.556023    Objective Loss 1.556023    Top1 45.833333    Top5 100.000000    LR 0.000100    Time 0.064503    
2022-12-29 10:02:42,825 - --- validate (epoch=55)-----------
2022-12-29 10:02:42,825 - 200 samples (16 per mini-batch)
2022-12-29 10:02:43,360 - Epoch: [55][   10/   13]    Loss 1.496693    Top1 37.500000    Top5 98.125000    
2022-12-29 10:02:43,444 - Epoch: [55][   13/   13]    Loss 1.501600    Top1 38.000000    Top5 98.000000    
2022-12-29 10:02:43,495 - ==> Top1: 38.000    Top5: 98.000    Loss: 1.502

2022-12-29 10:02:43,495 - ==> Confusion:
[[17  6  3  4  4  0]
 [ 8 12  7  5 15  0]
 [ 2  4  7  3  5  0]
 [ 6  8 10 23  4  0]
 [ 6  9  5  2 17  0]
 [ 1  4  1  1  1  0]]

2022-12-29 10:02:43,497 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:02:43,498 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:43,521 - 

2022-12-29 10:02:43,521 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:44,340 - Epoch: [56][   10/  113]    Overall Loss 1.589708    Objective Loss 1.589708                                        LR 0.000100    Time 0.081767    
2022-12-29 10:02:44,973 - Epoch: [56][   20/  113]    Overall Loss 1.567515    Objective Loss 1.567515                                        LR 0.000100    Time 0.072517    
2022-12-29 10:02:45,606 - Epoch: [56][   30/  113]    Overall Loss 1.541666    Objective Loss 1.541666                                        LR 0.000100    Time 0.069436    
2022-12-29 10:02:46,237 - Epoch: [56][   40/  113]    Overall Loss 1.509816    Objective Loss 1.509816                                        LR 0.000100    Time 0.067836    
2022-12-29 10:02:46,860 - Epoch: [56][   50/  113]    Overall Loss 1.507245    Objective Loss 1.507245                                        LR 0.000100    Time 0.066730    
2022-12-29 10:02:47,485 - Epoch: [56][   60/  113]    Overall Loss 1.506277    Objective Loss 1.506277                                        LR 0.000100    Time 0.066016    
2022-12-29 10:02:48,121 - Epoch: [56][   70/  113]    Overall Loss 1.521449    Objective Loss 1.521449                                        LR 0.000100    Time 0.065656    
2022-12-29 10:02:48,745 - Epoch: [56][   80/  113]    Overall Loss 1.521699    Objective Loss 1.521699                                        LR 0.000100    Time 0.065250    
2022-12-29 10:02:49,376 - Epoch: [56][   90/  113]    Overall Loss 1.514983    Objective Loss 1.514983                                        LR 0.000100    Time 0.064999    
2022-12-29 10:02:50,007 - Epoch: [56][  100/  113]    Overall Loss 1.515378    Objective Loss 1.515378                                        LR 0.000100    Time 0.064810    
2022-12-29 10:02:50,635 - Epoch: [56][  110/  113]    Overall Loss 1.519533    Objective Loss 1.519533                                        LR 0.000100    Time 0.064625    
2022-12-29 10:02:50,805 - Epoch: [56][  113/  113]    Overall Loss 1.519466    Objective Loss 1.519466    Top1 37.500000    Top5 100.000000    LR 0.000100    Time 0.064407    
2022-12-29 10:02:50,861 - --- validate (epoch=56)-----------
2022-12-29 10:02:50,861 - 200 samples (16 per mini-batch)
2022-12-29 10:02:51,395 - Epoch: [56][   10/   13]    Loss 1.646610    Top1 33.125000    Top5 95.000000    
2022-12-29 10:02:51,480 - Epoch: [56][   13/   13]    Loss 1.593475    Top1 35.000000    Top5 95.000000    
2022-12-29 10:02:51,543 - ==> Top1: 35.000    Top5: 95.000    Loss: 1.593

2022-12-29 10:02:51,543 - ==> Confusion:
[[ 8 11  3  1  7  0]
 [ 3 15 12  1  8  0]
 [ 2  7 17  3  6  0]
 [ 0 10 14 14 11  0]
 [ 1  5  5  7 16  0]
 [ 2  8  0  1  2  0]]

2022-12-29 10:02:51,546 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:02:51,546 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:51,559 - 

2022-12-29 10:02:51,560 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:02:52,394 - Epoch: [57][   10/  113]    Overall Loss 1.552630    Objective Loss 1.552630                                        LR 0.000100    Time 0.083323    
2022-12-29 10:02:53,025 - Epoch: [57][   20/  113]    Overall Loss 1.548858    Objective Loss 1.548858                                        LR 0.000100    Time 0.073190    
2022-12-29 10:02:53,651 - Epoch: [57][   30/  113]    Overall Loss 1.546044    Objective Loss 1.546044                                        LR 0.000100    Time 0.069658    
2022-12-29 10:02:54,278 - Epoch: [57][   40/  113]    Overall Loss 1.527159    Objective Loss 1.527159                                        LR 0.000100    Time 0.067902    
2022-12-29 10:02:54,899 - Epoch: [57][   50/  113]    Overall Loss 1.525744    Objective Loss 1.525744                                        LR 0.000100    Time 0.066730    
2022-12-29 10:02:55,521 - Epoch: [57][   60/  113]    Overall Loss 1.531119    Objective Loss 1.531119                                        LR 0.000100    Time 0.065961    
2022-12-29 10:02:56,150 - Epoch: [57][   70/  113]    Overall Loss 1.538432    Objective Loss 1.538432                                        LR 0.000100    Time 0.065522    
2022-12-29 10:02:56,777 - Epoch: [57][   80/  113]    Overall Loss 1.538282    Objective Loss 1.538282                                        LR 0.000100    Time 0.065161    
2022-12-29 10:02:57,407 - Epoch: [57][   90/  113]    Overall Loss 1.526743    Objective Loss 1.526743                                        LR 0.000100    Time 0.064921    
2022-12-29 10:02:58,036 - Epoch: [57][  100/  113]    Overall Loss 1.513856    Objective Loss 1.513856                                        LR 0.000100    Time 0.064708    
2022-12-29 10:02:58,663 - Epoch: [57][  110/  113]    Overall Loss 1.509550    Objective Loss 1.509550                                        LR 0.000100    Time 0.064522    
2022-12-29 10:02:58,833 - Epoch: [57][  113/  113]    Overall Loss 1.509937    Objective Loss 1.509937    Top1 37.500000    Top5 100.000000    LR 0.000100    Time 0.064313    
2022-12-29 10:02:58,889 - --- validate (epoch=57)-----------
2022-12-29 10:02:58,889 - 200 samples (16 per mini-batch)
2022-12-29 10:02:59,436 - Epoch: [57][   10/   13]    Loss 1.528096    Top1 40.000000    Top5 95.000000    
2022-12-29 10:02:59,521 - Epoch: [57][   13/   13]    Loss 1.510491    Top1 40.500000    Top5 94.500000    
2022-12-29 10:02:59,562 - ==> Top1: 40.500    Top5: 94.500    Loss: 1.510

2022-12-29 10:02:59,562 - ==> Confusion:
[[16  5  0  9  3  0]
 [ 2 13  3 15 14  0]
 [ 1  2  5  9 10  0]
 [ 2  5  7 33  4  0]
 [ 2  5  1 10 14  0]
 [ 1  1  1  4  3  0]]

2022-12-29 10:02:59,564 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:02:59,564 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:02:59,593 - 

2022-12-29 10:02:59,593 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:00,433 - Epoch: [58][   10/  113]    Overall Loss 1.639069    Objective Loss 1.639069                                        LR 0.000100    Time 0.083813    
2022-12-29 10:03:01,066 - Epoch: [58][   20/  113]    Overall Loss 1.574122    Objective Loss 1.574122                                        LR 0.000100    Time 0.073528    
2022-12-29 10:03:01,700 - Epoch: [58][   30/  113]    Overall Loss 1.577023    Objective Loss 1.577023                                        LR 0.000100    Time 0.070125    
2022-12-29 10:03:02,332 - Epoch: [58][   40/  113]    Overall Loss 1.567093    Objective Loss 1.567093                                        LR 0.000100    Time 0.068395    
2022-12-29 10:03:02,955 - Epoch: [58][   50/  113]    Overall Loss 1.537592    Objective Loss 1.537592                                        LR 0.000100    Time 0.067158    
2022-12-29 10:03:03,579 - Epoch: [58][   60/  113]    Overall Loss 1.526521    Objective Loss 1.526521                                        LR 0.000100    Time 0.066354    
2022-12-29 10:03:04,208 - Epoch: [58][   70/  113]    Overall Loss 1.540179    Objective Loss 1.540179                                        LR 0.000100    Time 0.065855    
2022-12-29 10:03:04,833 - Epoch: [58][   80/  113]    Overall Loss 1.557001    Objective Loss 1.557001                                        LR 0.000100    Time 0.065433    
2022-12-29 10:03:05,461 - Epoch: [58][   90/  113]    Overall Loss 1.552284    Objective Loss 1.552284                                        LR 0.000100    Time 0.065134    
2022-12-29 10:03:06,087 - Epoch: [58][  100/  113]    Overall Loss 1.552719    Objective Loss 1.552719                                        LR 0.000100    Time 0.064882    
2022-12-29 10:03:06,706 - Epoch: [58][  110/  113]    Overall Loss 1.549599    Objective Loss 1.549599                                        LR 0.000100    Time 0.064606    
2022-12-29 10:03:06,881 - Epoch: [58][  113/  113]    Overall Loss 1.550122    Objective Loss 1.550122    Top1 45.833333    Top5 95.833333    LR 0.000100    Time 0.064437    
2022-12-29 10:03:06,927 - --- validate (epoch=58)-----------
2022-12-29 10:03:06,927 - 200 samples (16 per mini-batch)
2022-12-29 10:03:07,484 - Epoch: [58][   10/   13]    Loss 1.479634    Top1 37.500000    Top5 95.625000    
2022-12-29 10:03:07,570 - Epoch: [58][   13/   13]    Loss 1.520885    Top1 36.500000    Top5 94.500000    
2022-12-29 10:03:07,619 - ==> Top1: 36.500    Top5: 94.500    Loss: 1.521

2022-12-29 10:03:07,620 - ==> Confusion:
[[11  5  0  4  4  0]
 [ 2 15  2 21  5  0]
 [ 0 10  6 12  5  0]
 [ 3 10  0 38  4  0]
 [ 3 12  3 11  3  0]
 [ 1  4  1  3  2  0]]

2022-12-29 10:03:07,622 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:07,622 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:07,649 - 

2022-12-29 10:03:07,650 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:08,473 - Epoch: [59][   10/  113]    Overall Loss 1.461667    Objective Loss 1.461667                                        LR 0.000100    Time 0.082166    
2022-12-29 10:03:09,103 - Epoch: [59][   20/  113]    Overall Loss 1.486390    Objective Loss 1.486390                                        LR 0.000100    Time 0.072533    
2022-12-29 10:03:09,729 - Epoch: [59][   30/  113]    Overall Loss 1.509024    Objective Loss 1.509024                                        LR 0.000100    Time 0.069212    
2022-12-29 10:03:10,360 - Epoch: [59][   40/  113]    Overall Loss 1.505122    Objective Loss 1.505122                                        LR 0.000100    Time 0.067678    
2022-12-29 10:03:10,984 - Epoch: [59][   50/  113]    Overall Loss 1.524109    Objective Loss 1.524109                                        LR 0.000100    Time 0.066621    
2022-12-29 10:03:11,618 - Epoch: [59][   60/  113]    Overall Loss 1.522556    Objective Loss 1.522556                                        LR 0.000100    Time 0.066059    
2022-12-29 10:03:12,244 - Epoch: [59][   70/  113]    Overall Loss 1.523781    Objective Loss 1.523781                                        LR 0.000100    Time 0.065570    
2022-12-29 10:03:12,873 - Epoch: [59][   80/  113]    Overall Loss 1.529857    Objective Loss 1.529857                                        LR 0.000100    Time 0.065232    
2022-12-29 10:03:13,508 - Epoch: [59][   90/  113]    Overall Loss 1.525797    Objective Loss 1.525797                                        LR 0.000100    Time 0.065026    
2022-12-29 10:03:14,137 - Epoch: [59][  100/  113]    Overall Loss 1.523518    Objective Loss 1.523518                                        LR 0.000100    Time 0.064811    
2022-12-29 10:03:14,764 - Epoch: [59][  110/  113]    Overall Loss 1.522115    Objective Loss 1.522115                                        LR 0.000100    Time 0.064621    
2022-12-29 10:03:14,935 - Epoch: [59][  113/  113]    Overall Loss 1.526981    Objective Loss 1.526981    Top1 37.500000    Top5 87.500000    LR 0.000100    Time 0.064414    
2022-12-29 10:03:15,003 - --- validate (epoch=59)-----------
2022-12-29 10:03:15,004 - 200 samples (16 per mini-batch)
2022-12-29 10:03:15,548 - Epoch: [59][   10/   13]    Loss 1.530588    Top1 40.000000    Top5 95.625000    
2022-12-29 10:03:15,632 - Epoch: [59][   13/   13]    Loss 1.546403    Top1 39.500000    Top5 95.500000    
2022-12-29 10:03:15,686 - ==> Top1: 39.500    Top5: 95.500    Loss: 1.546

2022-12-29 10:03:15,686 - ==> Confusion:
[[19  4  0  6  1  0]
 [10 12  0  8  6  0]
 [11  5  6  4  1  0]
 [10  9  3 35  5  0]
 [ 9 12  1  5  7  0]
 [ 2  4  1  3  1  0]]

2022-12-29 10:03:15,688 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:15,689 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:15,698 - 

2022-12-29 10:03:15,698 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:16,533 - Epoch: [60][   10/  113]    Overall Loss 1.541359    Objective Loss 1.541359                                        LR 0.000100    Time 0.083401    
2022-12-29 10:03:17,165 - Epoch: [60][   20/  113]    Overall Loss 1.559742    Objective Loss 1.559742                                        LR 0.000100    Time 0.073241    
2022-12-29 10:03:17,795 - Epoch: [60][   30/  113]    Overall Loss 1.569419    Objective Loss 1.569419                                        LR 0.000100    Time 0.069812    
2022-12-29 10:03:18,423 - Epoch: [60][   40/  113]    Overall Loss 1.570591    Objective Loss 1.570591                                        LR 0.000100    Time 0.068063    
2022-12-29 10:03:19,048 - Epoch: [60][   50/  113]    Overall Loss 1.567063    Objective Loss 1.567063                                        LR 0.000100    Time 0.066934    
2022-12-29 10:03:19,674 - Epoch: [60][   60/  113]    Overall Loss 1.563736    Objective Loss 1.563736                                        LR 0.000100    Time 0.066204    
2022-12-29 10:03:20,309 - Epoch: [60][   70/  113]    Overall Loss 1.564278    Objective Loss 1.564278                                        LR 0.000100    Time 0.065813    
2022-12-29 10:03:20,933 - Epoch: [60][   80/  113]    Overall Loss 1.553286    Objective Loss 1.553286                                        LR 0.000100    Time 0.065385    
2022-12-29 10:03:21,570 - Epoch: [60][   90/  113]    Overall Loss 1.549525    Objective Loss 1.549525                                        LR 0.000100    Time 0.065185    
2022-12-29 10:03:22,202 - Epoch: [60][  100/  113]    Overall Loss 1.540828    Objective Loss 1.540828                                        LR 0.000100    Time 0.064987    
2022-12-29 10:03:22,828 - Epoch: [60][  110/  113]    Overall Loss 1.540841    Objective Loss 1.540841                                        LR 0.000100    Time 0.064768    
2022-12-29 10:03:22,999 - Epoch: [60][  113/  113]    Overall Loss 1.538610    Objective Loss 1.538610    Top1 45.833333    Top5 100.000000    LR 0.000100    Time 0.064556    
2022-12-29 10:03:23,051 - --- validate (epoch=60)-----------
2022-12-29 10:03:23,051 - 200 samples (16 per mini-batch)
2022-12-29 10:03:23,590 - Epoch: [60][   10/   13]    Loss 1.497853    Top1 37.500000    Top5 96.250000    
2022-12-29 10:03:23,673 - Epoch: [60][   13/   13]    Loss 1.511853    Top1 37.500000    Top5 95.000000    
2022-12-29 10:03:23,733 - ==> Top1: 37.500    Top5: 95.000    Loss: 1.512

2022-12-29 10:03:23,733 - ==> Confusion:
[[12  5  0  6  4  0]
 [ 1 18  0 17  8  0]
 [ 1 14  1 10  5  0]
 [ 0  8  0 38  6  0]
 [ 1  6  0 21  6  0]
 [ 1  5  0  5  1  0]]

2022-12-29 10:03:23,735 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:23,735 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:23,761 - 

2022-12-29 10:03:23,762 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:24,594 - Epoch: [61][   10/  113]    Overall Loss 1.492461    Objective Loss 1.492461                                        LR 0.000100    Time 0.083028    
2022-12-29 10:03:25,223 - Epoch: [61][   20/  113]    Overall Loss 1.518392    Objective Loss 1.518392                                        LR 0.000100    Time 0.072959    
2022-12-29 10:03:25,858 - Epoch: [61][   30/  113]    Overall Loss 1.512666    Objective Loss 1.512666                                        LR 0.000100    Time 0.069790    
2022-12-29 10:03:26,490 - Epoch: [61][   40/  113]    Overall Loss 1.511155    Objective Loss 1.511155                                        LR 0.000100    Time 0.068112    
2022-12-29 10:03:27,113 - Epoch: [61][   50/  113]    Overall Loss 1.498468    Objective Loss 1.498468                                        LR 0.000100    Time 0.066959    
2022-12-29 10:03:27,743 - Epoch: [61][   60/  113]    Overall Loss 1.495939    Objective Loss 1.495939                                        LR 0.000100    Time 0.066287    
2022-12-29 10:03:28,379 - Epoch: [61][   70/  113]    Overall Loss 1.510997    Objective Loss 1.510997                                        LR 0.000100    Time 0.065888    
2022-12-29 10:03:29,010 - Epoch: [61][   80/  113]    Overall Loss 1.508779    Objective Loss 1.508779                                        LR 0.000100    Time 0.065538    
2022-12-29 10:03:29,633 - Epoch: [61][   90/  113]    Overall Loss 1.508598    Objective Loss 1.508598                                        LR 0.000100    Time 0.065180    
2022-12-29 10:03:30,263 - Epoch: [61][  100/  113]    Overall Loss 1.502797    Objective Loss 1.502797                                        LR 0.000100    Time 0.064954    
2022-12-29 10:03:30,890 - Epoch: [61][  110/  113]    Overall Loss 1.515442    Objective Loss 1.515442                                        LR 0.000100    Time 0.064739    
2022-12-29 10:03:31,066 - Epoch: [61][  113/  113]    Overall Loss 1.512570    Objective Loss 1.512570    Top1 37.500000    Top5 100.000000    LR 0.000100    Time 0.064584    
2022-12-29 10:03:31,120 - --- validate (epoch=61)-----------
2022-12-29 10:03:31,120 - 200 samples (16 per mini-batch)
2022-12-29 10:03:31,667 - Epoch: [61][   10/   13]    Loss 1.547700    Top1 35.000000    Top5 96.875000    
2022-12-29 10:03:31,753 - Epoch: [61][   13/   13]    Loss 1.585103    Top1 33.000000    Top5 96.500000    
2022-12-29 10:03:31,808 - ==> Top1: 33.000    Top5: 96.500    Loss: 1.585

2022-12-29 10:03:31,808 - ==> Confusion:
[[13  4  0 12  7  0]
 [ 3  7  1 19 11  0]
 [ 1  6  2 13  8  0]
 [ 1  1  2 24 10  0]
 [ 2  8  2 14 20  0]
 [ 2  2  1  1  3  0]]

2022-12-29 10:03:31,810 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:31,810 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:31,833 - 

2022-12-29 10:03:31,833 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:32,658 - Epoch: [62][   10/  113]    Overall Loss 1.549803    Objective Loss 1.549803                                        LR 0.000100    Time 0.082334    
2022-12-29 10:03:33,292 - Epoch: [62][   20/  113]    Overall Loss 1.540741    Objective Loss 1.540741                                        LR 0.000100    Time 0.072829    
2022-12-29 10:03:33,924 - Epoch: [62][   30/  113]    Overall Loss 1.560668    Objective Loss 1.560668                                        LR 0.000100    Time 0.069629    
2022-12-29 10:03:34,553 - Epoch: [62][   40/  113]    Overall Loss 1.536237    Objective Loss 1.536237                                        LR 0.000100    Time 0.067928    
2022-12-29 10:03:35,177 - Epoch: [62][   50/  113]    Overall Loss 1.530905    Objective Loss 1.530905                                        LR 0.000100    Time 0.066819    
2022-12-29 10:03:35,804 - Epoch: [62][   60/  113]    Overall Loss 1.533625    Objective Loss 1.533625                                        LR 0.000100    Time 0.066120    
2022-12-29 10:03:36,434 - Epoch: [62][   70/  113]    Overall Loss 1.533909    Objective Loss 1.533909                                        LR 0.000100    Time 0.065672    
2022-12-29 10:03:37,061 - Epoch: [62][   80/  113]    Overall Loss 1.526687    Objective Loss 1.526687                                        LR 0.000100    Time 0.065289    
2022-12-29 10:03:37,687 - Epoch: [62][   90/  113]    Overall Loss 1.527815    Objective Loss 1.527815                                        LR 0.000100    Time 0.064982    
2022-12-29 10:03:38,319 - Epoch: [62][  100/  113]    Overall Loss 1.523077    Objective Loss 1.523077                                        LR 0.000100    Time 0.064804    
2022-12-29 10:03:38,947 - Epoch: [62][  110/  113]    Overall Loss 1.521070    Objective Loss 1.521070                                        LR 0.000100    Time 0.064614    
2022-12-29 10:03:39,124 - Epoch: [62][  113/  113]    Overall Loss 1.521879    Objective Loss 1.521879    Top1 58.333333    Top5 95.833333    LR 0.000100    Time 0.064460    
2022-12-29 10:03:39,164 - --- validate (epoch=62)-----------
2022-12-29 10:03:39,165 - 200 samples (16 per mini-batch)
2022-12-29 10:03:39,702 - Epoch: [62][   10/   13]    Loss 1.491057    Top1 37.500000    Top5 96.875000    
2022-12-29 10:03:39,787 - Epoch: [62][   13/   13]    Loss 1.489872    Top1 37.500000    Top5 96.000000    
2022-12-29 10:03:39,841 - ==> Top1: 37.500    Top5: 96.000    Loss: 1.490

2022-12-29 10:03:39,841 - ==> Confusion:
[[12  4  0  8  2  0]
 [ 0 14  1 18 10  0]
 [ 1 12  2 18  6  0]
 [ 1  7  1 34  8  0]
 [ 0  7  1 11 13  0]
 [ 1  4  0  2  2  0]]

2022-12-29 10:03:39,844 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:39,845 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:39,865 - 

2022-12-29 10:03:39,866 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:40,674 - Epoch: [63][   10/  113]    Overall Loss 1.587068    Objective Loss 1.587068                                        LR 0.000100    Time 0.080650    
2022-12-29 10:03:41,303 - Epoch: [63][   20/  113]    Overall Loss 1.535031    Objective Loss 1.535031                                        LR 0.000100    Time 0.071741    
2022-12-29 10:03:41,933 - Epoch: [63][   30/  113]    Overall Loss 1.550885    Objective Loss 1.550885                                        LR 0.000100    Time 0.068830    
2022-12-29 10:03:42,560 - Epoch: [63][   40/  113]    Overall Loss 1.553084    Objective Loss 1.553084                                        LR 0.000100    Time 0.067277    
2022-12-29 10:03:43,184 - Epoch: [63][   50/  113]    Overall Loss 1.558011    Objective Loss 1.558011                                        LR 0.000100    Time 0.066296    
2022-12-29 10:03:43,810 - Epoch: [63][   60/  113]    Overall Loss 1.543606    Objective Loss 1.543606                                        LR 0.000100    Time 0.065660    
2022-12-29 10:03:44,437 - Epoch: [63][   70/  113]    Overall Loss 1.528537    Objective Loss 1.528537                                        LR 0.000100    Time 0.065241    
2022-12-29 10:03:45,063 - Epoch: [63][   80/  113]    Overall Loss 1.518871    Objective Loss 1.518871                                        LR 0.000100    Time 0.064899    
2022-12-29 10:03:45,691 - Epoch: [63][   90/  113]    Overall Loss 1.519292    Objective Loss 1.519292                                        LR 0.000100    Time 0.064669    
2022-12-29 10:03:46,314 - Epoch: [63][  100/  113]    Overall Loss 1.512305    Objective Loss 1.512305                                        LR 0.000100    Time 0.064425    
2022-12-29 10:03:46,943 - Epoch: [63][  110/  113]    Overall Loss 1.516436    Objective Loss 1.516436                                        LR 0.000100    Time 0.064276    
2022-12-29 10:03:47,118 - Epoch: [63][  113/  113]    Overall Loss 1.514797    Objective Loss 1.514797    Top1 54.166667    Top5 95.833333    LR 0.000100    Time 0.064119    
2022-12-29 10:03:47,179 - --- validate (epoch=63)-----------
2022-12-29 10:03:47,179 - 200 samples (16 per mini-batch)
2022-12-29 10:03:47,717 - Epoch: [63][   10/   13]    Loss 1.498497    Top1 39.375000    Top5 96.250000    
2022-12-29 10:03:47,802 - Epoch: [63][   13/   13]    Loss 1.491523    Top1 39.500000    Top5 97.000000    
2022-12-29 10:03:47,850 - ==> Top1: 39.500    Top5: 97.000    Loss: 1.492

2022-12-29 10:03:47,850 - ==> Confusion:
[[12  3  1  9  3  0]
 [ 3  6  1 16 11  0]
 [ 0  4  6 15  9  0]
 [ 3  4  4 39 16  0]
 [ 2  3  0  9 16  0]
 [ 0  0  0  3  2  0]]

2022-12-29 10:03:47,852 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:47,852 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:47,878 - 

2022-12-29 10:03:47,879 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:48,691 - Epoch: [64][   10/  113]    Overall Loss 1.517473    Objective Loss 1.517473                                        LR 0.000100    Time 0.081117    
2022-12-29 10:03:49,326 - Epoch: [64][   20/  113]    Overall Loss 1.440824    Objective Loss 1.440824                                        LR 0.000100    Time 0.072283    
2022-12-29 10:03:49,954 - Epoch: [64][   30/  113]    Overall Loss 1.455473    Objective Loss 1.455473                                        LR 0.000100    Time 0.069128    
2022-12-29 10:03:50,584 - Epoch: [64][   40/  113]    Overall Loss 1.465469    Objective Loss 1.465469                                        LR 0.000100    Time 0.067580    
2022-12-29 10:03:51,216 - Epoch: [64][   50/  113]    Overall Loss 1.470120    Objective Loss 1.470120                                        LR 0.000100    Time 0.066688    
2022-12-29 10:03:51,842 - Epoch: [64][   60/  113]    Overall Loss 1.466179    Objective Loss 1.466179                                        LR 0.000100    Time 0.065995    
2022-12-29 10:03:52,473 - Epoch: [64][   70/  113]    Overall Loss 1.464071    Objective Loss 1.464071                                        LR 0.000100    Time 0.065584    
2022-12-29 10:03:53,097 - Epoch: [64][   80/  113]    Overall Loss 1.479841    Objective Loss 1.479841                                        LR 0.000100    Time 0.065174    
2022-12-29 10:03:53,725 - Epoch: [64][   90/  113]    Overall Loss 1.476644    Objective Loss 1.476644                                        LR 0.000100    Time 0.064903    
2022-12-29 10:03:54,351 - Epoch: [64][  100/  113]    Overall Loss 1.476207    Objective Loss 1.476207                                        LR 0.000100    Time 0.064670    
2022-12-29 10:03:54,974 - Epoch: [64][  110/  113]    Overall Loss 1.479149    Objective Loss 1.479149                                        LR 0.000100    Time 0.064450    
2022-12-29 10:03:55,148 - Epoch: [64][  113/  113]    Overall Loss 1.485073    Objective Loss 1.485073    Top1 37.500000    Top5 95.833333    LR 0.000100    Time 0.064279    
2022-12-29 10:03:55,201 - --- validate (epoch=64)-----------
2022-12-29 10:03:55,201 - 200 samples (16 per mini-batch)
2022-12-29 10:03:55,752 - Epoch: [64][   10/   13]    Loss 1.575004    Top1 41.250000    Top5 91.875000    
2022-12-29 10:03:55,837 - Epoch: [64][   13/   13]    Loss 1.564475    Top1 42.500000    Top5 92.000000    
2022-12-29 10:03:55,896 - ==> Top1: 42.500    Top5: 92.000    Loss: 1.564

2022-12-29 10:03:55,897 - ==> Confusion:
[[18  0  0 11  4  0]
 [ 4  5  2 13  6  0]
 [ 3  4  7 11  5  0]
 [ 3  4  4 41  6  0]
 [ 1  3  3 10 14  0]
 [ 1  1  3  9  4  0]]

2022-12-29 10:03:55,899 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:03:55,900 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:03:55,922 - 

2022-12-29 10:03:55,922 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:03:56,736 - Epoch: [65][   10/  113]    Overall Loss 1.469131    Objective Loss 1.469131                                        LR 0.000100    Time 0.081310    
2022-12-29 10:03:57,370 - Epoch: [65][   20/  113]    Overall Loss 1.519702    Objective Loss 1.519702                                        LR 0.000100    Time 0.072297    
2022-12-29 10:03:58,005 - Epoch: [65][   30/  113]    Overall Loss 1.525852    Objective Loss 1.525852                                        LR 0.000100    Time 0.069369    
2022-12-29 10:03:58,639 - Epoch: [65][   40/  113]    Overall Loss 1.531875    Objective Loss 1.531875                                        LR 0.000100    Time 0.067869    
2022-12-29 10:03:59,264 - Epoch: [65][   50/  113]    Overall Loss 1.535492    Objective Loss 1.535492                                        LR 0.000100    Time 0.066791    
2022-12-29 10:03:59,897 - Epoch: [65][   60/  113]    Overall Loss 1.526287    Objective Loss 1.526287                                        LR 0.000100    Time 0.066187    
2022-12-29 10:04:00,532 - Epoch: [65][   70/  113]    Overall Loss 1.520874    Objective Loss 1.520874                                        LR 0.000100    Time 0.065800    
2022-12-29 10:04:01,161 - Epoch: [65][   80/  113]    Overall Loss 1.531707    Objective Loss 1.531707                                        LR 0.000100    Time 0.065434    
2022-12-29 10:04:01,792 - Epoch: [65][   90/  113]    Overall Loss 1.531047    Objective Loss 1.531047                                        LR 0.000100    Time 0.065170    
2022-12-29 10:04:02,424 - Epoch: [65][  100/  113]    Overall Loss 1.522146    Objective Loss 1.522146                                        LR 0.000100    Time 0.064964    
2022-12-29 10:04:03,055 - Epoch: [65][  110/  113]    Overall Loss 1.523357    Objective Loss 1.523357                                        LR 0.000100    Time 0.064792    
2022-12-29 10:04:03,225 - Epoch: [65][  113/  113]    Overall Loss 1.518500    Objective Loss 1.518500    Top1 50.000000    Top5 100.000000    LR 0.000100    Time 0.064574    
2022-12-29 10:04:03,286 - --- validate (epoch=65)-----------
2022-12-29 10:04:03,287 - 200 samples (16 per mini-batch)
2022-12-29 10:04:03,829 - Epoch: [65][   10/   13]    Loss 1.437620    Top1 42.500000    Top5 96.875000    
2022-12-29 10:04:03,914 - Epoch: [65][   13/   13]    Loss 1.450061    Top1 42.000000    Top5 97.000000    
2022-12-29 10:04:03,966 - ==> Top1: 42.000    Top5: 97.000    Loss: 1.450

2022-12-29 10:04:03,967 - ==> Confusion:
[[13  7  0  4  1  0]
 [ 3 20  4  8 14  0]
 [ 1  5 12  7  6  0]
 [ 0 13  7 28  6  0]
 [ 3  8  3  7 11  0]
 [ 0  4  0  1  4  0]]

2022-12-29 10:04:03,969 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:03,970 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:03,991 - 

2022-12-29 10:04:03,992 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:04,820 - Epoch: [66][   10/  113]    Overall Loss 1.535395    Objective Loss 1.535395                                        LR 0.000100    Time 0.082794    
2022-12-29 10:04:05,453 - Epoch: [66][   20/  113]    Overall Loss 1.473351    Objective Loss 1.473351                                        LR 0.000100    Time 0.073003    
2022-12-29 10:04:06,086 - Epoch: [66][   30/  113]    Overall Loss 1.481339    Objective Loss 1.481339                                        LR 0.000100    Time 0.069745    
2022-12-29 10:04:06,717 - Epoch: [66][   40/  113]    Overall Loss 1.486687    Objective Loss 1.486687                                        LR 0.000100    Time 0.068067    
2022-12-29 10:04:07,341 - Epoch: [66][   50/  113]    Overall Loss 1.496089    Objective Loss 1.496089                                        LR 0.000100    Time 0.066925    
2022-12-29 10:04:07,968 - Epoch: [66][   60/  113]    Overall Loss 1.494354    Objective Loss 1.494354                                        LR 0.000100    Time 0.066215    
2022-12-29 10:04:08,591 - Epoch: [66][   70/  113]    Overall Loss 1.507135    Objective Loss 1.507135                                        LR 0.000100    Time 0.065651    
2022-12-29 10:04:09,217 - Epoch: [66][   80/  113]    Overall Loss 1.509441    Objective Loss 1.509441                                        LR 0.000100    Time 0.065261    
2022-12-29 10:04:09,842 - Epoch: [66][   90/  113]    Overall Loss 1.520179    Objective Loss 1.520179                                        LR 0.000100    Time 0.064947    
2022-12-29 10:04:10,469 - Epoch: [66][  100/  113]    Overall Loss 1.521990    Objective Loss 1.521990                                        LR 0.000100    Time 0.064719    
2022-12-29 10:04:11,091 - Epoch: [66][  110/  113]    Overall Loss 1.527103    Objective Loss 1.527103                                        LR 0.000100    Time 0.064486    
2022-12-29 10:04:11,268 - Epoch: [66][  113/  113]    Overall Loss 1.527176    Objective Loss 1.527176    Top1 37.500000    Top5 95.833333    LR 0.000100    Time 0.064333    
2022-12-29 10:04:11,319 - --- validate (epoch=66)-----------
2022-12-29 10:04:11,319 - 200 samples (16 per mini-batch)
2022-12-29 10:04:11,858 - Epoch: [66][   10/   13]    Loss 1.491187    Top1 39.375000    Top5 95.625000    
2022-12-29 10:04:11,942 - Epoch: [66][   13/   13]    Loss 1.480079    Top1 39.500000    Top5 95.000000    
2022-12-29 10:04:11,995 - ==> Top1: 39.500    Top5: 95.000    Loss: 1.480

2022-12-29 10:04:11,996 - ==> Confusion:
[[20  0  2  3  1  0]
 [12 15  4  9 14  0]
 [ 3  6 11  6 10  0]
 [11  5  6 20  4  0]
 [ 3  3  5  5 13  0]
 [ 2  1  0  3  3  0]]

2022-12-29 10:04:11,998 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:11,998 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:12,011 - 

2022-12-29 10:04:12,012 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:12,829 - Epoch: [67][   10/  113]    Overall Loss 1.537207    Objective Loss 1.537207                                        LR 0.000100    Time 0.081662    
2022-12-29 10:04:13,456 - Epoch: [67][   20/  113]    Overall Loss 1.498634    Objective Loss 1.498634                                        LR 0.000100    Time 0.072139    
2022-12-29 10:04:14,084 - Epoch: [67][   30/  113]    Overall Loss 1.519420    Objective Loss 1.519420                                        LR 0.000100    Time 0.069012    
2022-12-29 10:04:14,716 - Epoch: [67][   40/  113]    Overall Loss 1.505805    Objective Loss 1.505805                                        LR 0.000100    Time 0.067539    
2022-12-29 10:04:15,343 - Epoch: [67][   50/  113]    Overall Loss 1.495981    Objective Loss 1.495981                                        LR 0.000100    Time 0.066566    
2022-12-29 10:04:15,969 - Epoch: [67][   60/  113]    Overall Loss 1.493881    Objective Loss 1.493881                                        LR 0.000100    Time 0.065893    
2022-12-29 10:04:16,597 - Epoch: [67][   70/  113]    Overall Loss 1.493629    Objective Loss 1.493629                                        LR 0.000100    Time 0.065446    
2022-12-29 10:04:17,225 - Epoch: [67][   80/  113]    Overall Loss 1.473231    Objective Loss 1.473231                                        LR 0.000100    Time 0.065108    
2022-12-29 10:04:17,857 - Epoch: [67][   90/  113]    Overall Loss 1.472442    Objective Loss 1.472442                                        LR 0.000100    Time 0.064893    
2022-12-29 10:04:18,484 - Epoch: [67][  100/  113]    Overall Loss 1.470744    Objective Loss 1.470744                                        LR 0.000100    Time 0.064667    
2022-12-29 10:04:19,112 - Epoch: [67][  110/  113]    Overall Loss 1.477143    Objective Loss 1.477143                                        LR 0.000100    Time 0.064489    
2022-12-29 10:04:19,284 - Epoch: [67][  113/  113]    Overall Loss 1.482505    Objective Loss 1.482505    Top1 37.500000    Top5 95.833333    LR 0.000100    Time 0.064300    
2022-12-29 10:04:19,335 - --- validate (epoch=67)-----------
2022-12-29 10:04:19,335 - 200 samples (16 per mini-batch)
2022-12-29 10:04:19,877 - Epoch: [67][   10/   13]    Loss 1.502597    Top1 36.250000    Top5 96.875000    
2022-12-29 10:04:19,963 - Epoch: [67][   13/   13]    Loss 1.519240    Top1 36.000000    Top5 96.500000    
2022-12-29 10:04:20,013 - ==> Top1: 36.000    Top5: 96.500    Loss: 1.519

2022-12-29 10:04:20,014 - ==> Confusion:
[[11  4  2  5  4  0]
 [ 5 21  1  6  6  0]
 [ 3 24  3  9  4  0]
 [ 0  9  5 23  8  0]
 [ 4  9  1  9 14  0]
 [ 1  8  1  0  0  0]]

2022-12-29 10:04:20,017 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:20,017 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:20,039 - 

2022-12-29 10:04:20,039 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:20,878 - Epoch: [68][   10/  113]    Overall Loss 1.433939    Objective Loss 1.433939                                        LR 0.000100    Time 0.083800    
2022-12-29 10:04:21,518 - Epoch: [68][   20/  113]    Overall Loss 1.443878    Objective Loss 1.443878                                        LR 0.000100    Time 0.073868    
2022-12-29 10:04:22,151 - Epoch: [68][   30/  113]    Overall Loss 1.460667    Objective Loss 1.460667                                        LR 0.000100    Time 0.070342    
2022-12-29 10:04:22,783 - Epoch: [68][   40/  113]    Overall Loss 1.475370    Objective Loss 1.475370                                        LR 0.000100    Time 0.068537    
2022-12-29 10:04:23,408 - Epoch: [68][   50/  113]    Overall Loss 1.469091    Objective Loss 1.469091                                        LR 0.000100    Time 0.067309    
2022-12-29 10:04:24,033 - Epoch: [68][   60/  113]    Overall Loss 1.473725    Objective Loss 1.473725                                        LR 0.000100    Time 0.066504    
2022-12-29 10:04:24,661 - Epoch: [68][   70/  113]    Overall Loss 1.483047    Objective Loss 1.483047                                        LR 0.000100    Time 0.065971    
2022-12-29 10:04:25,288 - Epoch: [68][   80/  113]    Overall Loss 1.469863    Objective Loss 1.469863                                        LR 0.000100    Time 0.065550    
2022-12-29 10:04:25,915 - Epoch: [68][   90/  113]    Overall Loss 1.474291    Objective Loss 1.474291                                        LR 0.000100    Time 0.065227    
2022-12-29 10:04:26,540 - Epoch: [68][  100/  113]    Overall Loss 1.476267    Objective Loss 1.476267                                        LR 0.000100    Time 0.064953    
2022-12-29 10:04:27,163 - Epoch: [68][  110/  113]    Overall Loss 1.484571    Objective Loss 1.484571                                        LR 0.000100    Time 0.064709    
2022-12-29 10:04:27,336 - Epoch: [68][  113/  113]    Overall Loss 1.482052    Objective Loss 1.482052    Top1 50.000000    Top5 100.000000    LR 0.000100    Time 0.064519    
2022-12-29 10:04:27,387 - --- validate (epoch=68)-----------
2022-12-29 10:04:27,387 - 200 samples (16 per mini-batch)
2022-12-29 10:04:27,924 - Epoch: [68][   10/   13]    Loss 1.498203    Top1 36.250000    Top5 97.500000    
2022-12-29 10:04:28,007 - Epoch: [68][   13/   13]    Loss 1.469082    Top1 37.000000    Top5 97.000000    
2022-12-29 10:04:28,074 - ==> Top1: 37.000    Top5: 97.000    Loss: 1.469

2022-12-29 10:04:28,075 - ==> Confusion:
[[13  3  0  0  3  0]
 [ 3 30  0  2 13  0]
 [ 0 20  3  7 11  0]
 [ 5 13  3  9 18  0]
 [ 2 13  0  1 19  0]
 [ 0  9  0  0  0  0]]

2022-12-29 10:04:28,078 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:28,079 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:28,092 - 

2022-12-29 10:04:28,092 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:28,935 - Epoch: [69][   10/  113]    Overall Loss 1.523959    Objective Loss 1.523959                                        LR 0.000100    Time 0.084163    
2022-12-29 10:04:29,569 - Epoch: [69][   20/  113]    Overall Loss 1.558449    Objective Loss 1.558449                                        LR 0.000100    Time 0.073763    
2022-12-29 10:04:30,204 - Epoch: [69][   30/  113]    Overall Loss 1.531962    Objective Loss 1.531962                                        LR 0.000100    Time 0.070331    
2022-12-29 10:04:30,832 - Epoch: [69][   40/  113]    Overall Loss 1.509042    Objective Loss 1.509042                                        LR 0.000100    Time 0.068425    
2022-12-29 10:04:31,464 - Epoch: [69][   50/  113]    Overall Loss 1.492874    Objective Loss 1.492874                                        LR 0.000100    Time 0.067379    
2022-12-29 10:04:32,093 - Epoch: [69][   60/  113]    Overall Loss 1.497903    Objective Loss 1.497903                                        LR 0.000100    Time 0.066620    
2022-12-29 10:04:32,724 - Epoch: [69][   70/  113]    Overall Loss 1.497741    Objective Loss 1.497741                                        LR 0.000100    Time 0.066108    
2022-12-29 10:04:33,351 - Epoch: [69][   80/  113]    Overall Loss 1.502662    Objective Loss 1.502662                                        LR 0.000100    Time 0.065679    
2022-12-29 10:04:33,980 - Epoch: [69][   90/  113]    Overall Loss 1.502632    Objective Loss 1.502632                                        LR 0.000100    Time 0.065371    
2022-12-29 10:04:34,613 - Epoch: [69][  100/  113]    Overall Loss 1.497970    Objective Loss 1.497970                                        LR 0.000100    Time 0.065155    
2022-12-29 10:04:35,239 - Epoch: [69][  110/  113]    Overall Loss 1.485269    Objective Loss 1.485269                                        LR 0.000100    Time 0.064914    
2022-12-29 10:04:35,410 - Epoch: [69][  113/  113]    Overall Loss 1.487309    Objective Loss 1.487309    Top1 29.166667    Top5 95.833333    LR 0.000100    Time 0.064702    
2022-12-29 10:04:35,462 - --- validate (epoch=69)-----------
2022-12-29 10:04:35,462 - 200 samples (16 per mini-batch)
2022-12-29 10:04:36,011 - Epoch: [69][   10/   13]    Loss 1.461859    Top1 35.000000    Top5 98.750000    
2022-12-29 10:04:36,095 - Epoch: [69][   13/   13]    Loss 1.464315    Top1 37.500000    Top5 98.000000    
2022-12-29 10:04:36,147 - ==> Top1: 37.500    Top5: 98.000    Loss: 1.464

2022-12-29 10:04:36,148 - ==> Confusion:
[[10  2  0  8  3  0]
 [ 3  9  4 13  8  0]
 [ 1  6 15 13  4  0]
 [ 2  3  7 28 10  0]
 [ 3  7  2 18 13  0]
 [ 0  2  1  3  2  0]]

2022-12-29 10:04:36,149 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:36,150 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:36,172 - 

2022-12-29 10:04:36,173 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:36,995 - Epoch: [70][   10/  113]    Overall Loss 1.538679    Objective Loss 1.538679                                        LR 0.000100    Time 0.082150    
2022-12-29 10:04:37,629 - Epoch: [70][   20/  113]    Overall Loss 1.488185    Objective Loss 1.488185                                        LR 0.000100    Time 0.072752    
2022-12-29 10:04:38,258 - Epoch: [70][   30/  113]    Overall Loss 1.508031    Objective Loss 1.508031                                        LR 0.000100    Time 0.069440    
2022-12-29 10:04:38,889 - Epoch: [70][   40/  113]    Overall Loss 1.510182    Objective Loss 1.510182                                        LR 0.000100    Time 0.067855    
2022-12-29 10:04:39,516 - Epoch: [70][   50/  113]    Overall Loss 1.513000    Objective Loss 1.513000                                        LR 0.000100    Time 0.066806    
2022-12-29 10:04:40,141 - Epoch: [70][   60/  113]    Overall Loss 1.527948    Objective Loss 1.527948                                        LR 0.000100    Time 0.066081    
2022-12-29 10:04:40,768 - Epoch: [70][   70/  113]    Overall Loss 1.531456    Objective Loss 1.531456                                        LR 0.000100    Time 0.065586    
2022-12-29 10:04:41,394 - Epoch: [70][   80/  113]    Overall Loss 1.532859    Objective Loss 1.532859                                        LR 0.000100    Time 0.065209    
2022-12-29 10:04:42,027 - Epoch: [70][   90/  113]    Overall Loss 1.525784    Objective Loss 1.525784                                        LR 0.000100    Time 0.064991    
2022-12-29 10:04:42,656 - Epoch: [70][  100/  113]    Overall Loss 1.519852    Objective Loss 1.519852                                        LR 0.000100    Time 0.064722    
2022-12-29 10:04:43,279 - Epoch: [70][  110/  113]    Overall Loss 1.520457    Objective Loss 1.520457                                        LR 0.000100    Time 0.064497    
2022-12-29 10:04:43,448 - Epoch: [70][  113/  113]    Overall Loss 1.519618    Objective Loss 1.519618    Top1 41.666667    Top5 100.000000    LR 0.000100    Time 0.064279    
2022-12-29 10:04:43,505 - --- validate (epoch=70)-----------
2022-12-29 10:04:43,505 - 200 samples (16 per mini-batch)
2022-12-29 10:04:44,055 - Epoch: [70][   10/   13]    Loss 1.444803    Top1 38.750000    Top5 95.000000    
2022-12-29 10:04:44,140 - Epoch: [70][   13/   13]    Loss 1.464881    Top1 38.500000    Top5 95.000000    
2022-12-29 10:04:44,184 - ==> Top1: 38.500    Top5: 95.000    Loss: 1.465

2022-12-29 10:04:44,184 - ==> Confusion:
[[17  4  0  8  2  0]
 [ 4 17  2 17  7  0]
 [ 1  3  0  4  8  0]
 [ 5  5  1 33  9  0]
 [ 0 10  3 16 10  0]
 [ 3  5  0  4  2  0]]

2022-12-29 10:04:44,187 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:44,187 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:44,200 - 

2022-12-29 10:04:44,201 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:45,027 - Epoch: [71][   10/  113]    Overall Loss 1.454063    Objective Loss 1.454063                                        LR 0.000100    Time 0.082547    
2022-12-29 10:04:45,661 - Epoch: [71][   20/  113]    Overall Loss 1.488871    Objective Loss 1.488871                                        LR 0.000100    Time 0.072962    
2022-12-29 10:04:46,294 - Epoch: [71][   30/  113]    Overall Loss 1.490911    Objective Loss 1.490911                                        LR 0.000100    Time 0.069724    
2022-12-29 10:04:46,924 - Epoch: [71][   40/  113]    Overall Loss 1.476013    Objective Loss 1.476013                                        LR 0.000100    Time 0.068030    
2022-12-29 10:04:47,553 - Epoch: [71][   50/  113]    Overall Loss 1.475214    Objective Loss 1.475214                                        LR 0.000100    Time 0.066995    
2022-12-29 10:04:48,182 - Epoch: [71][   60/  113]    Overall Loss 1.471203    Objective Loss 1.471203                                        LR 0.000100    Time 0.066291    
2022-12-29 10:04:48,812 - Epoch: [71][   70/  113]    Overall Loss 1.469494    Objective Loss 1.469494                                        LR 0.000100    Time 0.065817    
2022-12-29 10:04:49,435 - Epoch: [71][   80/  113]    Overall Loss 1.459539    Objective Loss 1.459539                                        LR 0.000100    Time 0.065380    
2022-12-29 10:04:50,057 - Epoch: [71][   90/  113]    Overall Loss 1.468788    Objective Loss 1.468788                                        LR 0.000100    Time 0.065020    
2022-12-29 10:04:50,681 - Epoch: [71][  100/  113]    Overall Loss 1.472309    Objective Loss 1.472309                                        LR 0.000100    Time 0.064752    
2022-12-29 10:04:51,307 - Epoch: [71][  110/  113]    Overall Loss 1.473005    Objective Loss 1.473005                                        LR 0.000100    Time 0.064548    
2022-12-29 10:04:51,476 - Epoch: [71][  113/  113]    Overall Loss 1.480091    Objective Loss 1.480091    Top1 20.833333    Top5 95.833333    LR 0.000100    Time 0.064331    
2022-12-29 10:04:51,527 - --- validate (epoch=71)-----------
2022-12-29 10:04:51,527 - 200 samples (16 per mini-batch)
2022-12-29 10:04:52,070 - Epoch: [71][   10/   13]    Loss 1.597304    Top1 31.250000    Top5 96.250000    
2022-12-29 10:04:52,157 - Epoch: [71][   13/   13]    Loss 1.573380    Top1 34.000000    Top5 96.000000    
2022-12-29 10:04:52,201 - ==> Top1: 34.000    Top5: 96.000    Loss: 1.573

2022-12-29 10:04:52,202 - ==> Confusion:
[[11  5  0 12  2  0]
 [ 3 10  0 15  9  0]
 [ 0  6  3 17  8  0]
 [ 2  7  0 30 11  0]
 [ 0  9  0 14 14  0]
 [ 1  1  1  5  4  0]]

2022-12-29 10:04:52,204 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:04:52,204 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:04:52,229 - 

2022-12-29 10:04:52,229 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:04:53,069 - Epoch: [72][   10/  113]    Overall Loss 1.580041    Objective Loss 1.580041                                        LR 0.000100    Time 0.083889    
2022-12-29 10:04:53,699 - Epoch: [72][   20/  113]    Overall Loss 1.543046    Objective Loss 1.543046                                        LR 0.000100    Time 0.073050    
2022-12-29 10:04:54,327 - Epoch: [72][   30/  113]    Overall Loss 1.546626    Objective Loss 1.546626                                        LR 0.000100    Time 0.069630    
2022-12-29 10:04:54,958 - Epoch: [72][   40/  113]    Overall Loss 1.519256    Objective Loss 1.519256                                        LR 0.000100    Time 0.067979    
2022-12-29 10:04:55,580 - Epoch: [72][   50/  113]    Overall Loss 1.525458    Objective Loss 1.525458                                        LR 0.000100    Time 0.066817    
2022-12-29 10:04:56,212 - Epoch: [72][   60/  113]    Overall Loss 1.527724    Objective Loss 1.527724                                        LR 0.000100    Time 0.066205    
2022-12-29 10:04:56,845 - Epoch: [72][   70/  113]    Overall Loss 1.525024    Objective Loss 1.525024                                        LR 0.000100    Time 0.065778    
2022-12-29 10:04:57,474 - Epoch: [72][   80/  113]    Overall Loss 1.519278    Objective Loss 1.519278                                        LR 0.000100    Time 0.065408    
2022-12-29 10:04:58,107 - Epoch: [72][   90/  113]    Overall Loss 1.503075    Objective Loss 1.503075                                        LR 0.000100    Time 0.065177    
2022-12-29 10:04:58,740 - Epoch: [72][  100/  113]    Overall Loss 1.498886    Objective Loss 1.498886                                        LR 0.000100    Time 0.064978    
2022-12-29 10:04:59,362 - Epoch: [72][  110/  113]    Overall Loss 1.494361    Objective Loss 1.494361                                        LR 0.000100    Time 0.064721    
2022-12-29 10:04:59,533 - Epoch: [72][  113/  113]    Overall Loss 1.493606    Objective Loss 1.493606    Top1 37.500000    Top5 91.666667    LR 0.000100    Time 0.064515    
2022-12-29 10:04:59,596 - --- validate (epoch=72)-----------
2022-12-29 10:04:59,596 - 200 samples (16 per mini-batch)
2022-12-29 10:05:00,139 - Epoch: [72][   10/   13]    Loss 1.570049    Top1 36.250000    Top5 92.500000    
2022-12-29 10:05:00,224 - Epoch: [72][   13/   13]    Loss 1.526085    Top1 37.500000    Top5 94.000000    
2022-12-29 10:05:00,268 - ==> Top1: 37.500    Top5: 94.000    Loss: 1.526

2022-12-29 10:05:00,269 - ==> Confusion:
[[10  1  0 14  2  0]
 [ 2  9  1 18  9  0]
 [ 3 11  3 10  6  0]
 [ 1  4  0 42  4  0]
 [ 0  9  0 15 11  0]
 [ 2  3  0  5  5  0]]

2022-12-29 10:05:00,271 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:00,271 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:00,286 - 

2022-12-29 10:05:00,286 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:01,118 - Epoch: [73][   10/  113]    Overall Loss 1.502303    Objective Loss 1.502303                                        LR 0.000100    Time 0.083068    
2022-12-29 10:05:01,755 - Epoch: [73][   20/  113]    Overall Loss 1.510336    Objective Loss 1.510336                                        LR 0.000100    Time 0.073346    
2022-12-29 10:05:02,386 - Epoch: [73][   30/  113]    Overall Loss 1.503943    Objective Loss 1.503943                                        LR 0.000100    Time 0.069930    
2022-12-29 10:05:03,015 - Epoch: [73][   40/  113]    Overall Loss 1.484663    Objective Loss 1.484663                                        LR 0.000100    Time 0.068154    
2022-12-29 10:05:03,645 - Epoch: [73][   50/  113]    Overall Loss 1.486307    Objective Loss 1.486307                                        LR 0.000100    Time 0.067113    
2022-12-29 10:05:04,277 - Epoch: [73][   60/  113]    Overall Loss 1.484212    Objective Loss 1.484212                                        LR 0.000100    Time 0.066463    
2022-12-29 10:05:04,916 - Epoch: [73][   70/  113]    Overall Loss 1.486738    Objective Loss 1.486738                                        LR 0.000100    Time 0.066084    
2022-12-29 10:05:05,550 - Epoch: [73][   80/  113]    Overall Loss 1.490426    Objective Loss 1.490426                                        LR 0.000100    Time 0.065739    
2022-12-29 10:05:06,183 - Epoch: [73][   90/  113]    Overall Loss 1.492708    Objective Loss 1.492708                                        LR 0.000100    Time 0.065467    
2022-12-29 10:05:06,815 - Epoch: [73][  100/  113]    Overall Loss 1.489492    Objective Loss 1.489492                                        LR 0.000100    Time 0.065238    
2022-12-29 10:05:07,441 - Epoch: [73][  110/  113]    Overall Loss 1.499816    Objective Loss 1.499816                                        LR 0.000100    Time 0.064991    
2022-12-29 10:05:07,610 - Epoch: [73][  113/  113]    Overall Loss 1.499207    Objective Loss 1.499207    Top1 50.000000    Top5 100.000000    LR 0.000100    Time 0.064756    
2022-12-29 10:05:07,662 - --- validate (epoch=73)-----------
2022-12-29 10:05:07,662 - 200 samples (16 per mini-batch)
2022-12-29 10:05:08,203 - Epoch: [73][   10/   13]    Loss 1.469096    Top1 41.250000    Top5 96.875000    
2022-12-29 10:05:08,289 - Epoch: [73][   13/   13]    Loss 1.471996    Top1 40.000000    Top5 96.500000    
2022-12-29 10:05:08,338 - ==> Top1: 40.000    Top5: 96.500    Loss: 1.472

2022-12-29 10:05:08,339 - ==> Confusion:
[[11  3  1  4 10  0]
 [ 2 21  1  3 22  0]
 [ 1 11  6  2 15  0]
 [ 2  6  3 17 14  0]
 [ 1  5  1  4 25  0]
 [ 0  0  2  1  6  0]]

2022-12-29 10:05:08,341 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:08,341 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:08,355 - 

2022-12-29 10:05:08,355 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:09,182 - Epoch: [74][   10/  113]    Overall Loss 1.417249    Objective Loss 1.417249                                        LR 0.000100    Time 0.082660    
2022-12-29 10:05:09,811 - Epoch: [74][   20/  113]    Overall Loss 1.435020    Objective Loss 1.435020                                        LR 0.000100    Time 0.072720    
2022-12-29 10:05:10,446 - Epoch: [74][   30/  113]    Overall Loss 1.434400    Objective Loss 1.434400                                        LR 0.000100    Time 0.069627    
2022-12-29 10:05:11,069 - Epoch: [74][   40/  113]    Overall Loss 1.435770    Objective Loss 1.435770                                        LR 0.000100    Time 0.067782    
2022-12-29 10:05:11,697 - Epoch: [74][   50/  113]    Overall Loss 1.461695    Objective Loss 1.461695                                        LR 0.000100    Time 0.066781    
2022-12-29 10:05:12,325 - Epoch: [74][   60/  113]    Overall Loss 1.465094    Objective Loss 1.465094                                        LR 0.000100    Time 0.066117    
2022-12-29 10:05:12,955 - Epoch: [74][   70/  113]    Overall Loss 1.475970    Objective Loss 1.475970                                        LR 0.000100    Time 0.065662    
2022-12-29 10:05:13,581 - Epoch: [74][   80/  113]    Overall Loss 1.476966    Objective Loss 1.476966                                        LR 0.000100    Time 0.065265    
2022-12-29 10:05:14,207 - Epoch: [74][   90/  113]    Overall Loss 1.478476    Objective Loss 1.478476                                        LR 0.000100    Time 0.064972    
2022-12-29 10:05:14,838 - Epoch: [74][  100/  113]    Overall Loss 1.479785    Objective Loss 1.479785                                        LR 0.000100    Time 0.064774    
2022-12-29 10:05:15,464 - Epoch: [74][  110/  113]    Overall Loss 1.476214    Objective Loss 1.476214                                        LR 0.000100    Time 0.064577    
2022-12-29 10:05:15,634 - Epoch: [74][  113/  113]    Overall Loss 1.481408    Objective Loss 1.481408    Top1 20.833333    Top5 91.666667    LR 0.000100    Time 0.064365    
2022-12-29 10:05:15,698 - --- validate (epoch=74)-----------
2022-12-29 10:05:15,698 - 200 samples (16 per mini-batch)
2022-12-29 10:05:16,243 - Epoch: [74][   10/   13]    Loss 1.529570    Top1 36.875000    Top5 95.625000    
2022-12-29 10:05:16,329 - Epoch: [74][   13/   13]    Loss 1.525978    Top1 36.500000    Top5 95.500000    
2022-12-29 10:05:16,398 - ==> Top1: 36.500    Top5: 95.500    Loss: 1.526

2022-12-29 10:05:16,398 - ==> Confusion:
[[ 8  1  1  3  9  0]
 [ 1  7  6  6 20  0]
 [ 0  3 12  9 13  0]
 [ 0  9  2 28 11  0]
 [ 2  2  1 15 18  0]
 [ 0  1  0  3  9  0]]

2022-12-29 10:05:16,400 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:16,400 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:16,422 - 

2022-12-29 10:05:16,422 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:17,245 - Epoch: [75][   10/  113]    Overall Loss 1.408869    Objective Loss 1.408869                                        LR 0.000100    Time 0.082216    
2022-12-29 10:05:17,879 - Epoch: [75][   20/  113]    Overall Loss 1.437104    Objective Loss 1.437104                                        LR 0.000100    Time 0.072769    
2022-12-29 10:05:18,508 - Epoch: [75][   30/  113]    Overall Loss 1.471619    Objective Loss 1.471619                                        LR 0.000100    Time 0.069451    
2022-12-29 10:05:19,133 - Epoch: [75][   40/  113]    Overall Loss 1.450978    Objective Loss 1.450978                                        LR 0.000100    Time 0.067718    
2022-12-29 10:05:19,758 - Epoch: [75][   50/  113]    Overall Loss 1.451246    Objective Loss 1.451246                                        LR 0.000100    Time 0.066657    
2022-12-29 10:05:20,385 - Epoch: [75][   60/  113]    Overall Loss 1.451619    Objective Loss 1.451619                                        LR 0.000100    Time 0.065998    
2022-12-29 10:05:21,013 - Epoch: [75][   70/  113]    Overall Loss 1.459643    Objective Loss 1.459643                                        LR 0.000100    Time 0.065530    
2022-12-29 10:05:21,638 - Epoch: [75][   80/  113]    Overall Loss 1.465745    Objective Loss 1.465745                                        LR 0.000100    Time 0.065150    
2022-12-29 10:05:22,265 - Epoch: [75][   90/  113]    Overall Loss 1.462053    Objective Loss 1.462053                                        LR 0.000100    Time 0.064871    
2022-12-29 10:05:22,888 - Epoch: [75][  100/  113]    Overall Loss 1.463310    Objective Loss 1.463310                                        LR 0.000100    Time 0.064607    
2022-12-29 10:05:23,510 - Epoch: [75][  110/  113]    Overall Loss 1.467274    Objective Loss 1.467274                                        LR 0.000100    Time 0.064387    
2022-12-29 10:05:23,686 - Epoch: [75][  113/  113]    Overall Loss 1.472558    Objective Loss 1.472558    Top1 33.333333    Top5 100.000000    LR 0.000100    Time 0.064224    
2022-12-29 10:05:23,741 - --- validate (epoch=75)-----------
2022-12-29 10:05:23,742 - 200 samples (16 per mini-batch)
2022-12-29 10:05:24,280 - Epoch: [75][   10/   13]    Loss 1.542704    Top1 34.375000    Top5 97.500000    
2022-12-29 10:05:24,364 - Epoch: [75][   13/   13]    Loss 1.514952    Top1 33.500000    Top5 97.500000    
2022-12-29 10:05:24,423 - ==> Top1: 33.500    Top5: 97.500    Loss: 1.515

2022-12-29 10:05:24,424 - ==> Confusion:
[[13  2  1  9  6  0]
 [ 3 18  3 15  9  0]
 [ 0  4 11  4 11  0]
 [ 1  2 19 12 12  0]
 [ 1  7  3 12 13  0]
 [ 0  2  1  1  5  0]]

2022-12-29 10:05:24,427 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:24,427 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:24,439 - 

2022-12-29 10:05:24,439 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:25,270 - Epoch: [76][   10/  113]    Overall Loss 1.354486    Objective Loss 1.354486                                        LR 0.000100    Time 0.082977    
2022-12-29 10:05:25,897 - Epoch: [76][   20/  113]    Overall Loss 1.446643    Objective Loss 1.446643                                        LR 0.000100    Time 0.072795    
2022-12-29 10:05:26,533 - Epoch: [76][   30/  113]    Overall Loss 1.436721    Objective Loss 1.436721                                        LR 0.000100    Time 0.069713    
2022-12-29 10:05:27,164 - Epoch: [76][   40/  113]    Overall Loss 1.457645    Objective Loss 1.457645                                        LR 0.000100    Time 0.068051    
2022-12-29 10:05:27,789 - Epoch: [76][   50/  113]    Overall Loss 1.473212    Objective Loss 1.473212                                        LR 0.000100    Time 0.066944    
2022-12-29 10:05:28,418 - Epoch: [76][   60/  113]    Overall Loss 1.482209    Objective Loss 1.482209                                        LR 0.000100    Time 0.066251    
2022-12-29 10:05:29,049 - Epoch: [76][   70/  113]    Overall Loss 1.482655    Objective Loss 1.482655                                        LR 0.000100    Time 0.065795    
2022-12-29 10:05:29,678 - Epoch: [76][   80/  113]    Overall Loss 1.483450    Objective Loss 1.483450                                        LR 0.000100    Time 0.065435    
2022-12-29 10:05:30,312 - Epoch: [76][   90/  113]    Overall Loss 1.485952    Objective Loss 1.485952                                        LR 0.000100    Time 0.065197    
2022-12-29 10:05:30,940 - Epoch: [76][  100/  113]    Overall Loss 1.480119    Objective Loss 1.480119                                        LR 0.000100    Time 0.064958    
2022-12-29 10:05:31,567 - Epoch: [76][  110/  113]    Overall Loss 1.482532    Objective Loss 1.482532                                        LR 0.000100    Time 0.064743    
2022-12-29 10:05:31,741 - Epoch: [76][  113/  113]    Overall Loss 1.489218    Objective Loss 1.489218    Top1 20.833333    Top5 91.666667    LR 0.000100    Time 0.064565    
2022-12-29 10:05:31,789 - --- validate (epoch=76)-----------
2022-12-29 10:05:31,789 - 200 samples (16 per mini-batch)
2022-12-29 10:05:32,336 - Epoch: [76][   10/   13]    Loss 1.424983    Top1 40.000000    Top5 95.000000    
2022-12-29 10:05:32,421 - Epoch: [76][   13/   13]    Loss 1.472442    Top1 39.000000    Top5 94.500000    
2022-12-29 10:05:32,475 - ==> Top1: 39.000    Top5: 94.500    Loss: 1.472

2022-12-29 10:05:32,475 - ==> Confusion:
[[14  4  0  6  3  0]
 [ 5 13  4 10 10  0]
 [ 1 10 12  3  9  0]
 [ 2  7  4 33  6  0]
 [ 1  7  7 12  6  0]
 [ 2  4  0  3  2  0]]

2022-12-29 10:05:32,477 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:32,477 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:32,497 - 

2022-12-29 10:05:32,497 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:33,315 - Epoch: [77][   10/  113]    Overall Loss 1.451729    Objective Loss 1.451729                                        LR 0.000100    Time 0.081718    
2022-12-29 10:05:33,944 - Epoch: [77][   20/  113]    Overall Loss 1.458854    Objective Loss 1.458854                                        LR 0.000100    Time 0.072271    
2022-12-29 10:05:34,576 - Epoch: [77][   30/  113]    Overall Loss 1.484120    Objective Loss 1.484120                                        LR 0.000100    Time 0.069246    
2022-12-29 10:05:35,209 - Epoch: [77][   40/  113]    Overall Loss 1.481199    Objective Loss 1.481199                                        LR 0.000100    Time 0.067751    
2022-12-29 10:05:35,835 - Epoch: [77][   50/  113]    Overall Loss 1.498562    Objective Loss 1.498562                                        LR 0.000100    Time 0.066702    
2022-12-29 10:05:36,464 - Epoch: [77][   60/  113]    Overall Loss 1.482323    Objective Loss 1.482323                                        LR 0.000100    Time 0.066053    
2022-12-29 10:05:37,089 - Epoch: [77][   70/  113]    Overall Loss 1.482620    Objective Loss 1.482620                                        LR 0.000100    Time 0.065541    
2022-12-29 10:05:37,713 - Epoch: [77][   80/  113]    Overall Loss 1.469463    Objective Loss 1.469463                                        LR 0.000100    Time 0.065152    
2022-12-29 10:05:38,339 - Epoch: [77][   90/  113]    Overall Loss 1.478832    Objective Loss 1.478832                                        LR 0.000100    Time 0.064864    
2022-12-29 10:05:38,968 - Epoch: [77][  100/  113]    Overall Loss 1.480435    Objective Loss 1.480435                                        LR 0.000100    Time 0.064655    
2022-12-29 10:05:39,596 - Epoch: [77][  110/  113]    Overall Loss 1.483345    Objective Loss 1.483345                                        LR 0.000100    Time 0.064481    
2022-12-29 10:05:39,767 - Epoch: [77][  113/  113]    Overall Loss 1.482990    Objective Loss 1.482990    Top1 50.000000    Top5 100.000000    LR 0.000100    Time 0.064284    
2022-12-29 10:05:39,829 - --- validate (epoch=77)-----------
2022-12-29 10:05:39,830 - 200 samples (16 per mini-batch)
2022-12-29 10:05:40,371 - Epoch: [77][   10/   13]    Loss 1.469416    Top1 38.125000    Top5 96.875000    
2022-12-29 10:05:40,454 - Epoch: [77][   13/   13]    Loss 1.493775    Top1 38.500000    Top5 96.500000    
2022-12-29 10:05:40,512 - ==> Top1: 38.500    Top5: 96.500    Loss: 1.494

2022-12-29 10:05:40,513 - ==> Confusion:
[[22  4  0 10  2  0]
 [ 7  8  7  9  6  0]
 [ 4  6  8  8  6  0]
 [ 4  2  1 26  8  0]
 [ 6  6  4  9 13  0]
 [ 4  1  3  3  3  0]]

2022-12-29 10:05:40,515 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:40,515 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:40,528 - 

2022-12-29 10:05:40,528 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:41,346 - Epoch: [78][   10/  113]    Overall Loss 1.528643    Objective Loss 1.528643                                        LR 0.000100    Time 0.081619    
2022-12-29 10:05:41,974 - Epoch: [78][   20/  113]    Overall Loss 1.506314    Objective Loss 1.506314                                        LR 0.000100    Time 0.072231    
2022-12-29 10:05:42,603 - Epoch: [78][   30/  113]    Overall Loss 1.504115    Objective Loss 1.504115                                        LR 0.000100    Time 0.069101    
2022-12-29 10:05:43,227 - Epoch: [78][   40/  113]    Overall Loss 1.491657    Objective Loss 1.491657                                        LR 0.000100    Time 0.067413    
2022-12-29 10:05:43,856 - Epoch: [78][   50/  113]    Overall Loss 1.472820    Objective Loss 1.472820                                        LR 0.000100    Time 0.066486    
2022-12-29 10:05:44,488 - Epoch: [78][   60/  113]    Overall Loss 1.475520    Objective Loss 1.475520                                        LR 0.000100    Time 0.065942    
2022-12-29 10:05:45,120 - Epoch: [78][   70/  113]    Overall Loss 1.467892    Objective Loss 1.467892                                        LR 0.000100    Time 0.065537    
2022-12-29 10:05:45,745 - Epoch: [78][   80/  113]    Overall Loss 1.474283    Objective Loss 1.474283                                        LR 0.000100    Time 0.065151    
2022-12-29 10:05:46,377 - Epoch: [78][   90/  113]    Overall Loss 1.468419    Objective Loss 1.468419                                        LR 0.000100    Time 0.064931    
2022-12-29 10:05:47,005 - Epoch: [78][  100/  113]    Overall Loss 1.472443    Objective Loss 1.472443                                        LR 0.000100    Time 0.064714    
2022-12-29 10:05:47,625 - Epoch: [78][  110/  113]    Overall Loss 1.469995    Objective Loss 1.469995                                        LR 0.000100    Time 0.064459    
2022-12-29 10:05:47,801 - Epoch: [78][  113/  113]    Overall Loss 1.473264    Objective Loss 1.473264    Top1 33.333333    Top5 91.666667    LR 0.000100    Time 0.064305    
2022-12-29 10:05:47,858 - --- validate (epoch=78)-----------
2022-12-29 10:05:47,859 - 200 samples (16 per mini-batch)
2022-12-29 10:05:48,393 - Epoch: [78][   10/   13]    Loss 1.362660    Top1 46.250000    Top5 98.750000    
2022-12-29 10:05:48,478 - Epoch: [78][   13/   13]    Loss 1.435852    Top1 42.000000    Top5 98.000000    
2022-12-29 10:05:48,542 - ==> Top1: 42.000    Top5: 98.000    Loss: 1.436

2022-12-29 10:05:48,543 - ==> Confusion:
[[24  3  0  7  6  0]
 [ 5 18  3  9  4  0]
 [ 2  7  4  7  4  0]
 [ 5  6  5 22  9  0]
 [ 4 16  4  6 15  0]
 [ 1  1  0  1  1  1]]

2022-12-29 10:05:48,545 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:48,545 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:48,569 - 

2022-12-29 10:05:48,569 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:49,387 - Epoch: [79][   10/  113]    Overall Loss 1.534705    Objective Loss 1.534705                                        LR 0.000100    Time 0.081594    
2022-12-29 10:05:50,018 - Epoch: [79][   20/  113]    Overall Loss 1.495743    Objective Loss 1.495743                                        LR 0.000100    Time 0.072330    
2022-12-29 10:05:50,646 - Epoch: [79][   30/  113]    Overall Loss 1.498328    Objective Loss 1.498328                                        LR 0.000100    Time 0.069112    
2022-12-29 10:05:51,278 - Epoch: [79][   40/  113]    Overall Loss 1.473788    Objective Loss 1.473788                                        LR 0.000100    Time 0.067631    
2022-12-29 10:05:51,907 - Epoch: [79][   50/  113]    Overall Loss 1.474670    Objective Loss 1.474670                                        LR 0.000100    Time 0.066677    
2022-12-29 10:05:52,538 - Epoch: [79][   60/  113]    Overall Loss 1.492004    Objective Loss 1.492004                                        LR 0.000100    Time 0.066073    
2022-12-29 10:05:53,167 - Epoch: [79][   70/  113]    Overall Loss 1.505292    Objective Loss 1.505292                                        LR 0.000100    Time 0.065621    
2022-12-29 10:05:53,798 - Epoch: [79][   80/  113]    Overall Loss 1.496075    Objective Loss 1.496075                                        LR 0.000100    Time 0.065289    
2022-12-29 10:05:54,431 - Epoch: [79][   90/  113]    Overall Loss 1.497300    Objective Loss 1.497300                                        LR 0.000100    Time 0.065063    
2022-12-29 10:05:55,056 - Epoch: [79][  100/  113]    Overall Loss 1.490842    Objective Loss 1.490842                                        LR 0.000100    Time 0.064811    
2022-12-29 10:05:55,682 - Epoch: [79][  110/  113]    Overall Loss 1.483119    Objective Loss 1.483119                                        LR 0.000100    Time 0.064605    
2022-12-29 10:05:55,855 - Epoch: [79][  113/  113]    Overall Loss 1.488157    Objective Loss 1.488157    Top1 25.000000    Top5 91.666667    LR 0.000100    Time 0.064410    
2022-12-29 10:05:55,919 - --- validate (epoch=79)-----------
2022-12-29 10:05:55,919 - 200 samples (16 per mini-batch)
2022-12-29 10:05:56,467 - Epoch: [79][   10/   13]    Loss 1.492342    Top1 40.000000    Top5 95.625000    
2022-12-29 10:05:56,552 - Epoch: [79][   13/   13]    Loss 1.497565    Top1 41.000000    Top5 95.500000    
2022-12-29 10:05:56,611 - ==> Top1: 41.000    Top5: 95.500    Loss: 1.498

2022-12-29 10:05:56,612 - ==> Confusion:
[[13  1  0  6  6  0]
 [ 2  9  7  1 19  0]
 [ 0  7  9  1 14  0]
 [ 1 10  4 26  7  0]
 [ 4  5  5  6 25  0]
 [ 2  3  0  1  6  0]]

2022-12-29 10:05:56,615 - ==> Best [Top1: 42.500   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 53]
2022-12-29 10:05:56,615 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:05:56,641 - 

2022-12-29 10:05:56,642 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:05:57,464 - Epoch: [80][   10/  113]    Overall Loss 1.481405    Objective Loss 1.481405                                        LR 0.000100    Time 0.082067    
2022-12-29 10:05:58,096 - Epoch: [80][   20/  113]    Overall Loss 1.478275    Objective Loss 1.478275                                        LR 0.000100    Time 0.072634    
2022-12-29 10:05:58,726 - Epoch: [80][   30/  113]    Overall Loss 1.497656    Objective Loss 1.497656                                        LR 0.000100    Time 0.069380    
2022-12-29 10:05:59,353 - Epoch: [80][   40/  113]    Overall Loss 1.478341    Objective Loss 1.478341                                        LR 0.000100    Time 0.067705    
2022-12-29 10:05:59,981 - Epoch: [80][   50/  113]    Overall Loss 1.476609    Objective Loss 1.476609                                        LR 0.000100    Time 0.066708    
2022-12-29 10:06:00,612 - Epoch: [80][   60/  113]    Overall Loss 1.491295    Objective Loss 1.491295                                        LR 0.000100    Time 0.066100    
2022-12-29 10:06:01,245 - Epoch: [80][   70/  113]    Overall Loss 1.484506    Objective Loss 1.484506                                        LR 0.000100    Time 0.065704    
2022-12-29 10:06:01,874 - Epoch: [80][   80/  113]    Overall Loss 1.473234    Objective Loss 1.473234                                        LR 0.000100    Time 0.065340    
2022-12-29 10:06:02,505 - Epoch: [80][   90/  113]    Overall Loss 1.467681    Objective Loss 1.467681                                        LR 0.000100    Time 0.065086    
2022-12-29 10:06:03,131 - Epoch: [80][  100/  113]    Overall Loss 1.465404    Objective Loss 1.465404                                        LR 0.000100    Time 0.064833    
2022-12-29 10:06:03,753 - Epoch: [80][  110/  113]    Overall Loss 1.475847    Objective Loss 1.475847                                        LR 0.000100    Time 0.064589    
2022-12-29 10:06:03,923 - Epoch: [80][  113/  113]    Overall Loss 1.477078    Objective Loss 1.477078    Top1 33.333333    Top5 95.833333    LR 0.000100    Time 0.064376    
2022-12-29 10:06:03,980 - --- validate (epoch=80)-----------
2022-12-29 10:06:03,981 - 200 samples (16 per mini-batch)
2022-12-29 10:06:04,527 - Epoch: [80][   10/   13]    Loss 1.438880    Top1 45.000000    Top5 96.250000    
2022-12-29 10:06:04,613 - Epoch: [80][   13/   13]    Loss 1.434548    Top1 45.000000    Top5 96.000000    
2022-12-29 10:06:04,662 - ==> Top1: 45.000    Top5: 96.000    Loss: 1.435

2022-12-29 10:06:04,662 - ==> Confusion:
[[14  3  1  8  3  0]
 [ 3 29  1 14  2  0]
 [ 1 11  6  9  3  0]
 [ 3  3  0 37  2  0]
 [ 2 12  4 15  4  0]
 [ 2  5  1  2  0  0]]

2022-12-29 10:06:04,664 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:04,665 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:04,698 - 

2022-12-29 10:06:04,699 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:05,532 - Epoch: [81][   10/  113]    Overall Loss 1.455688    Objective Loss 1.455688                                        LR 0.000100    Time 0.083136    
2022-12-29 10:06:06,162 - Epoch: [81][   20/  113]    Overall Loss 1.412242    Objective Loss 1.412242                                        LR 0.000100    Time 0.073064    
2022-12-29 10:06:06,793 - Epoch: [81][   30/  113]    Overall Loss 1.436074    Objective Loss 1.436074                                        LR 0.000100    Time 0.069727    
2022-12-29 10:06:07,424 - Epoch: [81][   40/  113]    Overall Loss 1.408082    Objective Loss 1.408082                                        LR 0.000100    Time 0.068070    
2022-12-29 10:06:08,050 - Epoch: [81][   50/  113]    Overall Loss 1.419533    Objective Loss 1.419533                                        LR 0.000100    Time 0.066958    
2022-12-29 10:06:08,675 - Epoch: [81][   60/  113]    Overall Loss 1.427458    Objective Loss 1.427458                                        LR 0.000100    Time 0.066201    
2022-12-29 10:06:09,303 - Epoch: [81][   70/  113]    Overall Loss 1.435776    Objective Loss 1.435776                                        LR 0.000100    Time 0.065721    
2022-12-29 10:06:09,931 - Epoch: [81][   80/  113]    Overall Loss 1.431499    Objective Loss 1.431499                                        LR 0.000100    Time 0.065350    
2022-12-29 10:06:10,562 - Epoch: [81][   90/  113]    Overall Loss 1.427138    Objective Loss 1.427138                                        LR 0.000100    Time 0.065088    
2022-12-29 10:06:11,194 - Epoch: [81][  100/  113]    Overall Loss 1.430953    Objective Loss 1.430953                                        LR 0.000100    Time 0.064894    
2022-12-29 10:06:11,824 - Epoch: [81][  110/  113]    Overall Loss 1.432699    Objective Loss 1.432699                                        LR 0.000100    Time 0.064717    
2022-12-29 10:06:11,998 - Epoch: [81][  113/  113]    Overall Loss 1.441418    Objective Loss 1.441418    Top1 20.833333    Top5 95.833333    LR 0.000100    Time 0.064534    
2022-12-29 10:06:12,052 - --- validate (epoch=81)-----------
2022-12-29 10:06:12,053 - 200 samples (16 per mini-batch)
2022-12-29 10:06:12,594 - Epoch: [81][   10/   13]    Loss 1.513400    Top1 36.875000    Top5 99.375000    
2022-12-29 10:06:12,680 - Epoch: [81][   13/   13]    Loss 1.500943    Top1 38.000000    Top5 98.000000    
2022-12-29 10:06:12,731 - ==> Top1: 38.000    Top5: 98.000    Loss: 1.501

2022-12-29 10:06:12,731 - ==> Confusion:
[[13  3  0  9  5  0]
 [ 7 15  2 12 14  0]
 [ 2  7  7  9  8  0]
 [ 5  5  2 26  3  0]
 [ 3  4  4  8 15  0]
 [ 2  5  3  0  2  0]]

2022-12-29 10:06:12,734 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:12,734 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:12,758 - 

2022-12-29 10:06:12,758 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:13,584 - Epoch: [82][   10/  113]    Overall Loss 1.461667    Objective Loss 1.461667                                        LR 0.000100    Time 0.082403    
2022-12-29 10:06:14,216 - Epoch: [82][   20/  113]    Overall Loss 1.484251    Objective Loss 1.484251                                        LR 0.000100    Time 0.072750    
2022-12-29 10:06:14,852 - Epoch: [82][   30/  113]    Overall Loss 1.516108    Objective Loss 1.516108                                        LR 0.000100    Time 0.069684    
2022-12-29 10:06:15,483 - Epoch: [82][   40/  113]    Overall Loss 1.508147    Objective Loss 1.508147                                        LR 0.000100    Time 0.068019    
2022-12-29 10:06:16,109 - Epoch: [82][   50/  113]    Overall Loss 1.509429    Objective Loss 1.509429                                        LR 0.000100    Time 0.066932    
2022-12-29 10:06:16,741 - Epoch: [82][   60/  113]    Overall Loss 1.512942    Objective Loss 1.512942                                        LR 0.000100    Time 0.066308    
2022-12-29 10:06:17,372 - Epoch: [82][   70/  113]    Overall Loss 1.499341    Objective Loss 1.499341                                        LR 0.000100    Time 0.065837    
2022-12-29 10:06:17,998 - Epoch: [82][   80/  113]    Overall Loss 1.483682    Objective Loss 1.483682                                        LR 0.000100    Time 0.065424    
2022-12-29 10:06:18,628 - Epoch: [82][   90/  113]    Overall Loss 1.472371    Objective Loss 1.472371                                        LR 0.000100    Time 0.065152    
2022-12-29 10:06:19,256 - Epoch: [82][  100/  113]    Overall Loss 1.466886    Objective Loss 1.466886                                        LR 0.000100    Time 0.064905    
2022-12-29 10:06:19,884 - Epoch: [82][  110/  113]    Overall Loss 1.470725    Objective Loss 1.470725                                        LR 0.000100    Time 0.064712    
2022-12-29 10:06:20,059 - Epoch: [82][  113/  113]    Overall Loss 1.471527    Objective Loss 1.471527    Top1 41.666667    Top5 95.833333    LR 0.000100    Time 0.064541    
2022-12-29 10:06:20,117 - --- validate (epoch=82)-----------
2022-12-29 10:06:20,118 - 200 samples (16 per mini-batch)
2022-12-29 10:06:20,666 - Epoch: [82][   10/   13]    Loss 1.473602    Top1 38.750000    Top5 96.250000    
2022-12-29 10:06:20,752 - Epoch: [82][   13/   13]    Loss 1.493789    Top1 38.000000    Top5 96.500000    
2022-12-29 10:06:20,813 - ==> Top1: 38.000    Top5: 96.500    Loss: 1.494

2022-12-29 10:06:20,814 - ==> Confusion:
[[ 8  7  2  3  7  0]
 [ 0 12  7  8 13  0]
 [ 0  6 12  5 10  0]
 [ 4  5  6 24 17  0]
 [ 1  6  9  3 20  0]
 [ 1  0  0  0  4  0]]

2022-12-29 10:06:20,817 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:20,817 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:20,840 - 

2022-12-29 10:06:20,841 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:21,688 - Epoch: [83][   10/  113]    Overall Loss 1.563663    Objective Loss 1.563663                                        LR 0.000100    Time 0.084620    
2022-12-29 10:06:22,319 - Epoch: [83][   20/  113]    Overall Loss 1.562218    Objective Loss 1.562218                                        LR 0.000100    Time 0.073860    
2022-12-29 10:06:22,952 - Epoch: [83][   30/  113]    Overall Loss 1.576187    Objective Loss 1.576187                                        LR 0.000100    Time 0.070316    
2022-12-29 10:06:23,581 - Epoch: [83][   40/  113]    Overall Loss 1.533016    Objective Loss 1.533016                                        LR 0.000100    Time 0.068454    
2022-12-29 10:06:24,206 - Epoch: [83][   50/  113]    Overall Loss 1.490176    Objective Loss 1.490176                                        LR 0.000100    Time 0.067238    
2022-12-29 10:06:24,832 - Epoch: [83][   60/  113]    Overall Loss 1.488806    Objective Loss 1.488806                                        LR 0.000100    Time 0.066473    
2022-12-29 10:06:25,464 - Epoch: [83][   70/  113]    Overall Loss 1.483361    Objective Loss 1.483361                                        LR 0.000100    Time 0.065991    
2022-12-29 10:06:26,092 - Epoch: [83][   80/  113]    Overall Loss 1.477277    Objective Loss 1.477277                                        LR 0.000100    Time 0.065591    
2022-12-29 10:06:26,720 - Epoch: [83][   90/  113]    Overall Loss 1.470738    Objective Loss 1.470738                                        LR 0.000100    Time 0.065267    
2022-12-29 10:06:27,346 - Epoch: [83][  100/  113]    Overall Loss 1.471388    Objective Loss 1.471388                                        LR 0.000100    Time 0.064996    
2022-12-29 10:06:27,974 - Epoch: [83][  110/  113]    Overall Loss 1.468126    Objective Loss 1.468126                                        LR 0.000100    Time 0.064797    
2022-12-29 10:06:28,149 - Epoch: [83][  113/  113]    Overall Loss 1.472273    Objective Loss 1.472273    Top1 25.000000    Top5 100.000000    LR 0.000100    Time 0.064622    
2022-12-29 10:06:28,210 - --- validate (epoch=83)-----------
2022-12-29 10:06:28,211 - 200 samples (16 per mini-batch)
2022-12-29 10:06:28,764 - Epoch: [83][   10/   13]    Loss 1.460442    Top1 41.875000    Top5 95.625000    
2022-12-29 10:06:28,849 - Epoch: [83][   13/   13]    Loss 1.443079    Top1 43.000000    Top5 95.500000    
2022-12-29 10:06:28,904 - ==> Top1: 43.000    Top5: 95.500    Loss: 1.443

2022-12-29 10:06:28,904 - ==> Confusion:
[[18  5  2  2  6  0]
 [ 2 16  3  7 18  0]
 [ 2  3  7  4 11  0]
 [ 0  6  1 20 15  0]
 [ 2  8  0  4 25  0]
 [ 2  3  2  1  5  0]]

2022-12-29 10:06:28,906 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:28,907 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:28,916 - 

2022-12-29 10:06:28,916 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:29,755 - Epoch: [84][   10/  113]    Overall Loss 1.412983    Objective Loss 1.412983                                        LR 0.000100    Time 0.083733    
2022-12-29 10:06:30,387 - Epoch: [84][   20/  113]    Overall Loss 1.421903    Objective Loss 1.421903                                        LR 0.000100    Time 0.073457    
2022-12-29 10:06:31,017 - Epoch: [84][   30/  113]    Overall Loss 1.436332    Objective Loss 1.436332                                        LR 0.000100    Time 0.069957    
2022-12-29 10:06:31,653 - Epoch: [84][   40/  113]    Overall Loss 1.411334    Objective Loss 1.411334                                        LR 0.000100    Time 0.068354    
2022-12-29 10:06:32,285 - Epoch: [84][   50/  113]    Overall Loss 1.448748    Objective Loss 1.448748                                        LR 0.000100    Time 0.067314    
2022-12-29 10:06:32,913 - Epoch: [84][   60/  113]    Overall Loss 1.443873    Objective Loss 1.443873                                        LR 0.000100    Time 0.066564    
2022-12-29 10:06:33,542 - Epoch: [84][   70/  113]    Overall Loss 1.436373    Objective Loss 1.436373                                        LR 0.000100    Time 0.066033    
2022-12-29 10:06:34,167 - Epoch: [84][   80/  113]    Overall Loss 1.429282    Objective Loss 1.429282                                        LR 0.000100    Time 0.065578    
2022-12-29 10:06:34,797 - Epoch: [84][   90/  113]    Overall Loss 1.439046    Objective Loss 1.439046                                        LR 0.000100    Time 0.065289    
2022-12-29 10:06:35,424 - Epoch: [84][  100/  113]    Overall Loss 1.436781    Objective Loss 1.436781                                        LR 0.000100    Time 0.065020    
2022-12-29 10:06:36,051 - Epoch: [84][  110/  113]    Overall Loss 1.437337    Objective Loss 1.437337                                        LR 0.000100    Time 0.064812    
2022-12-29 10:06:36,224 - Epoch: [84][  113/  113]    Overall Loss 1.437152    Objective Loss 1.437152    Top1 41.666667    Top5 100.000000    LR 0.000100    Time 0.064619    
2022-12-29 10:06:36,279 - --- validate (epoch=84)-----------
2022-12-29 10:06:36,279 - 200 samples (16 per mini-batch)
2022-12-29 10:06:36,823 - Epoch: [84][   10/   13]    Loss 1.510626    Top1 36.875000    Top5 96.250000    
2022-12-29 10:06:36,913 - Epoch: [84][   13/   13]    Loss 1.441271    Top1 41.500000    Top5 96.500000    
2022-12-29 10:06:36,974 - ==> Top1: 41.500    Top5: 96.500    Loss: 1.441

2022-12-29 10:06:36,974 - ==> Confusion:
[[15  3  1  3  2  0]
 [ 0 11  3 17  9  0]
 [ 0  1  9 11 13  0]
 [ 6  9  5 32  3  0]
 [ 4  7  2  8 16  0]
 [ 5  1  1  0  3  0]]

2022-12-29 10:06:36,977 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:36,977 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:36,992 - 

2022-12-29 10:06:36,992 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:37,815 - Epoch: [85][   10/  113]    Overall Loss 1.479480    Objective Loss 1.479480                                        LR 0.000100    Time 0.082142    
2022-12-29 10:06:38,448 - Epoch: [85][   20/  113]    Overall Loss 1.449067    Objective Loss 1.449067                                        LR 0.000100    Time 0.072708    
2022-12-29 10:06:39,080 - Epoch: [85][   30/  113]    Overall Loss 1.440569    Objective Loss 1.440569                                        LR 0.000100    Time 0.069527    
2022-12-29 10:06:39,712 - Epoch: [85][   40/  113]    Overall Loss 1.442046    Objective Loss 1.442046                                        LR 0.000100    Time 0.067920    
2022-12-29 10:06:40,343 - Epoch: [85][   50/  113]    Overall Loss 1.454644    Objective Loss 1.454644                                        LR 0.000100    Time 0.066959    
2022-12-29 10:06:40,972 - Epoch: [85][   60/  113]    Overall Loss 1.462073    Objective Loss 1.462073                                        LR 0.000100    Time 0.066268    
2022-12-29 10:06:41,603 - Epoch: [85][   70/  113]    Overall Loss 1.462952    Objective Loss 1.462952                                        LR 0.000100    Time 0.065815    
2022-12-29 10:06:42,233 - Epoch: [85][   80/  113]    Overall Loss 1.449933    Objective Loss 1.449933                                        LR 0.000100    Time 0.065456    
2022-12-29 10:06:42,866 - Epoch: [85][   90/  113]    Overall Loss 1.437384    Objective Loss 1.437384                                        LR 0.000100    Time 0.065208    
2022-12-29 10:06:43,498 - Epoch: [85][  100/  113]    Overall Loss 1.444877    Objective Loss 1.444877                                        LR 0.000100    Time 0.064998    
2022-12-29 10:06:44,123 - Epoch: [85][  110/  113]    Overall Loss 1.450375    Objective Loss 1.450375                                        LR 0.000100    Time 0.064768    
2022-12-29 10:06:44,294 - Epoch: [85][  113/  113]    Overall Loss 1.448225    Objective Loss 1.448225    Top1 25.000000    Top5 100.000000    LR 0.000100    Time 0.064558    
2022-12-29 10:06:44,349 - --- validate (epoch=85)-----------
2022-12-29 10:06:44,349 - 200 samples (16 per mini-batch)
2022-12-29 10:06:44,882 - Epoch: [85][   10/   13]    Loss 1.476066    Top1 42.500000    Top5 96.250000    
2022-12-29 10:06:44,967 - Epoch: [85][   13/   13]    Loss 1.505680    Top1 41.000000    Top5 96.500000    
2022-12-29 10:06:45,015 - ==> Top1: 41.000    Top5: 96.500    Loss: 1.506

2022-12-29 10:06:45,015 - ==> Confusion:
[[14  2  1  3  4  0]
 [ 2 17  1 15 14  0]
 [ 3  6  6 13 11  0]
 [ 4  3  1 21  8  0]
 [ 3  8  2  9 24  0]
 [ 0  0  0  3  2  0]]

2022-12-29 10:06:45,018 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:45,018 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:45,041 - 

2022-12-29 10:06:45,042 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:45,868 - Epoch: [86][   10/  113]    Overall Loss 1.469447    Objective Loss 1.469447                                        LR 0.000100    Time 0.082479    
2022-12-29 10:06:46,504 - Epoch: [86][   20/  113]    Overall Loss 1.437197    Objective Loss 1.437197                                        LR 0.000100    Time 0.072968    
2022-12-29 10:06:47,137 - Epoch: [86][   30/  113]    Overall Loss 1.470219    Objective Loss 1.470219                                        LR 0.000100    Time 0.069745    
2022-12-29 10:06:47,773 - Epoch: [86][   40/  113]    Overall Loss 1.434132    Objective Loss 1.434132                                        LR 0.000100    Time 0.068195    
2022-12-29 10:06:48,399 - Epoch: [86][   50/  113]    Overall Loss 1.435387    Objective Loss 1.435387                                        LR 0.000100    Time 0.067073    
2022-12-29 10:06:49,025 - Epoch: [86][   60/  113]    Overall Loss 1.447018    Objective Loss 1.447018                                        LR 0.000100    Time 0.066319    
2022-12-29 10:06:49,650 - Epoch: [86][   70/  113]    Overall Loss 1.439661    Objective Loss 1.439661                                        LR 0.000100    Time 0.065770    
2022-12-29 10:06:50,276 - Epoch: [86][   80/  113]    Overall Loss 1.440132    Objective Loss 1.440132                                        LR 0.000100    Time 0.065367    
2022-12-29 10:06:50,906 - Epoch: [86][   90/  113]    Overall Loss 1.436761    Objective Loss 1.436761                                        LR 0.000100    Time 0.065093    
2022-12-29 10:06:51,537 - Epoch: [86][  100/  113]    Overall Loss 1.430341    Objective Loss 1.430341                                        LR 0.000100    Time 0.064894    
2022-12-29 10:06:52,169 - Epoch: [86][  110/  113]    Overall Loss 1.431374    Objective Loss 1.431374                                        LR 0.000100    Time 0.064731    
2022-12-29 10:06:52,343 - Epoch: [86][  113/  113]    Overall Loss 1.433057    Objective Loss 1.433057    Top1 29.166667    Top5 95.833333    LR 0.000100    Time 0.064549    
2022-12-29 10:06:52,394 - --- validate (epoch=86)-----------
2022-12-29 10:06:52,395 - 200 samples (16 per mini-batch)
2022-12-29 10:06:52,944 - Epoch: [86][   10/   13]    Loss 1.554636    Top1 36.250000    Top5 95.000000    
2022-12-29 10:06:53,028 - Epoch: [86][   13/   13]    Loss 1.532296    Top1 37.500000    Top5 95.000000    
2022-12-29 10:06:53,083 - ==> Top1: 37.500    Top5: 95.000    Loss: 1.532

2022-12-29 10:06:53,084 - ==> Confusion:
[[17  8  2  5  1  0]
 [ 0  7  8  6 12  0]
 [ 0  9 13  4  7  0]
 [ 1  3  4 29  8  0]
 [ 0  7  8 15  9  0]
 [ 0  2  5  4  6  0]]

2022-12-29 10:06:53,086 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:06:53,086 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:06:53,109 - 

2022-12-29 10:06:53,109 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:06:53,944 - Epoch: [87][   10/  113]    Overall Loss 1.447121    Objective Loss 1.447121                                        LR 0.000100    Time 0.083352    
2022-12-29 10:06:54,576 - Epoch: [87][   20/  113]    Overall Loss 1.399583    Objective Loss 1.399583                                        LR 0.000100    Time 0.073267    
2022-12-29 10:06:55,215 - Epoch: [87][   30/  113]    Overall Loss 1.462778    Objective Loss 1.462778                                        LR 0.000100    Time 0.070124    
2022-12-29 10:06:55,849 - Epoch: [87][   40/  113]    Overall Loss 1.468481    Objective Loss 1.468481                                        LR 0.000100    Time 0.068425    
2022-12-29 10:06:56,470 - Epoch: [87][   50/  113]    Overall Loss 1.454046    Objective Loss 1.454046                                        LR 0.000100    Time 0.067149    
2022-12-29 10:06:57,094 - Epoch: [87][   60/  113]    Overall Loss 1.458427    Objective Loss 1.458427                                        LR 0.000100    Time 0.066351    
2022-12-29 10:06:57,727 - Epoch: [87][   70/  113]    Overall Loss 1.456636    Objective Loss 1.456636                                        LR 0.000100    Time 0.065917    
2022-12-29 10:06:58,352 - Epoch: [87][   80/  113]    Overall Loss 1.440164    Objective Loss 1.440164                                        LR 0.000100    Time 0.065475    
2022-12-29 10:06:58,978 - Epoch: [87][   90/  113]    Overall Loss 1.448690    Objective Loss 1.448690                                        LR 0.000100    Time 0.065152    
2022-12-29 10:06:59,603 - Epoch: [87][  100/  113]    Overall Loss 1.445040    Objective Loss 1.445040                                        LR 0.000100    Time 0.064888    
2022-12-29 10:07:00,239 - Epoch: [87][  110/  113]    Overall Loss 1.434997    Objective Loss 1.434997                                        LR 0.000100    Time 0.064759    
2022-12-29 10:07:00,414 - Epoch: [87][  113/  113]    Overall Loss 1.432232    Objective Loss 1.432232    Top1 45.833333    Top5 100.000000    LR 0.000100    Time 0.064592    
2022-12-29 10:07:00,472 - --- validate (epoch=87)-----------
2022-12-29 10:07:00,473 - 200 samples (16 per mini-batch)
2022-12-29 10:07:01,023 - Epoch: [87][   10/   13]    Loss 1.561318    Top1 39.375000    Top5 95.625000    
2022-12-29 10:07:01,108 - Epoch: [87][   13/   13]    Loss 1.579183    Top1 41.500000    Top5 96.000000    
2022-12-29 10:07:01,154 - ==> Top1: 41.500    Top5: 96.000    Loss: 1.579

2022-12-29 10:07:01,154 - ==> Confusion:
[[16  2  0 19  2  0]
 [ 3  8  3 12  3  0]
 [ 3  4  7  4  3  0]
 [ 2  4  1 46  5  0]
 [ 5  6  1 23  6  0]
 [ 0  4  1  4  3  0]]

2022-12-29 10:07:01,156 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:01,157 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:01,174 - 

2022-12-29 10:07:01,174 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:02,007 - Epoch: [88][   10/  113]    Overall Loss 1.543484    Objective Loss 1.543484                                        LR 0.000100    Time 0.083252    
2022-12-29 10:07:02,641 - Epoch: [88][   20/  113]    Overall Loss 1.487747    Objective Loss 1.487747                                        LR 0.000100    Time 0.073268    
2022-12-29 10:07:03,271 - Epoch: [88][   30/  113]    Overall Loss 1.479600    Objective Loss 1.479600                                        LR 0.000100    Time 0.069848    
2022-12-29 10:07:03,911 - Epoch: [88][   40/  113]    Overall Loss 1.477094    Objective Loss 1.477094                                        LR 0.000100    Time 0.068357    
2022-12-29 10:07:04,540 - Epoch: [88][   50/  113]    Overall Loss 1.476463    Objective Loss 1.476463                                        LR 0.000100    Time 0.067275    
2022-12-29 10:07:05,171 - Epoch: [88][   60/  113]    Overall Loss 1.476280    Objective Loss 1.476280                                        LR 0.000100    Time 0.066564    
2022-12-29 10:07:05,801 - Epoch: [88][   70/  113]    Overall Loss 1.467334    Objective Loss 1.467334                                        LR 0.000100    Time 0.066048    
2022-12-29 10:07:06,425 - Epoch: [88][   80/  113]    Overall Loss 1.453022    Objective Loss 1.453022                                        LR 0.000100    Time 0.065584    
2022-12-29 10:07:07,054 - Epoch: [88][   90/  113]    Overall Loss 1.442064    Objective Loss 1.442064                                        LR 0.000100    Time 0.065288    
2022-12-29 10:07:07,681 - Epoch: [88][  100/  113]    Overall Loss 1.433070    Objective Loss 1.433070                                        LR 0.000100    Time 0.065023    
2022-12-29 10:07:08,312 - Epoch: [88][  110/  113]    Overall Loss 1.429844    Objective Loss 1.429844                                        LR 0.000100    Time 0.064841    
2022-12-29 10:07:08,484 - Epoch: [88][  113/  113]    Overall Loss 1.432995    Objective Loss 1.432995    Top1 33.333333    Top5 91.666667    LR 0.000100    Time 0.064641    
2022-12-29 10:07:08,535 - --- validate (epoch=88)-----------
2022-12-29 10:07:08,535 - 200 samples (16 per mini-batch)
2022-12-29 10:07:09,074 - Epoch: [88][   10/   13]    Loss 1.437328    Top1 41.250000    Top5 96.250000    
2022-12-29 10:07:09,158 - Epoch: [88][   13/   13]    Loss 1.468594    Top1 41.000000    Top5 96.000000    
2022-12-29 10:07:09,216 - ==> Top1: 41.000    Top5: 96.000    Loss: 1.469

2022-12-29 10:07:09,216 - ==> Confusion:
[[15  3  0  6  3  0]
 [ 5 19  3 14  6  0]
 [ 1  5  7  8  8  0]
 [ 8  0  1 33  7  0]
 [ 6  8  1 10  8  0]
 [ 2  6  0  2  5  0]]

2022-12-29 10:07:09,219 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:09,220 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:09,242 - 

2022-12-29 10:07:09,242 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:10,071 - Epoch: [89][   10/  113]    Overall Loss 1.371549    Objective Loss 1.371549                                        LR 0.000100    Time 0.082817    
2022-12-29 10:07:10,703 - Epoch: [89][   20/  113]    Overall Loss 1.406639    Objective Loss 1.406639                                        LR 0.000100    Time 0.072981    
2022-12-29 10:07:11,332 - Epoch: [89][   30/  113]    Overall Loss 1.412460    Objective Loss 1.412460                                        LR 0.000100    Time 0.069584    
2022-12-29 10:07:11,959 - Epoch: [89][   40/  113]    Overall Loss 1.408324    Objective Loss 1.408324                                        LR 0.000100    Time 0.067871    
2022-12-29 10:07:12,581 - Epoch: [89][   50/  113]    Overall Loss 1.439317    Objective Loss 1.439317                                        LR 0.000100    Time 0.066723    
2022-12-29 10:07:13,205 - Epoch: [89][   60/  113]    Overall Loss 1.433620    Objective Loss 1.433620                                        LR 0.000100    Time 0.065995    
2022-12-29 10:07:13,833 - Epoch: [89][   70/  113]    Overall Loss 1.442384    Objective Loss 1.442384                                        LR 0.000100    Time 0.065537    
2022-12-29 10:07:14,460 - Epoch: [89][   80/  113]    Overall Loss 1.437301    Objective Loss 1.437301                                        LR 0.000100    Time 0.065168    
2022-12-29 10:07:15,090 - Epoch: [89][   90/  113]    Overall Loss 1.438678    Objective Loss 1.438678                                        LR 0.000100    Time 0.064923    
2022-12-29 10:07:15,716 - Epoch: [89][  100/  113]    Overall Loss 1.429780    Objective Loss 1.429780                                        LR 0.000100    Time 0.064687    
2022-12-29 10:07:16,340 - Epoch: [89][  110/  113]    Overall Loss 1.446293    Objective Loss 1.446293                                        LR 0.000100    Time 0.064478    
2022-12-29 10:07:16,513 - Epoch: [89][  113/  113]    Overall Loss 1.447329    Objective Loss 1.447329    Top1 41.666667    Top5 100.000000    LR 0.000100    Time 0.064287    
2022-12-29 10:07:16,572 - --- validate (epoch=89)-----------
2022-12-29 10:07:16,573 - 200 samples (16 per mini-batch)
2022-12-29 10:07:17,119 - Epoch: [89][   10/   13]    Loss 1.442278    Top1 41.250000    Top5 96.250000    
2022-12-29 10:07:17,204 - Epoch: [89][   13/   13]    Loss 1.423255    Top1 42.500000    Top5 96.500000    
2022-12-29 10:07:17,257 - ==> Top1: 42.500    Top5: 96.500    Loss: 1.423

2022-12-29 10:07:17,257 - ==> Confusion:
[[ 9  4  0  6  3  0]
 [ 0 26  1  6 12  0]
 [ 1 14  8  1 16  0]
 [ 1  5  5 27 12  0]
 [ 1  7  4  3 13  0]
 [ 0  5  2  3  3  2]]

2022-12-29 10:07:17,259 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:17,259 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:17,271 - 

2022-12-29 10:07:17,271 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:18,095 - Epoch: [90][   10/  113]    Overall Loss 1.429018    Objective Loss 1.429018                                        LR 0.000100    Time 0.082267    
2022-12-29 10:07:18,727 - Epoch: [90][   20/  113]    Overall Loss 1.441045    Objective Loss 1.441045                                        LR 0.000100    Time 0.072720    
2022-12-29 10:07:19,360 - Epoch: [90][   30/  113]    Overall Loss 1.436895    Objective Loss 1.436895                                        LR 0.000100    Time 0.069536    
2022-12-29 10:07:19,989 - Epoch: [90][   40/  113]    Overall Loss 1.415482    Objective Loss 1.415482                                        LR 0.000100    Time 0.067876    
2022-12-29 10:07:20,617 - Epoch: [90][   50/  113]    Overall Loss 1.421213    Objective Loss 1.421213                                        LR 0.000100    Time 0.066861    
2022-12-29 10:07:21,244 - Epoch: [90][   60/  113]    Overall Loss 1.416653    Objective Loss 1.416653                                        LR 0.000100    Time 0.066152    
2022-12-29 10:07:21,874 - Epoch: [90][   70/  113]    Overall Loss 1.406099    Objective Loss 1.406099                                        LR 0.000100    Time 0.065692    
2022-12-29 10:07:22,497 - Epoch: [90][   80/  113]    Overall Loss 1.408365    Objective Loss 1.408365                                        LR 0.000100    Time 0.065264    
2022-12-29 10:07:23,125 - Epoch: [90][   90/  113]    Overall Loss 1.407561    Objective Loss 1.407561                                        LR 0.000100    Time 0.064991    
2022-12-29 10:07:23,751 - Epoch: [90][  100/  113]    Overall Loss 1.420605    Objective Loss 1.420605                                        LR 0.000100    Time 0.064744    
2022-12-29 10:07:24,375 - Epoch: [90][  110/  113]    Overall Loss 1.424347    Objective Loss 1.424347                                        LR 0.000100    Time 0.064526    
2022-12-29 10:07:24,545 - Epoch: [90][  113/  113]    Overall Loss 1.424756    Objective Loss 1.424756    Top1 37.500000    Top5 91.666667    LR 0.000100    Time 0.064310    
2022-12-29 10:07:24,584 - --- validate (epoch=90)-----------
2022-12-29 10:07:24,584 - 200 samples (16 per mini-batch)
2022-12-29 10:07:25,139 - Epoch: [90][   10/   13]    Loss 1.436079    Top1 42.500000    Top5 98.125000    
2022-12-29 10:07:25,224 - Epoch: [90][   13/   13]    Loss 1.427503    Top1 42.000000    Top5 97.500000    
2022-12-29 10:07:25,277 - ==> Top1: 42.000    Top5: 97.500    Loss: 1.428

2022-12-29 10:07:25,278 - ==> Confusion:
[[12  3  1  9  2  0]
 [ 6 18  3 14  4  0]
 [ 0  7 14  8  5  0]
 [ 0  4  2 31  4  0]
 [ 0 11  1 25  9  0]
 [ 2  3  0  2  0  0]]

2022-12-29 10:07:25,281 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:25,281 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:25,296 - 

2022-12-29 10:07:25,296 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:26,241 - Epoch: [91][   10/  113]    Overall Loss 1.428434    Objective Loss 1.428434                                        LR 0.000100    Time 0.094355    
2022-12-29 10:07:26,866 - Epoch: [91][   20/  113]    Overall Loss 1.457354    Objective Loss 1.457354                                        LR 0.000100    Time 0.078437    
2022-12-29 10:07:27,495 - Epoch: [91][   30/  113]    Overall Loss 1.466780    Objective Loss 1.466780                                        LR 0.000100    Time 0.073238    
2022-12-29 10:07:28,121 - Epoch: [91][   40/  113]    Overall Loss 1.466323    Objective Loss 1.466323                                        LR 0.000100    Time 0.070563    
2022-12-29 10:07:28,750 - Epoch: [91][   50/  113]    Overall Loss 1.436104    Objective Loss 1.436104                                        LR 0.000100    Time 0.069015    
2022-12-29 10:07:29,376 - Epoch: [91][   60/  113]    Overall Loss 1.430334    Objective Loss 1.430334                                        LR 0.000100    Time 0.067949    
2022-12-29 10:07:30,010 - Epoch: [91][   70/  113]    Overall Loss 1.420321    Objective Loss 1.420321                                        LR 0.000100    Time 0.067286    
2022-12-29 10:07:30,642 - Epoch: [91][   80/  113]    Overall Loss 1.412762    Objective Loss 1.412762                                        LR 0.000100    Time 0.066774    
2022-12-29 10:07:31,275 - Epoch: [91][   90/  113]    Overall Loss 1.415132    Objective Loss 1.415132                                        LR 0.000100    Time 0.066374    
2022-12-29 10:07:31,904 - Epoch: [91][  100/  113]    Overall Loss 1.420324    Objective Loss 1.420324                                        LR 0.000100    Time 0.066029    
2022-12-29 10:07:32,527 - Epoch: [91][  110/  113]    Overall Loss 1.427698    Objective Loss 1.427698                                        LR 0.000100    Time 0.065679    
2022-12-29 10:07:32,695 - Epoch: [91][  113/  113]    Overall Loss 1.428734    Objective Loss 1.428734    Top1 45.833333    Top5 95.833333    LR 0.000100    Time 0.065421    
2022-12-29 10:07:32,755 - --- validate (epoch=91)-----------
2022-12-29 10:07:32,755 - 200 samples (16 per mini-batch)
2022-12-29 10:07:33,296 - Epoch: [91][   10/   13]    Loss 1.510232    Top1 38.750000    Top5 95.625000    
2022-12-29 10:07:33,382 - Epoch: [91][   13/   13]    Loss 1.540439    Top1 38.000000    Top5 95.500000    
2022-12-29 10:07:33,445 - ==> Top1: 38.000    Top5: 95.500    Loss: 1.540

2022-12-29 10:07:33,446 - ==> Confusion:
[[15 10  0  3  7  0]
 [ 5 19  1  1 14  0]
 [ 3  8 12  0  9  0]
 [ 5  9  2 10  9  0]
 [ 2 11  9  2 20  0]
 [ 2  5  2  1  4  0]]

2022-12-29 10:07:33,449 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:33,449 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:33,469 - 

2022-12-29 10:07:33,470 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:34,298 - Epoch: [92][   10/  113]    Overall Loss 1.442717    Objective Loss 1.442717                                        LR 0.000100    Time 0.082757    
2022-12-29 10:07:34,932 - Epoch: [92][   20/  113]    Overall Loss 1.397481    Objective Loss 1.397481                                        LR 0.000100    Time 0.073041    
2022-12-29 10:07:35,561 - Epoch: [92][   30/  113]    Overall Loss 1.417129    Objective Loss 1.417129                                        LR 0.000100    Time 0.069633    
2022-12-29 10:07:36,194 - Epoch: [92][   40/  113]    Overall Loss 1.435256    Objective Loss 1.435256                                        LR 0.000100    Time 0.068046    
2022-12-29 10:07:36,818 - Epoch: [92][   50/  113]    Overall Loss 1.429920    Objective Loss 1.429920                                        LR 0.000100    Time 0.066897    
2022-12-29 10:07:37,445 - Epoch: [92][   60/  113]    Overall Loss 1.428462    Objective Loss 1.428462                                        LR 0.000100    Time 0.066191    
2022-12-29 10:07:38,076 - Epoch: [92][   70/  113]    Overall Loss 1.423246    Objective Loss 1.423246                                        LR 0.000100    Time 0.065755    
2022-12-29 10:07:38,707 - Epoch: [92][   80/  113]    Overall Loss 1.422946    Objective Loss 1.422946                                        LR 0.000100    Time 0.065407    
2022-12-29 10:07:39,340 - Epoch: [92][   90/  113]    Overall Loss 1.427578    Objective Loss 1.427578                                        LR 0.000100    Time 0.065169    
2022-12-29 10:07:39,972 - Epoch: [92][  100/  113]    Overall Loss 1.417997    Objective Loss 1.417997                                        LR 0.000100    Time 0.064965    
2022-12-29 10:07:40,597 - Epoch: [92][  110/  113]    Overall Loss 1.432948    Objective Loss 1.432948                                        LR 0.000100    Time 0.064738    
2022-12-29 10:07:40,771 - Epoch: [92][  113/  113]    Overall Loss 1.440978    Objective Loss 1.440978    Top1 29.166667    Top5 91.666667    LR 0.000100    Time 0.064553    
2022-12-29 10:07:40,831 - --- validate (epoch=92)-----------
2022-12-29 10:07:40,831 - 200 samples (16 per mini-batch)
2022-12-29 10:07:41,385 - Epoch: [92][   10/   13]    Loss 1.440739    Top1 42.500000    Top5 97.500000    
2022-12-29 10:07:41,470 - Epoch: [92][   13/   13]    Loss 1.444463    Top1 42.000000    Top5 98.000000    
2022-12-29 10:07:41,532 - ==> Top1: 42.000    Top5: 98.000    Loss: 1.444

2022-12-29 10:07:41,533 - ==> Confusion:
[[13  5  0  8  4  0]
 [ 0 17  3  7 10  0]
 [ 1  5 10  9  9  0]
 [ 3  2  3 28 12  0]
 [ 3  6  2 11 15  0]
 [ 5  2  1  0  5  1]]

2022-12-29 10:07:41,535 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:41,535 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:41,548 - 

2022-12-29 10:07:41,549 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:42,403 - Epoch: [93][   10/  113]    Overall Loss 1.444158    Objective Loss 1.444158                                        LR 0.000100    Time 0.085352    
2022-12-29 10:07:43,038 - Epoch: [93][   20/  113]    Overall Loss 1.424410    Objective Loss 1.424410                                        LR 0.000100    Time 0.074403    
2022-12-29 10:07:43,669 - Epoch: [93][   30/  113]    Overall Loss 1.433907    Objective Loss 1.433907                                        LR 0.000100    Time 0.070616    
2022-12-29 10:07:44,307 - Epoch: [93][   40/  113]    Overall Loss 1.439532    Objective Loss 1.439532                                        LR 0.000100    Time 0.068890    
2022-12-29 10:07:44,941 - Epoch: [93][   50/  113]    Overall Loss 1.450532    Objective Loss 1.450532                                        LR 0.000100    Time 0.067794    
2022-12-29 10:07:45,573 - Epoch: [93][   60/  113]    Overall Loss 1.439782    Objective Loss 1.439782                                        LR 0.000100    Time 0.067012    
2022-12-29 10:07:46,206 - Epoch: [93][   70/  113]    Overall Loss 1.446721    Objective Loss 1.446721                                        LR 0.000100    Time 0.066478    
2022-12-29 10:07:46,835 - Epoch: [93][   80/  113]    Overall Loss 1.443797    Objective Loss 1.443797                                        LR 0.000100    Time 0.066029    
2022-12-29 10:07:47,466 - Epoch: [93][   90/  113]    Overall Loss 1.438131    Objective Loss 1.438131                                        LR 0.000100    Time 0.065692    
2022-12-29 10:07:48,097 - Epoch: [93][  100/  113]    Overall Loss 1.439271    Objective Loss 1.439271                                        LR 0.000100    Time 0.065430    
2022-12-29 10:07:48,722 - Epoch: [93][  110/  113]    Overall Loss 1.442457    Objective Loss 1.442457                                        LR 0.000100    Time 0.065161    
2022-12-29 10:07:48,896 - Epoch: [93][  113/  113]    Overall Loss 1.441179    Objective Loss 1.441179    Top1 54.166667    Top5 95.833333    LR 0.000100    Time 0.064966    
2022-12-29 10:07:48,954 - --- validate (epoch=93)-----------
2022-12-29 10:07:48,955 - 200 samples (16 per mini-batch)
2022-12-29 10:07:49,499 - Epoch: [93][   10/   13]    Loss 1.593465    Top1 34.375000    Top5 95.000000    
2022-12-29 10:07:49,584 - Epoch: [93][   13/   13]    Loss 1.534344    Top1 37.500000    Top5 96.000000    
2022-12-29 10:07:49,630 - ==> Top1: 37.500    Top5: 96.000    Loss: 1.534

2022-12-29 10:07:49,631 - ==> Confusion:
[[16  4  0  3  8  0]
 [ 7 14  2  7 12  0]
 [ 1  7  4  4 18  0]
 [ 3  3  2 20 11  0]
 [ 2 11  0  4 21  0]
 [ 1  3  2  1  9  0]]

2022-12-29 10:07:49,633 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:49,634 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:49,655 - 

2022-12-29 10:07:49,655 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:50,480 - Epoch: [94][   10/  113]    Overall Loss 1.458661    Objective Loss 1.458661                                        LR 0.000100    Time 0.082309    
2022-12-29 10:07:51,120 - Epoch: [94][   20/  113]    Overall Loss 1.425752    Objective Loss 1.425752                                        LR 0.000100    Time 0.073114    
2022-12-29 10:07:51,761 - Epoch: [94][   30/  113]    Overall Loss 1.459137    Objective Loss 1.459137                                        LR 0.000100    Time 0.070116    
2022-12-29 10:07:52,400 - Epoch: [94][   40/  113]    Overall Loss 1.438822    Objective Loss 1.438822                                        LR 0.000100    Time 0.068551    
2022-12-29 10:07:53,032 - Epoch: [94][   50/  113]    Overall Loss 1.418382    Objective Loss 1.418382                                        LR 0.000100    Time 0.067456    
2022-12-29 10:07:53,665 - Epoch: [94][   60/  113]    Overall Loss 1.421030    Objective Loss 1.421030                                        LR 0.000100    Time 0.066766    
2022-12-29 10:07:54,305 - Epoch: [94][   70/  113]    Overall Loss 1.425869    Objective Loss 1.425869                                        LR 0.000100    Time 0.066355    
2022-12-29 10:07:54,935 - Epoch: [94][   80/  113]    Overall Loss 1.417400    Objective Loss 1.417400                                        LR 0.000100    Time 0.065941    
2022-12-29 10:07:55,570 - Epoch: [94][   90/  113]    Overall Loss 1.414895    Objective Loss 1.414895                                        LR 0.000100    Time 0.065658    
2022-12-29 10:07:56,203 - Epoch: [94][  100/  113]    Overall Loss 1.416397    Objective Loss 1.416397                                        LR 0.000100    Time 0.065422    
2022-12-29 10:07:56,830 - Epoch: [94][  110/  113]    Overall Loss 1.419788    Objective Loss 1.419788                                        LR 0.000100    Time 0.065171    
2022-12-29 10:07:57,001 - Epoch: [94][  113/  113]    Overall Loss 1.418611    Objective Loss 1.418611    Top1 33.333333    Top5 95.833333    LR 0.000100    Time 0.064947    
2022-12-29 10:07:57,040 - --- validate (epoch=94)-----------
2022-12-29 10:07:57,041 - 200 samples (16 per mini-batch)
2022-12-29 10:07:57,584 - Epoch: [94][   10/   13]    Loss 1.475415    Top1 43.750000    Top5 95.625000    
2022-12-29 10:07:57,669 - Epoch: [94][   13/   13]    Loss 1.476857    Top1 45.000000    Top5 94.000000    
2022-12-29 10:07:57,732 - ==> Top1: 45.000    Top5: 94.000    Loss: 1.477

2022-12-29 10:07:57,732 - ==> Confusion:
[[17  5  0  7  4  1]
 [ 2 20  1 10  1  0]
 [ 4  9  5 10  5  0]
 [ 9  6  0 39  2  0]
 [ 1 10  0 12  9  0]
 [ 3  3  0  4  1  0]]

2022-12-29 10:07:57,735 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:07:57,735 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:07:57,761 - 

2022-12-29 10:07:57,761 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:07:58,587 - Epoch: [95][   10/  113]    Overall Loss 1.493195    Objective Loss 1.493195                                        LR 0.000100    Time 0.082437    
2022-12-29 10:07:59,226 - Epoch: [95][   20/  113]    Overall Loss 1.425672    Objective Loss 1.425672                                        LR 0.000100    Time 0.073125    
2022-12-29 10:07:59,863 - Epoch: [95][   30/  113]    Overall Loss 1.404758    Objective Loss 1.404758                                        LR 0.000100    Time 0.069995    
2022-12-29 10:08:00,495 - Epoch: [95][   40/  113]    Overall Loss 1.422599    Objective Loss 1.422599                                        LR 0.000100    Time 0.068283    
2022-12-29 10:08:01,130 - Epoch: [95][   50/  113]    Overall Loss 1.416676    Objective Loss 1.416676                                        LR 0.000100    Time 0.067309    
2022-12-29 10:08:01,772 - Epoch: [95][   60/  113]    Overall Loss 1.432885    Objective Loss 1.432885                                        LR 0.000100    Time 0.066784    
2022-12-29 10:08:02,404 - Epoch: [95][   70/  113]    Overall Loss 1.430903    Objective Loss 1.430903                                        LR 0.000100    Time 0.066266    
2022-12-29 10:08:03,037 - Epoch: [95][   80/  113]    Overall Loss 1.428135    Objective Loss 1.428135                                        LR 0.000100    Time 0.065888    
2022-12-29 10:08:03,668 - Epoch: [95][   90/  113]    Overall Loss 1.439335    Objective Loss 1.439335                                        LR 0.000100    Time 0.065577    
2022-12-29 10:08:04,302 - Epoch: [95][  100/  113]    Overall Loss 1.440314    Objective Loss 1.440314                                        LR 0.000100    Time 0.065348    
2022-12-29 10:08:04,931 - Epoch: [95][  110/  113]    Overall Loss 1.439193    Objective Loss 1.439193                                        LR 0.000100    Time 0.065120    
2022-12-29 10:08:05,106 - Epoch: [95][  113/  113]    Overall Loss 1.438920    Objective Loss 1.438920    Top1 37.500000    Top5 100.000000    LR 0.000100    Time 0.064934    
2022-12-29 10:08:05,162 - --- validate (epoch=95)-----------
2022-12-29 10:08:05,162 - 200 samples (16 per mini-batch)
2022-12-29 10:08:05,721 - Epoch: [95][   10/   13]    Loss 1.421315    Top1 45.000000    Top5 96.250000    
2022-12-29 10:08:05,808 - Epoch: [95][   13/   13]    Loss 1.416306    Top1 43.000000    Top5 96.500000    
2022-12-29 10:08:05,871 - ==> Top1: 43.000    Top5: 96.500    Loss: 1.416

2022-12-29 10:08:05,872 - ==> Confusion:
[[18  4  0  5  2  0]
 [ 7 23  0  9  9  0]
 [ 2 18  5  7  3  0]
 [ 3  7  0 28  1  0]
 [ 2 17  0  9 11  0]
 [ 2  3  0  2  2  1]]

2022-12-29 10:08:05,874 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:08:05,875 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:05,897 - 

2022-12-29 10:08:05,898 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:06,733 - Epoch: [96][   10/  113]    Overall Loss 1.428863    Objective Loss 1.428863                                        LR 0.000100    Time 0.083428    
2022-12-29 10:08:07,364 - Epoch: [96][   20/  113]    Overall Loss 1.447963    Objective Loss 1.447963                                        LR 0.000100    Time 0.073272    
2022-12-29 10:08:07,998 - Epoch: [96][   30/  113]    Overall Loss 1.449967    Objective Loss 1.449967                                        LR 0.000100    Time 0.069946    
2022-12-29 10:08:08,630 - Epoch: [96][   40/  113]    Overall Loss 1.444209    Objective Loss 1.444209                                        LR 0.000100    Time 0.068261    
2022-12-29 10:08:09,260 - Epoch: [96][   50/  113]    Overall Loss 1.427785    Objective Loss 1.427785                                        LR 0.000100    Time 0.067199    
2022-12-29 10:08:09,896 - Epoch: [96][   60/  113]    Overall Loss 1.413532    Objective Loss 1.413532                                        LR 0.000100    Time 0.066589    
2022-12-29 10:08:10,531 - Epoch: [96][   70/  113]    Overall Loss 1.407294    Objective Loss 1.407294                                        LR 0.000100    Time 0.066142    
2022-12-29 10:08:11,158 - Epoch: [96][   80/  113]    Overall Loss 1.430281    Objective Loss 1.430281                                        LR 0.000100    Time 0.065707    
2022-12-29 10:08:11,793 - Epoch: [96][   90/  113]    Overall Loss 1.430068    Objective Loss 1.430068                                        LR 0.000100    Time 0.065456    
2022-12-29 10:08:12,433 - Epoch: [96][  100/  113]    Overall Loss 1.433578    Objective Loss 1.433578                                        LR 0.000100    Time 0.065296    
2022-12-29 10:08:13,067 - Epoch: [96][  110/  113]    Overall Loss 1.439448    Objective Loss 1.439448                                        LR 0.000100    Time 0.065126    
2022-12-29 10:08:13,239 - Epoch: [96][  113/  113]    Overall Loss 1.439014    Objective Loss 1.439014    Top1 29.166667    Top5 100.000000    LR 0.000100    Time 0.064911    
2022-12-29 10:08:13,292 - --- validate (epoch=96)-----------
2022-12-29 10:08:13,292 - 200 samples (16 per mini-batch)
2022-12-29 10:08:13,841 - Epoch: [96][   10/   13]    Loss 1.590630    Top1 36.875000    Top5 95.000000    
2022-12-29 10:08:13,926 - Epoch: [96][   13/   13]    Loss 1.543264    Top1 38.000000    Top5 95.000000    
2022-12-29 10:08:13,983 - ==> Top1: 38.000    Top5: 95.000    Loss: 1.543

2022-12-29 10:08:13,983 - ==> Confusion:
[[14  4  1  3  9  0]
 [ 1  9  3  3 17  0]
 [ 2  3  8  0 14  0]
 [ 2  7 11 21 18  0]
 [ 1  4  4  3 24  0]
 [ 4  0  3  0  7  0]]

2022-12-29 10:08:13,985 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:08:13,985 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:13,999 - 

2022-12-29 10:08:13,999 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:14,841 - Epoch: [97][   10/  113]    Overall Loss 1.325923    Objective Loss 1.325923                                        LR 0.000100    Time 0.084138    
2022-12-29 10:08:15,476 - Epoch: [97][   20/  113]    Overall Loss 1.404318    Objective Loss 1.404318                                        LR 0.000100    Time 0.073780    
2022-12-29 10:08:16,109 - Epoch: [97][   30/  113]    Overall Loss 1.403535    Objective Loss 1.403535                                        LR 0.000100    Time 0.070257    
2022-12-29 10:08:16,746 - Epoch: [97][   40/  113]    Overall Loss 1.424194    Objective Loss 1.424194                                        LR 0.000100    Time 0.068600    
2022-12-29 10:08:17,374 - Epoch: [97][   50/  113]    Overall Loss 1.413898    Objective Loss 1.413898                                        LR 0.000100    Time 0.067451    
2022-12-29 10:08:18,002 - Epoch: [97][   60/  113]    Overall Loss 1.406225    Objective Loss 1.406225                                        LR 0.000100    Time 0.066662    
2022-12-29 10:08:18,637 - Epoch: [97][   70/  113]    Overall Loss 1.403770    Objective Loss 1.403770                                        LR 0.000100    Time 0.066203    
2022-12-29 10:08:19,269 - Epoch: [97][   80/  113]    Overall Loss 1.402672    Objective Loss 1.402672                                        LR 0.000100    Time 0.065825    
2022-12-29 10:08:19,905 - Epoch: [97][   90/  113]    Overall Loss 1.410692    Objective Loss 1.410692                                        LR 0.000100    Time 0.065569    
2022-12-29 10:08:20,536 - Epoch: [97][  100/  113]    Overall Loss 1.406527    Objective Loss 1.406527                                        LR 0.000100    Time 0.065319    
2022-12-29 10:08:21,165 - Epoch: [97][  110/  113]    Overall Loss 1.412239    Objective Loss 1.412239                                        LR 0.000100    Time 0.065088    
2022-12-29 10:08:21,340 - Epoch: [97][  113/  113]    Overall Loss 1.412059    Objective Loss 1.412059    Top1 50.000000    Top5 95.833333    LR 0.000100    Time 0.064908    
2022-12-29 10:08:21,388 - --- validate (epoch=97)-----------
2022-12-29 10:08:21,388 - 200 samples (16 per mini-batch)
2022-12-29 10:08:21,928 - Epoch: [97][   10/   13]    Loss 1.372418    Top1 44.375000    Top5 96.875000    
2022-12-29 10:08:22,013 - Epoch: [97][   13/   13]    Loss 1.414794    Top1 41.500000    Top5 96.000000    
2022-12-29 10:08:22,060 - ==> Top1: 41.500    Top5: 96.000    Loss: 1.415

2022-12-29 10:08:22,060 - ==> Confusion:
[[21  2  1  3  1  0]
 [ 6 24  4  8  8  0]
 [ 6 13 15  4  2  0]
 [ 5 13  8 15  3  0]
 [ 5  8  4  3  8  0]
 [ 2  5  0  3  0  0]]

2022-12-29 10:08:22,062 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:08:22,062 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:22,077 - 

2022-12-29 10:08:22,077 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:22,906 - Epoch: [98][   10/  113]    Overall Loss 1.424679    Objective Loss 1.424679                                        LR 0.000100    Time 0.082814    
2022-12-29 10:08:23,535 - Epoch: [98][   20/  113]    Overall Loss 1.433110    Objective Loss 1.433110                                        LR 0.000100    Time 0.072824    
2022-12-29 10:08:24,168 - Epoch: [98][   30/  113]    Overall Loss 1.424697    Objective Loss 1.424697                                        LR 0.000100    Time 0.069622    
2022-12-29 10:08:24,794 - Epoch: [98][   40/  113]    Overall Loss 1.409300    Objective Loss 1.409300                                        LR 0.000100    Time 0.067856    
2022-12-29 10:08:25,421 - Epoch: [98][   50/  113]    Overall Loss 1.409829    Objective Loss 1.409829                                        LR 0.000100    Time 0.066816    
2022-12-29 10:08:26,052 - Epoch: [98][   60/  113]    Overall Loss 1.414535    Objective Loss 1.414535                                        LR 0.000100    Time 0.066185    
2022-12-29 10:08:26,682 - Epoch: [98][   70/  113]    Overall Loss 1.430694    Objective Loss 1.430694                                        LR 0.000100    Time 0.065724    
2022-12-29 10:08:27,309 - Epoch: [98][   80/  113]    Overall Loss 1.418377    Objective Loss 1.418377                                        LR 0.000100    Time 0.065348    
2022-12-29 10:08:27,941 - Epoch: [98][   90/  113]    Overall Loss 1.417685    Objective Loss 1.417685                                        LR 0.000100    Time 0.065100    
2022-12-29 10:08:28,571 - Epoch: [98][  100/  113]    Overall Loss 1.411221    Objective Loss 1.411221                                        LR 0.000100    Time 0.064890    
2022-12-29 10:08:29,196 - Epoch: [98][  110/  113]    Overall Loss 1.411213    Objective Loss 1.411213                                        LR 0.000100    Time 0.064663    
2022-12-29 10:08:29,366 - Epoch: [98][  113/  113]    Overall Loss 1.409620    Objective Loss 1.409620    Top1 41.666667    Top5 100.000000    LR 0.000100    Time 0.064452    
2022-12-29 10:08:29,421 - --- validate (epoch=98)-----------
2022-12-29 10:08:29,422 - 200 samples (16 per mini-batch)
2022-12-29 10:08:29,964 - Epoch: [98][   10/   13]    Loss 1.518023    Top1 38.750000    Top5 97.500000    
2022-12-29 10:08:30,050 - Epoch: [98][   13/   13]    Loss 1.478714    Top1 40.500000    Top5 97.500000    
2022-12-29 10:08:30,112 - ==> Top1: 40.500    Top5: 97.500    Loss: 1.479

2022-12-29 10:08:30,113 - ==> Confusion:
[[17  4  1 15  5  0]
 [ 2 17  5 17 10  0]
 [ 1 10  3  6  1  0]
 [ 3  0  2 31  3  0]
 [ 2  8  0 18 13  0]
 [ 0  3  1  2  0  0]]

2022-12-29 10:08:30,117 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:08:30,117 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:30,138 - 

2022-12-29 10:08:30,138 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:30,974 - Epoch: [99][   10/  113]    Overall Loss 1.352211    Objective Loss 1.352211                                        LR 0.000100    Time 0.083476    
2022-12-29 10:08:31,622 - Epoch: [99][   20/  113]    Overall Loss 1.364067    Objective Loss 1.364067                                        LR 0.000100    Time 0.074086    
2022-12-29 10:08:32,262 - Epoch: [99][   30/  113]    Overall Loss 1.396560    Objective Loss 1.396560                                        LR 0.000100    Time 0.070723    
2022-12-29 10:08:32,900 - Epoch: [99][   40/  113]    Overall Loss 1.372972    Objective Loss 1.372972                                        LR 0.000100    Time 0.068979    
2022-12-29 10:08:33,535 - Epoch: [99][   50/  113]    Overall Loss 1.377035    Objective Loss 1.377035                                        LR 0.000100    Time 0.067866    
2022-12-29 10:08:34,162 - Epoch: [99][   60/  113]    Overall Loss 1.375476    Objective Loss 1.375476                                        LR 0.000100    Time 0.067003    
2022-12-29 10:08:34,799 - Epoch: [99][   70/  113]    Overall Loss 1.389507    Objective Loss 1.389507                                        LR 0.000100    Time 0.066527    
2022-12-29 10:08:35,433 - Epoch: [99][   80/  113]    Overall Loss 1.402637    Objective Loss 1.402637                                        LR 0.000100    Time 0.066125    
2022-12-29 10:08:36,070 - Epoch: [99][   90/  113]    Overall Loss 1.400605    Objective Loss 1.400605                                        LR 0.000100    Time 0.065849    
2022-12-29 10:08:36,704 - Epoch: [99][  100/  113]    Overall Loss 1.402192    Objective Loss 1.402192                                        LR 0.000100    Time 0.065605    
2022-12-29 10:08:37,333 - Epoch: [99][  110/  113]    Overall Loss 1.402656    Objective Loss 1.402656                                        LR 0.000100    Time 0.065348    
2022-12-29 10:08:37,507 - Epoch: [99][  113/  113]    Overall Loss 1.403830    Objective Loss 1.403830    Top1 45.833333    Top5 95.833333    LR 0.000100    Time 0.065147    
2022-12-29 10:08:37,544 - --- validate (epoch=99)-----------
2022-12-29 10:08:37,545 - 200 samples (16 per mini-batch)
2022-12-29 10:08:38,093 - Epoch: [99][   10/   13]    Loss 1.425281    Top1 39.375000    Top5 98.125000    
2022-12-29 10:08:38,176 - Epoch: [99][   13/   13]    Loss 1.420628    Top1 41.000000    Top5 97.000000    
2022-12-29 10:08:38,239 - ==> Top1: 41.000    Top5: 97.000    Loss: 1.421

2022-12-29 10:08:38,239 - ==> Confusion:
[[17  1  1 15  1  0]
 [ 2 11  1 17  9  0]
 [ 1 11  4 14  6  0]
 [ 4  3  1 37  4  0]
 [ 1  6  1 14 13  0]
 [ 3  1  0  0  1  0]]

2022-12-29 10:08:38,242 - ==> Best [Top1: 45.000   Top5: 96.000   Sparsity:0.00   Params: 289216 on epoch: 80]
2022-12-29 10:08:38,242 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:38,255 - 

2022-12-29 10:08:38,255 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:39,080 - Epoch: [100][   10/  113]    Overall Loss 1.443487    Objective Loss 1.443487                                        LR 0.000050    Time 0.082359    
2022-12-29 10:08:39,716 - Epoch: [100][   20/  113]    Overall Loss 1.436970    Objective Loss 1.436970                                        LR 0.000050    Time 0.072930    
2022-12-29 10:08:40,355 - Epoch: [100][   30/  113]    Overall Loss 1.418301    Objective Loss 1.418301                                        LR 0.000050    Time 0.069888    
2022-12-29 10:08:40,996 - Epoch: [100][   40/  113]    Overall Loss 1.434413    Objective Loss 1.434413                                        LR 0.000050    Time 0.068437    
2022-12-29 10:08:41,635 - Epoch: [100][   50/  113]    Overall Loss 1.404899    Objective Loss 1.404899                                        LR 0.000050    Time 0.067522    
2022-12-29 10:08:42,269 - Epoch: [100][   60/  113]    Overall Loss 1.393756    Objective Loss 1.393756                                        LR 0.000050    Time 0.066831    
2022-12-29 10:08:42,907 - Epoch: [100][   70/  113]    Overall Loss 1.392379    Objective Loss 1.392379                                        LR 0.000050    Time 0.066390    
2022-12-29 10:08:43,543 - Epoch: [100][   80/  113]    Overall Loss 1.376329    Objective Loss 1.376329                                        LR 0.000050    Time 0.066029    
2022-12-29 10:08:44,185 - Epoch: [100][   90/  113]    Overall Loss 1.366508    Objective Loss 1.366508                                        LR 0.000050    Time 0.065820    
2022-12-29 10:08:44,829 - Epoch: [100][  100/  113]    Overall Loss 1.367410    Objective Loss 1.367410                                        LR 0.000050    Time 0.065673    
2022-12-29 10:08:45,458 - Epoch: [100][  110/  113]    Overall Loss 1.367578    Objective Loss 1.367578                                        LR 0.000050    Time 0.065421    
2022-12-29 10:08:45,629 - Epoch: [100][  113/  113]    Overall Loss 1.363846    Objective Loss 1.363846    Top1 62.500000    Top5 100.000000    LR 0.000050    Time 0.065194    
2022-12-29 10:08:45,684 - --- validate (epoch=100)-----------
2022-12-29 10:08:45,685 - 200 samples (16 per mini-batch)
2022-12-29 10:08:46,226 - Epoch: [100][   10/   13]    Loss 1.351367    Top1 47.500000    Top5 97.500000    
2022-12-29 10:08:46,311 - Epoch: [100][   13/   13]    Loss 1.372167    Top1 48.000000    Top5 96.500000    
2022-12-29 10:08:46,354 - ==> Top1: 48.000    Top5: 96.500    Loss: 1.372

2022-12-29 10:08:46,355 - ==> Confusion:
[[23  2  2 10  6  0]
 [ 3 14  2  8  8  0]
 [ 1  6 12  7  8  0]
 [ 2  9  1 29  7  0]
 [ 0  5  2  9 18  0]
 [ 0  1  0  0  5  0]]

2022-12-29 10:08:46,357 - ==> Best [Top1: 48.000   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 100]
2022-12-29 10:08:46,358 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:46,383 - 

2022-12-29 10:08:46,384 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:47,222 - Epoch: [101][   10/  113]    Overall Loss 1.443383    Objective Loss 1.443383                                        LR 0.000050    Time 0.083695    
2022-12-29 10:08:47,854 - Epoch: [101][   20/  113]    Overall Loss 1.416828    Objective Loss 1.416828                                        LR 0.000050    Time 0.073389    
2022-12-29 10:08:48,491 - Epoch: [101][   30/  113]    Overall Loss 1.417379    Objective Loss 1.417379                                        LR 0.000050    Time 0.070139    
2022-12-29 10:08:49,127 - Epoch: [101][   40/  113]    Overall Loss 1.405735    Objective Loss 1.405735                                        LR 0.000050    Time 0.068496    
2022-12-29 10:08:49,759 - Epoch: [101][   50/  113]    Overall Loss 1.413717    Objective Loss 1.413717                                        LR 0.000050    Time 0.067438    
2022-12-29 10:08:50,389 - Epoch: [101][   60/  113]    Overall Loss 1.401172    Objective Loss 1.401172                                        LR 0.000050    Time 0.066689    
2022-12-29 10:08:51,027 - Epoch: [101][   70/  113]    Overall Loss 1.398375    Objective Loss 1.398375                                        LR 0.000050    Time 0.066271    
2022-12-29 10:08:51,666 - Epoch: [101][   80/  113]    Overall Loss 1.385502    Objective Loss 1.385502                                        LR 0.000050    Time 0.065963    
2022-12-29 10:08:52,298 - Epoch: [101][   90/  113]    Overall Loss 1.384903    Objective Loss 1.384903                                        LR 0.000050    Time 0.065647    
2022-12-29 10:08:52,927 - Epoch: [101][  100/  113]    Overall Loss 1.386676    Objective Loss 1.386676                                        LR 0.000050    Time 0.065377    
2022-12-29 10:08:53,550 - Epoch: [101][  110/  113]    Overall Loss 1.381351    Objective Loss 1.381351                                        LR 0.000050    Time 0.065089    
2022-12-29 10:08:53,725 - Epoch: [101][  113/  113]    Overall Loss 1.381374    Objective Loss 1.381374    Top1 54.166667    Top5 91.666667    LR 0.000050    Time 0.064909    
2022-12-29 10:08:53,784 - --- validate (epoch=101)-----------
2022-12-29 10:08:53,784 - 200 samples (16 per mini-batch)
2022-12-29 10:08:54,339 - Epoch: [101][   10/   13]    Loss 1.375801    Top1 38.125000    Top5 98.125000    
2022-12-29 10:08:54,426 - Epoch: [101][   13/   13]    Loss 1.352563    Top1 42.000000    Top5 98.500000    
2022-12-29 10:08:54,479 - ==> Top1: 42.000    Top5: 98.500    Loss: 1.353

2022-12-29 10:08:54,480 - ==> Confusion:
[[19  2  1  6  6  0]
 [ 0 19  2 11 10  0]
 [ 1 10  5  9  8  0]
 [ 2  6  1 32  8  0]
 [ 3  7  3  9  7  1]
 [ 3  2  0  1  4  2]]

2022-12-29 10:08:54,482 - ==> Best [Top1: 48.000   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 100]
2022-12-29 10:08:54,482 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:08:54,499 - 

2022-12-29 10:08:54,499 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:08:55,330 - Epoch: [102][   10/  113]    Overall Loss 1.371927    Objective Loss 1.371927                                        LR 0.000050    Time 0.082989    
2022-12-29 10:08:55,962 - Epoch: [102][   20/  113]    Overall Loss 1.325115    Objective Loss 1.325115                                        LR 0.000050    Time 0.073054    
2022-12-29 10:08:56,599 - Epoch: [102][   30/  113]    Overall Loss 1.329423    Objective Loss 1.329423                                        LR 0.000050    Time 0.069911    
2022-12-29 10:08:57,228 - Epoch: [102][   40/  113]    Overall Loss 1.367436    Objective Loss 1.367436                                        LR 0.000050    Time 0.068147    
2022-12-29 10:08:57,856 - Epoch: [102][   50/  113]    Overall Loss 1.368984    Objective Loss 1.368984                                        LR 0.000050    Time 0.067075    
2022-12-29 10:08:58,486 - Epoch: [102][   60/  113]    Overall Loss 1.365323    Objective Loss 1.365323                                        LR 0.000050    Time 0.066390    
2022-12-29 10:08:59,120 - Epoch: [102][   70/  113]    Overall Loss 1.375780    Objective Loss 1.375780                                        LR 0.000050    Time 0.065953    
2022-12-29 10:08:59,750 - Epoch: [102][   80/  113]    Overall Loss 1.387975    Objective Loss 1.387975                                        LR 0.000050    Time 0.065580    
2022-12-29 10:09:00,382 - Epoch: [102][   90/  113]    Overall Loss 1.386702    Objective Loss 1.386702                                        LR 0.000050    Time 0.065306    
2022-12-29 10:09:01,018 - Epoch: [102][  100/  113]    Overall Loss 1.392053    Objective Loss 1.392053                                        LR 0.000050    Time 0.065130    
2022-12-29 10:09:01,645 - Epoch: [102][  110/  113]    Overall Loss 1.396158    Objective Loss 1.396158                                        LR 0.000050    Time 0.064907    
2022-12-29 10:09:01,821 - Epoch: [102][  113/  113]    Overall Loss 1.398688    Objective Loss 1.398688    Top1 37.500000    Top5 95.833333    LR 0.000050    Time 0.064737    
2022-12-29 10:09:01,884 - --- validate (epoch=102)-----------
2022-12-29 10:09:01,885 - 200 samples (16 per mini-batch)
2022-12-29 10:09:02,426 - Epoch: [102][   10/   13]    Loss 1.421759    Top1 41.250000    Top5 96.250000    
2022-12-29 10:09:02,511 - Epoch: [102][   13/   13]    Loss 1.418711    Top1 41.500000    Top5 97.000000    
2022-12-29 10:09:02,561 - ==> Top1: 41.500    Top5: 97.000    Loss: 1.419

2022-12-29 10:09:02,562 - ==> Confusion:
[[16  2  1  7  5  0]
 [ 3 15  1  6  5  1]
 [ 0  5 13 12  5  0]
 [ 2  8  8 28  6  0]
 [ 7  6  4 11 10  0]
 [ 1  3  2  1  5  1]]

2022-12-29 10:09:02,564 - ==> Best [Top1: 48.000   Top5: 96.500   Sparsity:0.00   Params: 289216 on epoch: 100]
2022-12-29 10:09:02,565 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:02,588 - 

2022-12-29 10:09:02,589 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:03,417 - Epoch: [103][   10/  113]    Overall Loss 1.438119    Objective Loss 1.438119                                        LR 0.000050    Time 0.082664    
2022-12-29 10:09:04,053 - Epoch: [103][   20/  113]    Overall Loss 1.484645    Objective Loss 1.484645                                        LR 0.000050    Time 0.073100    
2022-12-29 10:09:04,691 - Epoch: [103][   30/  113]    Overall Loss 1.417788    Objective Loss 1.417788                                        LR 0.000050    Time 0.069992    
2022-12-29 10:09:05,331 - Epoch: [103][   40/  113]    Overall Loss 1.399432    Objective Loss 1.399432                                        LR 0.000050    Time 0.068487    
2022-12-29 10:09:05,967 - Epoch: [103][   50/  113]    Overall Loss 1.394868    Objective Loss 1.394868                                        LR 0.000050    Time 0.067487    
2022-12-29 10:09:06,601 - Epoch: [103][   60/  113]    Overall Loss 1.387216    Objective Loss 1.387216                                        LR 0.000050    Time 0.066799    
2022-12-29 10:09:07,240 - Epoch: [103][   70/  113]    Overall Loss 1.377217    Objective Loss 1.377217                                        LR 0.000050    Time 0.066391    
2022-12-29 10:09:07,869 - Epoch: [103][   80/  113]    Overall Loss 1.377964    Objective Loss 1.377964                                        LR 0.000050    Time 0.065937    
2022-12-29 10:09:08,497 - Epoch: [103][   90/  113]    Overall Loss 1.387530    Objective Loss 1.387530                                        LR 0.000050    Time 0.065586    
2022-12-29 10:09:09,132 - Epoch: [103][  100/  113]    Overall Loss 1.389212    Objective Loss 1.389212                                        LR 0.000050    Time 0.065371    
2022-12-29 10:09:09,759 - Epoch: [103][  110/  113]    Overall Loss 1.390383    Objective Loss 1.390383                                        LR 0.000050    Time 0.065127    
2022-12-29 10:09:09,929 - Epoch: [103][  113/  113]    Overall Loss 1.390359    Objective Loss 1.390359    Top1 37.500000    Top5 95.833333    LR 0.000050    Time 0.064901    
2022-12-29 10:09:09,975 - --- validate (epoch=103)-----------
2022-12-29 10:09:09,975 - 200 samples (16 per mini-batch)
2022-12-29 10:09:10,523 - Epoch: [103][   10/   13]    Loss 1.387625    Top1 46.875000    Top5 95.625000    
2022-12-29 10:09:10,610 - Epoch: [103][   13/   13]    Loss 1.360364    Top1 50.000000    Top5 95.500000    
2022-12-29 10:09:10,675 - ==> Top1: 50.000    Top5: 95.500    Loss: 1.360

2022-12-29 10:09:10,675 - ==> Confusion:
[[15  1  0 12  5  1]
 [ 3 22  0 11  3  1]
 [ 1  9 10  9  2  0]
 [ 2  3  0 42  6  0]
 [ 4  5  2  7 10  1]
 [ 0  5  1  5  1  1]]

2022-12-29 10:09:10,678 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:10,678 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:10,695 - 

2022-12-29 10:09:10,696 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:11,524 - Epoch: [104][   10/  113]    Overall Loss 1.296060    Objective Loss 1.296060                                        LR 0.000050    Time 0.082782    
2022-12-29 10:09:12,158 - Epoch: [104][   20/  113]    Overall Loss 1.374259    Objective Loss 1.374259                                        LR 0.000050    Time 0.073060    
2022-12-29 10:09:12,785 - Epoch: [104][   30/  113]    Overall Loss 1.395137    Objective Loss 1.395137                                        LR 0.000050    Time 0.069591    
2022-12-29 10:09:13,418 - Epoch: [104][   40/  113]    Overall Loss 1.356916    Objective Loss 1.356916                                        LR 0.000050    Time 0.067985    
2022-12-29 10:09:14,045 - Epoch: [104][   50/  113]    Overall Loss 1.344417    Objective Loss 1.344417                                        LR 0.000050    Time 0.066938    
2022-12-29 10:09:14,677 - Epoch: [104][   60/  113]    Overall Loss 1.364257    Objective Loss 1.364257                                        LR 0.000050    Time 0.066306    
2022-12-29 10:09:15,312 - Epoch: [104][   70/  113]    Overall Loss 1.365621    Objective Loss 1.365621                                        LR 0.000050    Time 0.065896    
2022-12-29 10:09:15,940 - Epoch: [104][   80/  113]    Overall Loss 1.359643    Objective Loss 1.359643                                        LR 0.000050    Time 0.065498    
2022-12-29 10:09:16,569 - Epoch: [104][   90/  113]    Overall Loss 1.368184    Objective Loss 1.368184                                        LR 0.000050    Time 0.065211    
2022-12-29 10:09:17,203 - Epoch: [104][  100/  113]    Overall Loss 1.371483    Objective Loss 1.371483                                        LR 0.000050    Time 0.065027    
2022-12-29 10:09:17,830 - Epoch: [104][  110/  113]    Overall Loss 1.371129    Objective Loss 1.371129                                        LR 0.000050    Time 0.064805    
2022-12-29 10:09:18,010 - Epoch: [104][  113/  113]    Overall Loss 1.372751    Objective Loss 1.372751    Top1 58.333333    Top5 100.000000    LR 0.000050    Time 0.064671    
2022-12-29 10:09:18,060 - --- validate (epoch=104)-----------
2022-12-29 10:09:18,061 - 200 samples (16 per mini-batch)
2022-12-29 10:09:18,601 - Epoch: [104][   10/   13]    Loss 1.375278    Top1 44.375000    Top5 97.500000    
2022-12-29 10:09:18,685 - Epoch: [104][   13/   13]    Loss 1.340067    Top1 46.500000    Top5 97.500000    
2022-12-29 10:09:18,741 - ==> Top1: 46.500    Top5: 97.500    Loss: 1.340

2022-12-29 10:09:18,742 - ==> Confusion:
[[23  8  1  2  2  0]
 [ 1 32  2  1  5  0]
 [ 1 15  7  3  7  0]
 [ 4  9  5 12  5  0]
 [ 3 16  3  5 19  0]
 [ 1  5  0  1  2  0]]

2022-12-29 10:09:18,744 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:18,745 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:18,765 - 

2022-12-29 10:09:18,766 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:19,591 - Epoch: [105][   10/  113]    Overall Loss 1.381422    Objective Loss 1.381422                                        LR 0.000050    Time 0.082421    
2022-12-29 10:09:20,228 - Epoch: [105][   20/  113]    Overall Loss 1.413198    Objective Loss 1.413198                                        LR 0.000050    Time 0.072993    
2022-12-29 10:09:20,864 - Epoch: [105][   30/  113]    Overall Loss 1.402871    Objective Loss 1.402871                                        LR 0.000050    Time 0.069871    
2022-12-29 10:09:21,506 - Epoch: [105][   40/  113]    Overall Loss 1.370710    Objective Loss 1.370710                                        LR 0.000050    Time 0.068422    
2022-12-29 10:09:22,139 - Epoch: [105][   50/  113]    Overall Loss 1.363528    Objective Loss 1.363528                                        LR 0.000050    Time 0.067395    
2022-12-29 10:09:22,772 - Epoch: [105][   60/  113]    Overall Loss 1.377058    Objective Loss 1.377058                                        LR 0.000050    Time 0.066704    
2022-12-29 10:09:23,406 - Epoch: [105][   70/  113]    Overall Loss 1.371869    Objective Loss 1.371869                                        LR 0.000050    Time 0.066232    
2022-12-29 10:09:24,040 - Epoch: [105][   80/  113]    Overall Loss 1.363026    Objective Loss 1.363026                                        LR 0.000050    Time 0.065869    
2022-12-29 10:09:24,677 - Epoch: [105][   90/  113]    Overall Loss 1.367256    Objective Loss 1.367256                                        LR 0.000050    Time 0.065620    
2022-12-29 10:09:25,312 - Epoch: [105][  100/  113]    Overall Loss 1.359157    Objective Loss 1.359157                                        LR 0.000050    Time 0.065402    
2022-12-29 10:09:25,938 - Epoch: [105][  110/  113]    Overall Loss 1.370295    Objective Loss 1.370295                                        LR 0.000050    Time 0.065146    
2022-12-29 10:09:26,110 - Epoch: [105][  113/  113]    Overall Loss 1.371675    Objective Loss 1.371675    Top1 45.833333    Top5 100.000000    LR 0.000050    Time 0.064936    
2022-12-29 10:09:26,173 - --- validate (epoch=105)-----------
2022-12-29 10:09:26,174 - 200 samples (16 per mini-batch)
2022-12-29 10:09:26,730 - Epoch: [105][   10/   13]    Loss 1.471120    Top1 39.375000    Top5 95.625000    
2022-12-29 10:09:26,814 - Epoch: [105][   13/   13]    Loss 1.456009    Top1 39.000000    Top5 96.500000    
2022-12-29 10:09:26,861 - ==> Top1: 39.000    Top5: 96.500    Loss: 1.456

2022-12-29 10:09:26,862 - ==> Confusion:
[[18  0  0  7  2  0]
 [ 7  7  8 12 16  0]
 [ 0  3  5  8  6  0]
 [ 5  1  2 36  9  0]
 [ 5  1  6 15 11  0]
 [ 1  0  0  2  6  1]]

2022-12-29 10:09:26,864 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:26,865 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:26,886 - 

2022-12-29 10:09:26,886 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:27,723 - Epoch: [106][   10/  113]    Overall Loss 1.435732    Objective Loss 1.435732                                        LR 0.000050    Time 0.083466    
2022-12-29 10:09:28,358 - Epoch: [106][   20/  113]    Overall Loss 1.433388    Objective Loss 1.433388                                        LR 0.000050    Time 0.073455    
2022-12-29 10:09:28,994 - Epoch: [106][   30/  113]    Overall Loss 1.404473    Objective Loss 1.404473                                        LR 0.000050    Time 0.070136    
2022-12-29 10:09:29,630 - Epoch: [106][   40/  113]    Overall Loss 1.398561    Objective Loss 1.398561                                        LR 0.000050    Time 0.068499    
2022-12-29 10:09:30,261 - Epoch: [106][   50/  113]    Overall Loss 1.390362    Objective Loss 1.390362                                        LR 0.000050    Time 0.067409    
2022-12-29 10:09:30,892 - Epoch: [106][   60/  113]    Overall Loss 1.405684    Objective Loss 1.405684                                        LR 0.000050    Time 0.066694    
2022-12-29 10:09:31,533 - Epoch: [106][   70/  113]    Overall Loss 1.414226    Objective Loss 1.414226                                        LR 0.000050    Time 0.066305    
2022-12-29 10:09:32,165 - Epoch: [106][   80/  113]    Overall Loss 1.409070    Objective Loss 1.409070                                        LR 0.000050    Time 0.065915    
2022-12-29 10:09:32,799 - Epoch: [106][   90/  113]    Overall Loss 1.402090    Objective Loss 1.402090                                        LR 0.000050    Time 0.065630    
2022-12-29 10:09:33,435 - Epoch: [106][  100/  113]    Overall Loss 1.411758    Objective Loss 1.411758                                        LR 0.000050    Time 0.065425    
2022-12-29 10:09:34,066 - Epoch: [106][  110/  113]    Overall Loss 1.406637    Objective Loss 1.406637                                        LR 0.000050    Time 0.065211    
2022-12-29 10:09:34,238 - Epoch: [106][  113/  113]    Overall Loss 1.404541    Objective Loss 1.404541    Top1 45.833333    Top5 95.833333    LR 0.000050    Time 0.064999    
2022-12-29 10:09:34,297 - --- validate (epoch=106)-----------
2022-12-29 10:09:34,297 - 200 samples (16 per mini-batch)
2022-12-29 10:09:34,836 - Epoch: [106][   10/   13]    Loss 1.400564    Top1 40.625000    Top5 98.125000    
2022-12-29 10:09:34,921 - Epoch: [106][   13/   13]    Loss 1.363206    Top1 42.000000    Top5 98.500000    
2022-12-29 10:09:34,975 - ==> Top1: 42.000    Top5: 98.500    Loss: 1.363

2022-12-29 10:09:34,975 - ==> Confusion:
[[16  1  0  6 11  0]
 [ 5  8  1  6 19  0]
 [ 1  7  7  6 13  0]
 [ 5  2  4 31  6  0]
 [ 3  2  3  7 21  0]
 [ 1  0  1  4  2  1]]

2022-12-29 10:09:34,977 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:34,977 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:34,999 - 

2022-12-29 10:09:35,000 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:35,829 - Epoch: [107][   10/  113]    Overall Loss 1.356512    Objective Loss 1.356512                                        LR 0.000050    Time 0.082828    
2022-12-29 10:09:36,459 - Epoch: [107][   20/  113]    Overall Loss 1.353961    Objective Loss 1.353961                                        LR 0.000050    Time 0.072878    
2022-12-29 10:09:37,091 - Epoch: [107][   30/  113]    Overall Loss 1.369514    Objective Loss 1.369514                                        LR 0.000050    Time 0.069652    
2022-12-29 10:09:37,720 - Epoch: [107][   40/  113]    Overall Loss 1.352619    Objective Loss 1.352619                                        LR 0.000050    Time 0.067962    
2022-12-29 10:09:38,356 - Epoch: [107][   50/  113]    Overall Loss 1.349589    Objective Loss 1.349589                                        LR 0.000050    Time 0.067068    
2022-12-29 10:09:38,989 - Epoch: [107][   60/  113]    Overall Loss 1.369170    Objective Loss 1.369170                                        LR 0.000050    Time 0.066435    
2022-12-29 10:09:39,621 - Epoch: [107][   70/  113]    Overall Loss 1.364764    Objective Loss 1.364764                                        LR 0.000050    Time 0.065972    
2022-12-29 10:09:40,252 - Epoch: [107][   80/  113]    Overall Loss 1.362852    Objective Loss 1.362852                                        LR 0.000050    Time 0.065605    
2022-12-29 10:09:40,886 - Epoch: [107][   90/  113]    Overall Loss 1.364585    Objective Loss 1.364585                                        LR 0.000050    Time 0.065356    
2022-12-29 10:09:41,529 - Epoch: [107][  100/  113]    Overall Loss 1.369115    Objective Loss 1.369115                                        LR 0.000050    Time 0.065246    
2022-12-29 10:09:42,155 - Epoch: [107][  110/  113]    Overall Loss 1.372477    Objective Loss 1.372477                                        LR 0.000050    Time 0.065000    
2022-12-29 10:09:42,335 - Epoch: [107][  113/  113]    Overall Loss 1.377801    Objective Loss 1.377801    Top1 41.666667    Top5 95.833333    LR 0.000050    Time 0.064861    
2022-12-29 10:09:42,398 - --- validate (epoch=107)-----------
2022-12-29 10:09:42,399 - 200 samples (16 per mini-batch)
2022-12-29 10:09:42,951 - Epoch: [107][   10/   13]    Loss 1.372419    Top1 46.875000    Top5 96.250000    
2022-12-29 10:09:43,038 - Epoch: [107][   13/   13]    Loss 1.357738    Top1 48.000000    Top5 96.500000    
2022-12-29 10:09:43,102 - ==> Top1: 48.000    Top5: 96.500    Loss: 1.358

2022-12-29 10:09:43,103 - ==> Confusion:
[[19  5  2  5  3  0]
 [ 1 16  2  8 10  0]
 [ 0  5 11  4  8  0]
 [ 3  5  2 31  9  0]
 [ 0  8  4  7 19  1]
 [ 2  0  1  2  7  0]]

2022-12-29 10:09:43,105 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:43,105 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:43,126 - 

2022-12-29 10:09:43,126 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:43,977 - Epoch: [108][   10/  113]    Overall Loss 1.309673    Objective Loss 1.309673                                        LR 0.000050    Time 0.084845    
2022-12-29 10:09:44,616 - Epoch: [108][   20/  113]    Overall Loss 1.349129    Objective Loss 1.349129                                        LR 0.000050    Time 0.074350    
2022-12-29 10:09:45,257 - Epoch: [108][   30/  113]    Overall Loss 1.347559    Objective Loss 1.347559                                        LR 0.000050    Time 0.070943    
2022-12-29 10:09:45,895 - Epoch: [108][   40/  113]    Overall Loss 1.349615    Objective Loss 1.349615                                        LR 0.000050    Time 0.069135    
2022-12-29 10:09:46,532 - Epoch: [108][   50/  113]    Overall Loss 1.316989    Objective Loss 1.316989                                        LR 0.000050    Time 0.068036    
2022-12-29 10:09:47,168 - Epoch: [108][   60/  113]    Overall Loss 1.334151    Objective Loss 1.334151                                        LR 0.000050    Time 0.067289    
2022-12-29 10:09:47,802 - Epoch: [108][   70/  113]    Overall Loss 1.341501    Objective Loss 1.341501                                        LR 0.000050    Time 0.066733    
2022-12-29 10:09:48,436 - Epoch: [108][   80/  113]    Overall Loss 1.339315    Objective Loss 1.339315                                        LR 0.000050    Time 0.066300    
2022-12-29 10:09:49,074 - Epoch: [108][   90/  113]    Overall Loss 1.340880    Objective Loss 1.340880                                        LR 0.000050    Time 0.066021    
2022-12-29 10:09:49,706 - Epoch: [108][  100/  113]    Overall Loss 1.332515    Objective Loss 1.332515                                        LR 0.000050    Time 0.065739    
2022-12-29 10:09:50,338 - Epoch: [108][  110/  113]    Overall Loss 1.338199    Objective Loss 1.338199                                        LR 0.000050    Time 0.065497    
2022-12-29 10:09:50,513 - Epoch: [108][  113/  113]    Overall Loss 1.344749    Objective Loss 1.344749    Top1 20.833333    Top5 100.000000    LR 0.000050    Time 0.065304    
2022-12-29 10:09:50,577 - --- validate (epoch=108)-----------
2022-12-29 10:09:50,578 - 200 samples (16 per mini-batch)
2022-12-29 10:09:51,120 - Epoch: [108][   10/   13]    Loss 1.319625    Top1 43.750000    Top5 98.750000    
2022-12-29 10:09:51,204 - Epoch: [108][   13/   13]    Loss 1.307748    Top1 47.500000    Top5 99.000000    
2022-12-29 10:09:51,260 - ==> Top1: 47.500    Top5: 99.000    Loss: 1.308

2022-12-29 10:09:51,260 - ==> Confusion:
[[20  3  1  8  6  0]
 [ 1 11  6 10  9  0]
 [ 2  4 10 10  7  0]
 [ 4  1  0 29  6  0]
 [ 5  5  4  7 25  0]
 [ 0  1  1  1  3  0]]

2022-12-29 10:09:51,263 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:51,263 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:51,289 - 

2022-12-29 10:09:51,290 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:09:52,130 - Epoch: [109][   10/  113]    Overall Loss 1.405997    Objective Loss 1.405997                                        LR 0.000050    Time 0.083833    
2022-12-29 10:09:52,766 - Epoch: [109][   20/  113]    Overall Loss 1.405406    Objective Loss 1.405406                                        LR 0.000050    Time 0.073685    
2022-12-29 10:09:53,401 - Epoch: [109][   30/  113]    Overall Loss 1.381701    Objective Loss 1.381701                                        LR 0.000050    Time 0.070292    
2022-12-29 10:09:54,034 - Epoch: [109][   40/  113]    Overall Loss 1.341911    Objective Loss 1.341911                                        LR 0.000050    Time 0.068535    
2022-12-29 10:09:54,665 - Epoch: [109][   50/  113]    Overall Loss 1.358748    Objective Loss 1.358748                                        LR 0.000050    Time 0.067427    
2022-12-29 10:09:55,295 - Epoch: [109][   60/  113]    Overall Loss 1.378559    Objective Loss 1.378559                                        LR 0.000050    Time 0.066680    
2022-12-29 10:09:55,923 - Epoch: [109][   70/  113]    Overall Loss 1.385587    Objective Loss 1.385587                                        LR 0.000050    Time 0.066117    
2022-12-29 10:09:56,553 - Epoch: [109][   80/  113]    Overall Loss 1.371697    Objective Loss 1.371697                                        LR 0.000050    Time 0.065721    
2022-12-29 10:09:57,185 - Epoch: [109][   90/  113]    Overall Loss 1.370653    Objective Loss 1.370653                                        LR 0.000050    Time 0.065440    
2022-12-29 10:09:57,817 - Epoch: [109][  100/  113]    Overall Loss 1.374905    Objective Loss 1.374905                                        LR 0.000050    Time 0.065211    
2022-12-29 10:09:58,444 - Epoch: [109][  110/  113]    Overall Loss 1.369819    Objective Loss 1.369819                                        LR 0.000050    Time 0.064979    
2022-12-29 10:09:58,615 - Epoch: [109][  113/  113]    Overall Loss 1.369873    Objective Loss 1.369873    Top1 45.833333    Top5 95.833333    LR 0.000050    Time 0.064757    
2022-12-29 10:09:58,672 - --- validate (epoch=109)-----------
2022-12-29 10:09:58,673 - 200 samples (16 per mini-batch)
2022-12-29 10:09:59,220 - Epoch: [109][   10/   13]    Loss 1.453953    Top1 43.125000    Top5 95.625000    
2022-12-29 10:09:59,307 - Epoch: [109][   13/   13]    Loss 1.466245    Top1 43.500000    Top5 96.000000    
2022-12-29 10:09:59,359 - ==> Top1: 43.500    Top5: 96.000    Loss: 1.466

2022-12-29 10:09:59,359 - ==> Confusion:
[[13  1  0  9  0  0]
 [ 3 21  0 11  9  0]
 [ 6  6  5 13 13  0]
 [ 1  4  0 37  2  0]
 [ 4  9  0  9 11  0]
 [ 5  4  0  1  3  0]]

2022-12-29 10:09:59,362 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:09:59,362 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:09:59,383 - 

2022-12-29 10:09:59,384 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:00,209 - Epoch: [110][   10/  113]    Overall Loss 1.372666    Objective Loss 1.372666                                        LR 0.000050    Time 0.082300    
2022-12-29 10:10:00,841 - Epoch: [110][   20/  113]    Overall Loss 1.303613    Objective Loss 1.303613                                        LR 0.000050    Time 0.072758    
2022-12-29 10:10:01,480 - Epoch: [110][   30/  113]    Overall Loss 1.312843    Objective Loss 1.312843                                        LR 0.000050    Time 0.069793    
2022-12-29 10:10:02,114 - Epoch: [110][   40/  113]    Overall Loss 1.331290    Objective Loss 1.331290                                        LR 0.000050    Time 0.068179    
2022-12-29 10:10:02,745 - Epoch: [110][   50/  113]    Overall Loss 1.335628    Objective Loss 1.335628                                        LR 0.000050    Time 0.067150    
2022-12-29 10:10:03,377 - Epoch: [110][   60/  113]    Overall Loss 1.356316    Objective Loss 1.356316                                        LR 0.000050    Time 0.066478    
2022-12-29 10:10:04,009 - Epoch: [110][   70/  113]    Overall Loss 1.346612    Objective Loss 1.346612                                        LR 0.000050    Time 0.066008    
2022-12-29 10:10:04,637 - Epoch: [110][   80/  113]    Overall Loss 1.364599    Objective Loss 1.364599                                        LR 0.000050    Time 0.065602    
2022-12-29 10:10:05,271 - Epoch: [110][   90/  113]    Overall Loss 1.361015    Objective Loss 1.361015                                        LR 0.000050    Time 0.065349    
2022-12-29 10:10:05,901 - Epoch: [110][  100/  113]    Overall Loss 1.364775    Objective Loss 1.364775                                        LR 0.000050    Time 0.065111    
2022-12-29 10:10:06,527 - Epoch: [110][  110/  113]    Overall Loss 1.371475    Objective Loss 1.371475                                        LR 0.000050    Time 0.064878    
2022-12-29 10:10:06,702 - Epoch: [110][  113/  113]    Overall Loss 1.380257    Objective Loss 1.380257    Top1 37.500000    Top5 91.666667    LR 0.000050    Time 0.064697    
2022-12-29 10:10:06,751 - --- validate (epoch=110)-----------
2022-12-29 10:10:06,751 - 200 samples (16 per mini-batch)
2022-12-29 10:10:07,297 - Epoch: [110][   10/   13]    Loss 1.403678    Top1 44.375000    Top5 96.250000    
2022-12-29 10:10:07,380 - Epoch: [110][   13/   13]    Loss 1.384193    Top1 44.500000    Top5 96.500000    
2022-12-29 10:10:07,435 - ==> Top1: 44.500    Top5: 96.500    Loss: 1.384

2022-12-29 10:10:07,435 - ==> Confusion:
[[17  8  0 11  7  0]
 [ 1 21  2  3 13  1]
 [ 3  6  5 13  8  0]
 [ 2  2  0 30  8  0]
 [ 2  4  0  7 16  0]
 [ 1  2  0  1  6  0]]

2022-12-29 10:10:07,438 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:10:07,438 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:07,461 - 

2022-12-29 10:10:07,462 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:08,282 - Epoch: [111][   10/  113]    Overall Loss 1.460208    Objective Loss 1.460208                                        LR 0.000050    Time 0.081809    
2022-12-29 10:10:08,914 - Epoch: [111][   20/  113]    Overall Loss 1.415499    Objective Loss 1.415499                                        LR 0.000050    Time 0.072480    
2022-12-29 10:10:09,547 - Epoch: [111][   30/  113]    Overall Loss 1.377322    Objective Loss 1.377322                                        LR 0.000050    Time 0.069405    
2022-12-29 10:10:10,175 - Epoch: [111][   40/  113]    Overall Loss 1.372296    Objective Loss 1.372296                                        LR 0.000050    Time 0.067747    
2022-12-29 10:10:10,809 - Epoch: [111][   50/  113]    Overall Loss 1.368097    Objective Loss 1.368097                                        LR 0.000050    Time 0.066872    
2022-12-29 10:10:11,446 - Epoch: [111][   60/  113]    Overall Loss 1.365032    Objective Loss 1.365032                                        LR 0.000050    Time 0.066332    
2022-12-29 10:10:12,082 - Epoch: [111][   70/  113]    Overall Loss 1.361855    Objective Loss 1.361855                                        LR 0.000050    Time 0.065941    
2022-12-29 10:10:12,711 - Epoch: [111][   80/  113]    Overall Loss 1.354378    Objective Loss 1.354378                                        LR 0.000050    Time 0.065553    
2022-12-29 10:10:13,343 - Epoch: [111][   90/  113]    Overall Loss 1.353478    Objective Loss 1.353478                                        LR 0.000050    Time 0.065290    
2022-12-29 10:10:13,970 - Epoch: [111][  100/  113]    Overall Loss 1.370330    Objective Loss 1.370330                                        LR 0.000050    Time 0.065022    
2022-12-29 10:10:14,597 - Epoch: [111][  110/  113]    Overall Loss 1.375182    Objective Loss 1.375182                                        LR 0.000050    Time 0.064808    
2022-12-29 10:10:14,767 - Epoch: [111][  113/  113]    Overall Loss 1.370217    Objective Loss 1.370217    Top1 37.500000    Top5 100.000000    LR 0.000050    Time 0.064587    
2022-12-29 10:10:14,827 - --- validate (epoch=111)-----------
2022-12-29 10:10:14,827 - 200 samples (16 per mini-batch)
2022-12-29 10:10:15,369 - Epoch: [111][   10/   13]    Loss 1.427661    Top1 43.750000    Top5 95.625000    
2022-12-29 10:10:15,453 - Epoch: [111][   13/   13]    Loss 1.416069    Top1 43.000000    Top5 96.500000    
2022-12-29 10:10:15,500 - ==> Top1: 43.000    Top5: 96.500    Loss: 1.416

2022-12-29 10:10:15,500 - ==> Confusion:
[[21  1  2 16  2  0]
 [ 2 16  2 20  7  0]
 [ 1  7  5 13  6  0]
 [ 5  2  1 33  3  0]
 [ 1  2  1 12 11  0]
 [ 0  1  1  5  1  0]]

2022-12-29 10:10:15,503 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:10:15,503 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:15,520 - 

2022-12-29 10:10:15,520 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:16,344 - Epoch: [112][   10/  113]    Overall Loss 1.377260    Objective Loss 1.377260                                        LR 0.000050    Time 0.082320    
2022-12-29 10:10:16,977 - Epoch: [112][   20/  113]    Overall Loss 1.375988    Objective Loss 1.375988                                        LR 0.000050    Time 0.072802    
2022-12-29 10:10:17,614 - Epoch: [112][   30/  113]    Overall Loss 1.371793    Objective Loss 1.371793                                        LR 0.000050    Time 0.069751    
2022-12-29 10:10:18,247 - Epoch: [112][   40/  113]    Overall Loss 1.370377    Objective Loss 1.370377                                        LR 0.000050    Time 0.068134    
2022-12-29 10:10:18,873 - Epoch: [112][   50/  113]    Overall Loss 1.369088    Objective Loss 1.369088                                        LR 0.000050    Time 0.067003    
2022-12-29 10:10:19,505 - Epoch: [112][   60/  113]    Overall Loss 1.377830    Objective Loss 1.377830                                        LR 0.000050    Time 0.066358    
2022-12-29 10:10:20,134 - Epoch: [112][   70/  113]    Overall Loss 1.366476    Objective Loss 1.366476                                        LR 0.000050    Time 0.065859    
2022-12-29 10:10:20,764 - Epoch: [112][   80/  113]    Overall Loss 1.363982    Objective Loss 1.363982                                        LR 0.000050    Time 0.065499    
2022-12-29 10:10:21,402 - Epoch: [112][   90/  113]    Overall Loss 1.371001    Objective Loss 1.371001                                        LR 0.000050    Time 0.065301    
2022-12-29 10:10:22,037 - Epoch: [112][  100/  113]    Overall Loss 1.369167    Objective Loss 1.369167                                        LR 0.000050    Time 0.065117    
2022-12-29 10:10:22,660 - Epoch: [112][  110/  113]    Overall Loss 1.364025    Objective Loss 1.364025                                        LR 0.000050    Time 0.064861    
2022-12-29 10:10:22,833 - Epoch: [112][  113/  113]    Overall Loss 1.370708    Objective Loss 1.370708    Top1 25.000000    Top5 95.833333    LR 0.000050    Time 0.064661    
2022-12-29 10:10:22,886 - --- validate (epoch=112)-----------
2022-12-29 10:10:22,887 - 200 samples (16 per mini-batch)
2022-12-29 10:10:23,436 - Epoch: [112][   10/   13]    Loss 1.368009    Top1 45.000000    Top5 97.500000    
2022-12-29 10:10:23,520 - Epoch: [112][   13/   13]    Loss 1.395982    Top1 46.000000    Top5 97.000000    
2022-12-29 10:10:23,570 - ==> Top1: 46.000    Top5: 97.000    Loss: 1.396

2022-12-29 10:10:23,571 - ==> Confusion:
[[27  2  1  2  2  0]
 [10 21  4  7  6  0]
 [ 2  8  6  4 12  0]
 [ 0  4  0 22 11  0]
 [ 5  8  3  7 16  0]
 [ 1  2  2  1  4  0]]

2022-12-29 10:10:23,573 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:10:23,573 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:23,583 - 

2022-12-29 10:10:23,583 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:24,421 - Epoch: [113][   10/  113]    Overall Loss 1.403140    Objective Loss 1.403140                                        LR 0.000050    Time 0.083692    
2022-12-29 10:10:25,063 - Epoch: [113][   20/  113]    Overall Loss 1.363952    Objective Loss 1.363952                                        LR 0.000050    Time 0.073939    
2022-12-29 10:10:25,702 - Epoch: [113][   30/  113]    Overall Loss 1.367138    Objective Loss 1.367138                                        LR 0.000050    Time 0.070574    
2022-12-29 10:10:26,330 - Epoch: [113][   40/  113]    Overall Loss 1.351761    Objective Loss 1.351761                                        LR 0.000050    Time 0.068613    
2022-12-29 10:10:26,959 - Epoch: [113][   50/  113]    Overall Loss 1.350848    Objective Loss 1.350848                                        LR 0.000050    Time 0.067466    
2022-12-29 10:10:27,593 - Epoch: [113][   60/  113]    Overall Loss 1.336777    Objective Loss 1.336777                                        LR 0.000050    Time 0.066782    
2022-12-29 10:10:28,223 - Epoch: [113][   70/  113]    Overall Loss 1.340122    Objective Loss 1.340122                                        LR 0.000050    Time 0.066227    
2022-12-29 10:10:28,850 - Epoch: [113][   80/  113]    Overall Loss 1.339052    Objective Loss 1.339052                                        LR 0.000050    Time 0.065789    
2022-12-29 10:10:29,481 - Epoch: [113][   90/  113]    Overall Loss 1.336927    Objective Loss 1.336927                                        LR 0.000050    Time 0.065482    
2022-12-29 10:10:30,112 - Epoch: [113][  100/  113]    Overall Loss 1.334605    Objective Loss 1.334605                                        LR 0.000050    Time 0.065243    
2022-12-29 10:10:30,736 - Epoch: [113][  110/  113]    Overall Loss 1.333306    Objective Loss 1.333306                                        LR 0.000050    Time 0.064976    
2022-12-29 10:10:30,908 - Epoch: [113][  113/  113]    Overall Loss 1.333521    Objective Loss 1.333521    Top1 62.500000    Top5 95.833333    LR 0.000050    Time 0.064769    
2022-12-29 10:10:30,970 - --- validate (epoch=113)-----------
2022-12-29 10:10:30,971 - 200 samples (16 per mini-batch)
2022-12-29 10:10:31,523 - Epoch: [113][   10/   13]    Loss 1.288632    Top1 48.125000    Top5 98.750000    
2022-12-29 10:10:31,608 - Epoch: [113][   13/   13]    Loss 1.327315    Top1 46.500000    Top5 98.000000    
2022-12-29 10:10:31,656 - ==> Top1: 46.500    Top5: 98.000    Loss: 1.327

2022-12-29 10:10:31,657 - ==> Confusion:
[[27  4  0  9  5  0]
 [ 2 18  7  3 15  0]
 [ 3  4  8  5  5  0]
 [ 6  1  2 30  6  0]
 [ 6  4  2 12 10  0]
 [ 2  3  0  0  1  0]]

2022-12-29 10:10:31,659 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:10:31,659 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:31,684 - 

2022-12-29 10:10:31,685 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:32,516 - Epoch: [114][   10/  113]    Overall Loss 1.387343    Objective Loss 1.387343                                        LR 0.000050    Time 0.083050    
2022-12-29 10:10:33,154 - Epoch: [114][   20/  113]    Overall Loss 1.332664    Objective Loss 1.332664                                        LR 0.000050    Time 0.073383    
2022-12-29 10:10:33,796 - Epoch: [114][   30/  113]    Overall Loss 1.366288    Objective Loss 1.366288                                        LR 0.000050    Time 0.070299    
2022-12-29 10:10:34,431 - Epoch: [114][   40/  113]    Overall Loss 1.352279    Objective Loss 1.352279                                        LR 0.000050    Time 0.068584    
2022-12-29 10:10:35,069 - Epoch: [114][   50/  113]    Overall Loss 1.339688    Objective Loss 1.339688                                        LR 0.000050    Time 0.067624    
2022-12-29 10:10:35,706 - Epoch: [114][   60/  113]    Overall Loss 1.347258    Objective Loss 1.347258                                        LR 0.000050    Time 0.066949    
2022-12-29 10:10:36,341 - Epoch: [114][   70/  113]    Overall Loss 1.339134    Objective Loss 1.339134                                        LR 0.000050    Time 0.066460    
2022-12-29 10:10:36,974 - Epoch: [114][   80/  113]    Overall Loss 1.333625    Objective Loss 1.333625                                        LR 0.000050    Time 0.066049    
2022-12-29 10:10:37,611 - Epoch: [114][   90/  113]    Overall Loss 1.317438    Objective Loss 1.317438                                        LR 0.000050    Time 0.065784    
2022-12-29 10:10:38,245 - Epoch: [114][  100/  113]    Overall Loss 1.315280    Objective Loss 1.315280                                        LR 0.000050    Time 0.065548    
2022-12-29 10:10:38,871 - Epoch: [114][  110/  113]    Overall Loss 1.320839    Objective Loss 1.320839                                        LR 0.000050    Time 0.065269    
2022-12-29 10:10:39,049 - Epoch: [114][  113/  113]    Overall Loss 1.325617    Objective Loss 1.325617    Top1 41.666667    Top5 95.833333    LR 0.000050    Time 0.065112    
2022-12-29 10:10:39,114 - --- validate (epoch=114)-----------
2022-12-29 10:10:39,114 - 200 samples (16 per mini-batch)
2022-12-29 10:10:39,664 - Epoch: [114][   10/   13]    Loss 1.392325    Top1 43.125000    Top5 96.875000    
2022-12-29 10:10:39,749 - Epoch: [114][   13/   13]    Loss 1.432273    Top1 42.000000    Top5 94.500000    
2022-12-29 10:10:39,795 - ==> Top1: 42.000    Top5: 94.500    Loss: 1.432

2022-12-29 10:10:39,796 - ==> Confusion:
[[14  3  0  7 10  0]
 [ 4 11  3  5 16  0]
 [ 2  3  7  8  7  1]
 [ 2  2  3 31 14  0]
 [ 3  2  2  6 21  0]
 [ 3  0  1  1  8  0]]

2022-12-29 10:10:39,798 - ==> Best [Top1: 50.000   Top5: 95.500   Sparsity:0.00   Params: 289216 on epoch: 103]
2022-12-29 10:10:39,798 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:39,817 - 

2022-12-29 10:10:39,818 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:40,665 - Epoch: [115][   10/  113]    Overall Loss 1.326615    Objective Loss 1.326615                                        LR 0.000050    Time 0.084592    
2022-12-29 10:10:41,301 - Epoch: [115][   20/  113]    Overall Loss 1.328325    Objective Loss 1.328325                                        LR 0.000050    Time 0.074069    
2022-12-29 10:10:41,956 - Epoch: [115][   30/  113]    Overall Loss 1.350388    Objective Loss 1.350388                                        LR 0.000050    Time 0.071169    
2022-12-29 10:10:42,591 - Epoch: [115][   40/  113]    Overall Loss 1.375884    Objective Loss 1.375884                                        LR 0.000050    Time 0.069256    
2022-12-29 10:10:43,227 - Epoch: [115][   50/  113]    Overall Loss 1.365596    Objective Loss 1.365596                                        LR 0.000050    Time 0.068110    
2022-12-29 10:10:43,864 - Epoch: [115][   60/  113]    Overall Loss 1.360486    Objective Loss 1.360486                                        LR 0.000050    Time 0.067361    
2022-12-29 10:10:44,505 - Epoch: [115][   70/  113]    Overall Loss 1.350298    Objective Loss 1.350298                                        LR 0.000050    Time 0.066891    
2022-12-29 10:10:45,138 - Epoch: [115][   80/  113]    Overall Loss 1.348562    Objective Loss 1.348562                                        LR 0.000050    Time 0.066442    
2022-12-29 10:10:45,780 - Epoch: [115][   90/  113]    Overall Loss 1.354539    Objective Loss 1.354539                                        LR 0.000050    Time 0.066187    
2022-12-29 10:10:46,414 - Epoch: [115][  100/  113]    Overall Loss 1.353156    Objective Loss 1.353156                                        LR 0.000050    Time 0.065904    
2022-12-29 10:10:47,039 - Epoch: [115][  110/  113]    Overall Loss 1.356062    Objective Loss 1.356062                                        LR 0.000050    Time 0.065587    
2022-12-29 10:10:47,211 - Epoch: [115][  113/  113]    Overall Loss 1.353705    Objective Loss 1.353705    Top1 41.666667    Top5 100.000000    LR 0.000050    Time 0.065364    
2022-12-29 10:10:47,272 - --- validate (epoch=115)-----------
2022-12-29 10:10:47,272 - 200 samples (16 per mini-batch)
2022-12-29 10:10:47,823 - Epoch: [115][   10/   13]    Loss 1.289277    Top1 53.125000    Top5 97.500000    
2022-12-29 10:10:47,908 - Epoch: [115][   13/   13]    Loss 1.329381    Top1 51.000000    Top5 98.000000    
2022-12-29 10:10:47,963 - ==> Top1: 51.000    Top5: 98.000    Loss: 1.329

2022-12-29 10:10:47,963 - ==> Confusion:
[[17  1  3  3  2  0]
 [ 3 16  7  9  4  0]
 [ 0  3 14  4 11  0]
 [ 3  1  4 34 10  0]
 [ 4  5  7  4 21  0]
 [ 1  4  1  2  2  0]]

2022-12-29 10:10:47,969 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:10:47,969 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:47,997 - 

2022-12-29 10:10:47,998 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:48,839 - Epoch: [116][   10/  113]    Overall Loss 1.377852    Objective Loss 1.377852                                        LR 0.000050    Time 0.083956    
2022-12-29 10:10:49,470 - Epoch: [116][   20/  113]    Overall Loss 1.314894    Objective Loss 1.314894                                        LR 0.000050    Time 0.073484    
2022-12-29 10:10:50,104 - Epoch: [116][   30/  113]    Overall Loss 1.383013    Objective Loss 1.383013                                        LR 0.000050    Time 0.070117    
2022-12-29 10:10:50,734 - Epoch: [116][   40/  113]    Overall Loss 1.391045    Objective Loss 1.391045                                        LR 0.000050    Time 0.068316    
2022-12-29 10:10:51,364 - Epoch: [116][   50/  113]    Overall Loss 1.371693    Objective Loss 1.371693                                        LR 0.000050    Time 0.067240    
2022-12-29 10:10:51,996 - Epoch: [116][   60/  113]    Overall Loss 1.364908    Objective Loss 1.364908                                        LR 0.000050    Time 0.066569    
2022-12-29 10:10:52,629 - Epoch: [116][   70/  113]    Overall Loss 1.369347    Objective Loss 1.369347                                        LR 0.000050    Time 0.066095    
2022-12-29 10:10:53,258 - Epoch: [116][   80/  113]    Overall Loss 1.361351    Objective Loss 1.361351                                        LR 0.000050    Time 0.065685    
2022-12-29 10:10:53,891 - Epoch: [116][   90/  113]    Overall Loss 1.358243    Objective Loss 1.358243                                        LR 0.000050    Time 0.065414    
2022-12-29 10:10:54,523 - Epoch: [116][  100/  113]    Overall Loss 1.358828    Objective Loss 1.358828                                        LR 0.000050    Time 0.065190    
2022-12-29 10:10:55,150 - Epoch: [116][  110/  113]    Overall Loss 1.363639    Objective Loss 1.363639                                        LR 0.000050    Time 0.064957    
2022-12-29 10:10:55,325 - Epoch: [116][  113/  113]    Overall Loss 1.361572    Objective Loss 1.361572    Top1 54.166667    Top5 91.666667    LR 0.000050    Time 0.064781    
2022-12-29 10:10:55,383 - --- validate (epoch=116)-----------
2022-12-29 10:10:55,384 - 200 samples (16 per mini-batch)
2022-12-29 10:10:55,940 - Epoch: [116][   10/   13]    Loss 1.432657    Top1 41.875000    Top5 96.875000    
2022-12-29 10:10:56,024 - Epoch: [116][   13/   13]    Loss 1.434097    Top1 41.000000    Top5 97.000000    
2022-12-29 10:10:56,070 - ==> Top1: 41.000    Top5: 97.000    Loss: 1.434

2022-12-29 10:10:56,070 - ==> Confusion:
[[16  1  1  6  4  0]
 [ 3 13  3  2 15  0]
 [ 1 12  3  3 14  0]
 [ 3  7  1 26 14  0]
 [ 2  5  3  5 23  0]
 [ 2  4  0  2  5  1]]

2022-12-29 10:10:56,072 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:10:56,072 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:10:56,089 - 

2022-12-29 10:10:56,089 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:10:56,931 - Epoch: [117][   10/  113]    Overall Loss 1.325355    Objective Loss 1.325355                                        LR 0.000050    Time 0.084169    
2022-12-29 10:10:57,569 - Epoch: [117][   20/  113]    Overall Loss 1.297420    Objective Loss 1.297420                                        LR 0.000050    Time 0.073911    
2022-12-29 10:10:58,205 - Epoch: [117][   30/  113]    Overall Loss 1.322909    Objective Loss 1.322909                                        LR 0.000050    Time 0.070474    
2022-12-29 10:10:58,838 - Epoch: [117][   40/  113]    Overall Loss 1.303964    Objective Loss 1.303964                                        LR 0.000050    Time 0.068681    
2022-12-29 10:10:59,476 - Epoch: [117][   50/  113]    Overall Loss 1.325696    Objective Loss 1.325696                                        LR 0.000050    Time 0.067687    
2022-12-29 10:11:00,116 - Epoch: [117][   60/  113]    Overall Loss 1.334606    Objective Loss 1.334606                                        LR 0.000050    Time 0.067070    
2022-12-29 10:11:00,758 - Epoch: [117][   70/  113]    Overall Loss 1.338874    Objective Loss 1.338874                                        LR 0.000050    Time 0.066650    
2022-12-29 10:11:01,390 - Epoch: [117][   80/  113]    Overall Loss 1.336313    Objective Loss 1.336313                                        LR 0.000050    Time 0.066215    
2022-12-29 10:11:02,034 - Epoch: [117][   90/  113]    Overall Loss 1.336419    Objective Loss 1.336419                                        LR 0.000050    Time 0.066007    
2022-12-29 10:11:02,671 - Epoch: [117][  100/  113]    Overall Loss 1.342345    Objective Loss 1.342345                                        LR 0.000050    Time 0.065766    
2022-12-29 10:11:03,294 - Epoch: [117][  110/  113]    Overall Loss 1.348102    Objective Loss 1.348102                                        LR 0.000050    Time 0.065447    
2022-12-29 10:11:03,471 - Epoch: [117][  113/  113]    Overall Loss 1.351731    Objective Loss 1.351731    Top1 37.500000    Top5 95.833333    LR 0.000050    Time 0.065277    
2022-12-29 10:11:03,532 - --- validate (epoch=117)-----------
2022-12-29 10:11:03,532 - 200 samples (16 per mini-batch)
2022-12-29 10:11:04,073 - Epoch: [117][   10/   13]    Loss 1.406189    Top1 45.625000    Top5 97.500000    
2022-12-29 10:11:04,161 - Epoch: [117][   13/   13]    Loss 1.415324    Top1 44.500000    Top5 98.000000    
2022-12-29 10:11:04,216 - ==> Top1: 44.500    Top5: 98.000    Loss: 1.415

2022-12-29 10:11:04,216 - ==> Confusion:
[[16  1  1  9  4  0]
 [ 3 14  2 15 16  0]
 [ 2  5 10 12  7  0]
 [ 1  3  1 32  2  0]
 [ 0  6  1 13 17  0]
 [ 0  0  0  3  4  0]]

2022-12-29 10:11:04,218 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:04,219 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:04,240 - 

2022-12-29 10:11:04,241 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:05,077 - Epoch: [118][   10/  113]    Overall Loss 1.373920    Objective Loss 1.373920                                        LR 0.000050    Time 0.083596    
2022-12-29 10:11:05,713 - Epoch: [118][   20/  113]    Overall Loss 1.446462    Objective Loss 1.446462                                        LR 0.000050    Time 0.073546    
2022-12-29 10:11:06,347 - Epoch: [118][   30/  113]    Overall Loss 1.439701    Objective Loss 1.439701                                        LR 0.000050    Time 0.070144    
2022-12-29 10:11:06,977 - Epoch: [118][   40/  113]    Overall Loss 1.403221    Objective Loss 1.403221                                        LR 0.000050    Time 0.068353    
2022-12-29 10:11:07,609 - Epoch: [118][   50/  113]    Overall Loss 1.402323    Objective Loss 1.402323                                        LR 0.000050    Time 0.067303    
2022-12-29 10:11:08,239 - Epoch: [118][   60/  113]    Overall Loss 1.384217    Objective Loss 1.384217                                        LR 0.000050    Time 0.066588    
2022-12-29 10:11:08,877 - Epoch: [118][   70/  113]    Overall Loss 1.379239    Objective Loss 1.379239                                        LR 0.000050    Time 0.066179    
2022-12-29 10:11:09,504 - Epoch: [118][   80/  113]    Overall Loss 1.383439    Objective Loss 1.383439                                        LR 0.000050    Time 0.065735    
2022-12-29 10:11:10,136 - Epoch: [118][   90/  113]    Overall Loss 1.383907    Objective Loss 1.383907                                        LR 0.000050    Time 0.065452    
2022-12-29 10:11:10,768 - Epoch: [118][  100/  113]    Overall Loss 1.379116    Objective Loss 1.379116                                        LR 0.000050    Time 0.065221    
2022-12-29 10:11:11,393 - Epoch: [118][  110/  113]    Overall Loss 1.380400    Objective Loss 1.380400                                        LR 0.000050    Time 0.064966    
2022-12-29 10:11:11,571 - Epoch: [118][  113/  113]    Overall Loss 1.381082    Objective Loss 1.381082    Top1 50.000000    Top5 87.500000    LR 0.000050    Time 0.064818    
2022-12-29 10:11:11,627 - --- validate (epoch=118)-----------
2022-12-29 10:11:11,627 - 200 samples (16 per mini-batch)
2022-12-29 10:11:12,185 - Epoch: [118][   10/   13]    Loss 1.364814    Top1 45.000000    Top5 97.500000    
2022-12-29 10:11:12,271 - Epoch: [118][   13/   13]    Loss 1.367319    Top1 45.000000    Top5 96.500000    
2022-12-29 10:11:12,318 - ==> Top1: 45.000    Top5: 96.500    Loss: 1.367

2022-12-29 10:11:12,319 - ==> Confusion:
[[12  5  0 10  1  0]
 [ 4 28  0 12  0  0]
 [ 0 14 14 13  2  1]
 [ 2 10  3 31  1  0]
 [ 1 13  4  7  5  0]
 [ 2  1  2  1  1  0]]

2022-12-29 10:11:12,321 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:12,321 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:12,330 - 

2022-12-29 10:11:12,331 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:13,176 - Epoch: [119][   10/  113]    Overall Loss 1.279097    Objective Loss 1.279097                                        LR 0.000050    Time 0.084469    
2022-12-29 10:11:13,817 - Epoch: [119][   20/  113]    Overall Loss 1.321584    Objective Loss 1.321584                                        LR 0.000050    Time 0.074234    
2022-12-29 10:11:14,458 - Epoch: [119][   30/  113]    Overall Loss 1.341114    Objective Loss 1.341114                                        LR 0.000050    Time 0.070855    
2022-12-29 10:11:15,090 - Epoch: [119][   40/  113]    Overall Loss 1.329089    Objective Loss 1.329089                                        LR 0.000050    Time 0.068910    
2022-12-29 10:11:15,729 - Epoch: [119][   50/  113]    Overall Loss 1.332013    Objective Loss 1.332013                                        LR 0.000050    Time 0.067916    
2022-12-29 10:11:16,366 - Epoch: [119][   60/  113]    Overall Loss 1.332393    Objective Loss 1.332393                                        LR 0.000050    Time 0.067202    
2022-12-29 10:11:17,000 - Epoch: [119][   70/  113]    Overall Loss 1.338898    Objective Loss 1.338898                                        LR 0.000050    Time 0.066653    
2022-12-29 10:11:17,629 - Epoch: [119][   80/  113]    Overall Loss 1.337546    Objective Loss 1.337546                                        LR 0.000050    Time 0.066174    
2022-12-29 10:11:18,266 - Epoch: [119][   90/  113]    Overall Loss 1.329757    Objective Loss 1.329757                                        LR 0.000050    Time 0.065899    
2022-12-29 10:11:18,899 - Epoch: [119][  100/  113]    Overall Loss 1.334785    Objective Loss 1.334785                                        LR 0.000050    Time 0.065627    
2022-12-29 10:11:19,526 - Epoch: [119][  110/  113]    Overall Loss 1.338861    Objective Loss 1.338861                                        LR 0.000050    Time 0.065355    
2022-12-29 10:11:19,697 - Epoch: [119][  113/  113]    Overall Loss 1.337443    Objective Loss 1.337443    Top1 62.500000    Top5 95.833333    LR 0.000050    Time 0.065137    
2022-12-29 10:11:19,760 - --- validate (epoch=119)-----------
2022-12-29 10:11:19,761 - 200 samples (16 per mini-batch)
2022-12-29 10:11:20,312 - Epoch: [119][   10/   13]    Loss 1.318394    Top1 48.750000    Top5 96.250000    
2022-12-29 10:11:20,397 - Epoch: [119][   13/   13]    Loss 1.331167    Top1 48.500000    Top5 96.000000    
2022-12-29 10:11:20,443 - ==> Top1: 48.500    Top5: 96.000    Loss: 1.331

2022-12-29 10:11:20,443 - ==> Confusion:
[[15  5  1  2  4  1]
 [ 2 35  3  3  9  0]
 [ 4 14  8  0 12  0]
 [ 3 10  1 19  5  0]
 [ 2  9  4  1 19  0]
 [ 1  3  2  0  2  1]]

2022-12-29 10:11:20,447 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:20,447 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:20,468 - 

2022-12-29 10:11:20,468 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:21,306 - Epoch: [120][   10/  113]    Overall Loss 1.311040    Objective Loss 1.311040                                        LR 0.000050    Time 0.083628    
2022-12-29 10:11:21,946 - Epoch: [120][   20/  113]    Overall Loss 1.305922    Objective Loss 1.305922                                        LR 0.000050    Time 0.073781    
2022-12-29 10:11:22,579 - Epoch: [120][   30/  113]    Overall Loss 1.337778    Objective Loss 1.337778                                        LR 0.000050    Time 0.070294    
2022-12-29 10:11:23,210 - Epoch: [120][   40/  113]    Overall Loss 1.335515    Objective Loss 1.335515                                        LR 0.000050    Time 0.068464    
2022-12-29 10:11:23,841 - Epoch: [120][   50/  113]    Overall Loss 1.316771    Objective Loss 1.316771                                        LR 0.000050    Time 0.067389    
2022-12-29 10:11:24,470 - Epoch: [120][   60/  113]    Overall Loss 1.323353    Objective Loss 1.323353                                        LR 0.000050    Time 0.066639    
2022-12-29 10:11:25,102 - Epoch: [120][   70/  113]    Overall Loss 1.328428    Objective Loss 1.328428                                        LR 0.000050    Time 0.066139    
2022-12-29 10:11:25,732 - Epoch: [120][   80/  113]    Overall Loss 1.335495    Objective Loss 1.335495                                        LR 0.000050    Time 0.065734    
2022-12-29 10:11:26,371 - Epoch: [120][   90/  113]    Overall Loss 1.327061    Objective Loss 1.327061                                        LR 0.000050    Time 0.065533    
2022-12-29 10:11:27,002 - Epoch: [120][  100/  113]    Overall Loss 1.325436    Objective Loss 1.325436                                        LR 0.000050    Time 0.065287    
2022-12-29 10:11:27,634 - Epoch: [120][  110/  113]    Overall Loss 1.330202    Objective Loss 1.330202                                        LR 0.000050    Time 0.065090    
2022-12-29 10:11:27,813 - Epoch: [120][  113/  113]    Overall Loss 1.330010    Objective Loss 1.330010    Top1 41.666667    Top5 95.833333    LR 0.000050    Time 0.064938    
2022-12-29 10:11:27,871 - --- validate (epoch=120)-----------
2022-12-29 10:11:27,872 - 200 samples (16 per mini-batch)
2022-12-29 10:11:28,417 - Epoch: [120][   10/   13]    Loss 1.382212    Top1 49.375000    Top5 96.875000    
2022-12-29 10:11:28,502 - Epoch: [120][   13/   13]    Loss 1.381808    Top1 48.000000    Top5 96.500000    
2022-12-29 10:11:28,552 - ==> Top1: 48.000    Top5: 96.500    Loss: 1.382

2022-12-29 10:11:28,552 - ==> Confusion:
[[17  7  0  5  4  0]
 [ 3 11  1  6 10  1]
 [ 3  6  8  2 12  0]
 [ 6  8  1 28 16  0]
 [ 1  5  0  3 31  0]
 [ 1  1  0  1  1  1]]

2022-12-29 10:11:28,556 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:28,556 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:28,571 - 

2022-12-29 10:11:28,571 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:29,410 - Epoch: [121][   10/  113]    Overall Loss 1.357143    Objective Loss 1.357143                                        LR 0.000050    Time 0.083742    
2022-12-29 10:11:30,044 - Epoch: [121][   20/  113]    Overall Loss 1.357513    Objective Loss 1.357513                                        LR 0.000050    Time 0.073580    
2022-12-29 10:11:30,679 - Epoch: [121][   30/  113]    Overall Loss 1.321692    Objective Loss 1.321692                                        LR 0.000050    Time 0.070199    
2022-12-29 10:11:31,313 - Epoch: [121][   40/  113]    Overall Loss 1.341320    Objective Loss 1.341320                                        LR 0.000050    Time 0.068474    
2022-12-29 10:11:31,955 - Epoch: [121][   50/  113]    Overall Loss 1.349046    Objective Loss 1.349046                                        LR 0.000050    Time 0.067614    
2022-12-29 10:11:32,589 - Epoch: [121][   60/  113]    Overall Loss 1.353203    Objective Loss 1.353203                                        LR 0.000050    Time 0.066908    
2022-12-29 10:11:33,224 - Epoch: [121][   70/  113]    Overall Loss 1.346269    Objective Loss 1.346269                                        LR 0.000050    Time 0.066406    
2022-12-29 10:11:33,861 - Epoch: [121][   80/  113]    Overall Loss 1.354582    Objective Loss 1.354582                                        LR 0.000050    Time 0.066072    
2022-12-29 10:11:34,496 - Epoch: [121][   90/  113]    Overall Loss 1.358202    Objective Loss 1.358202                                        LR 0.000050    Time 0.065779    
2022-12-29 10:11:35,137 - Epoch: [121][  100/  113]    Overall Loss 1.371622    Objective Loss 1.371622                                        LR 0.000050    Time 0.065606    
2022-12-29 10:11:35,771 - Epoch: [121][  110/  113]    Overall Loss 1.381792    Objective Loss 1.381792                                        LR 0.000050    Time 0.065399    
2022-12-29 10:11:35,944 - Epoch: [121][  113/  113]    Overall Loss 1.383985    Objective Loss 1.383985    Top1 33.333333    Top5 100.000000    LR 0.000050    Time 0.065187    
2022-12-29 10:11:36,004 - --- validate (epoch=121)-----------
2022-12-29 10:11:36,004 - 200 samples (16 per mini-batch)
2022-12-29 10:11:36,558 - Epoch: [121][   10/   13]    Loss 1.310010    Top1 51.875000    Top5 96.875000    
2022-12-29 10:11:36,643 - Epoch: [121][   13/   13]    Loss 1.357975    Top1 48.500000    Top5 95.500000    
2022-12-29 10:11:36,698 - ==> Top1: 48.500    Top5: 95.500    Loss: 1.358

2022-12-29 10:11:36,698 - ==> Confusion:
[[21  3  1  6  1  1]
 [ 5 20  6  5 15  1]
 [ 3  8 10  4  6  0]
 [ 3  3  2 30  2  0]
 [ 2  7  5  8 14  0]
 [ 1  1  2  2  0  2]]

2022-12-29 10:11:36,701 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:36,702 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:36,714 - 

2022-12-29 10:11:36,715 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:37,554 - Epoch: [122][   10/  113]    Overall Loss 1.332848    Objective Loss 1.332848                                        LR 0.000050    Time 0.083860    
2022-12-29 10:11:38,196 - Epoch: [122][   20/  113]    Overall Loss 1.340837    Objective Loss 1.340837                                        LR 0.000050    Time 0.073986    
2022-12-29 10:11:38,835 - Epoch: [122][   30/  113]    Overall Loss 1.323389    Objective Loss 1.323389                                        LR 0.000050    Time 0.070625    
2022-12-29 10:11:39,473 - Epoch: [122][   40/  113]    Overall Loss 1.302361    Objective Loss 1.302361                                        LR 0.000050    Time 0.068883    
2022-12-29 10:11:40,109 - Epoch: [122][   50/  113]    Overall Loss 1.326746    Objective Loss 1.326746                                        LR 0.000050    Time 0.067825    
2022-12-29 10:11:40,746 - Epoch: [122][   60/  113]    Overall Loss 1.344099    Objective Loss 1.344099                                        LR 0.000050    Time 0.067128    
2022-12-29 10:11:41,384 - Epoch: [122][   70/  113]    Overall Loss 1.358151    Objective Loss 1.358151                                        LR 0.000050    Time 0.066649    
2022-12-29 10:11:42,025 - Epoch: [122][   80/  113]    Overall Loss 1.356461    Objective Loss 1.356461                                        LR 0.000050    Time 0.066325    
2022-12-29 10:11:42,667 - Epoch: [122][   90/  113]    Overall Loss 1.360939    Objective Loss 1.360939                                        LR 0.000050    Time 0.066073    
2022-12-29 10:11:43,304 - Epoch: [122][  100/  113]    Overall Loss 1.354303    Objective Loss 1.354303                                        LR 0.000050    Time 0.065835    
2022-12-29 10:11:43,944 - Epoch: [122][  110/  113]    Overall Loss 1.345608    Objective Loss 1.345608                                        LR 0.000050    Time 0.065661    
2022-12-29 10:11:44,121 - Epoch: [122][  113/  113]    Overall Loss 1.350580    Objective Loss 1.350580    Top1 29.166667    Top5 95.833333    LR 0.000050    Time 0.065480    
2022-12-29 10:11:44,168 - --- validate (epoch=122)-----------
2022-12-29 10:11:44,168 - 200 samples (16 per mini-batch)
2022-12-29 10:11:44,717 - Epoch: [122][   10/   13]    Loss 1.263027    Top1 51.250000    Top5 97.500000    
2022-12-29 10:11:44,805 - Epoch: [122][   13/   13]    Loss 1.308838    Top1 48.000000    Top5 97.500000    
2022-12-29 10:11:44,857 - ==> Top1: 48.000    Top5: 97.500    Loss: 1.309

2022-12-29 10:11:44,858 - ==> Confusion:
[[25  4  0  4  2  0]
 [ 4 28  3  2  4  0]
 [ 4 13 11  5  3  0]
 [ 4  9  4 21  4  0]
 [ 7  9  6  7 11  0]
 [ 1  3  0  2  0  0]]

2022-12-29 10:11:44,860 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:44,861 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:44,886 - 

2022-12-29 10:11:44,887 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:45,728 - Epoch: [123][   10/  113]    Overall Loss 1.474063    Objective Loss 1.474063                                        LR 0.000050    Time 0.084030    
2022-12-29 10:11:46,366 - Epoch: [123][   20/  113]    Overall Loss 1.390730    Objective Loss 1.390730                                        LR 0.000050    Time 0.073910    
2022-12-29 10:11:47,005 - Epoch: [123][   30/  113]    Overall Loss 1.362444    Objective Loss 1.362444                                        LR 0.000050    Time 0.070547    
2022-12-29 10:11:47,640 - Epoch: [123][   40/  113]    Overall Loss 1.347110    Objective Loss 1.347110                                        LR 0.000050    Time 0.068774    
2022-12-29 10:11:48,283 - Epoch: [123][   50/  113]    Overall Loss 1.331879    Objective Loss 1.331879                                        LR 0.000050    Time 0.067863    
2022-12-29 10:11:48,921 - Epoch: [123][   60/  113]    Overall Loss 1.332140    Objective Loss 1.332140                                        LR 0.000050    Time 0.067185    
2022-12-29 10:11:49,558 - Epoch: [123][   70/  113]    Overall Loss 1.345147    Objective Loss 1.345147                                        LR 0.000050    Time 0.066678    
2022-12-29 10:11:50,192 - Epoch: [123][   80/  113]    Overall Loss 1.334296    Objective Loss 1.334296                                        LR 0.000050    Time 0.066265    
2022-12-29 10:11:50,829 - Epoch: [123][   90/  113]    Overall Loss 1.346569    Objective Loss 1.346569                                        LR 0.000050    Time 0.065972    
2022-12-29 10:11:51,461 - Epoch: [123][  100/  113]    Overall Loss 1.341499    Objective Loss 1.341499                                        LR 0.000050    Time 0.065691    
2022-12-29 10:11:52,087 - Epoch: [123][  110/  113]    Overall Loss 1.345637    Objective Loss 1.345637                                        LR 0.000050    Time 0.065406    
2022-12-29 10:11:52,264 - Epoch: [123][  113/  113]    Overall Loss 1.345836    Objective Loss 1.345836    Top1 41.666667    Top5 95.833333    LR 0.000050    Time 0.065232    
2022-12-29 10:11:52,312 - --- validate (epoch=123)-----------
2022-12-29 10:11:52,312 - 200 samples (16 per mini-batch)
2022-12-29 10:11:52,869 - Epoch: [123][   10/   13]    Loss 1.428180    Top1 43.750000    Top5 95.625000    
2022-12-29 10:11:52,954 - Epoch: [123][   13/   13]    Loss 1.387824    Top1 45.500000    Top5 95.500000    
2022-12-29 10:11:53,002 - ==> Top1: 45.500    Top5: 95.500    Loss: 1.388

2022-12-29 10:11:53,003 - ==> Confusion:
[[11  2  0  7  2  0]
 [ 2 13  4 12  9  0]
 [ 0  6 11 11  6  0]
 [ 2  1  0 39  6  0]
 [ 2  6  3 17 17  0]
 [ 1  2  2  1  5  0]]

2022-12-29 10:11:53,006 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:11:53,006 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:11:53,028 - 

2022-12-29 10:11:53,029 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:11:53,873 - Epoch: [124][   10/  113]    Overall Loss 1.306357    Objective Loss 1.306357                                        LR 0.000050    Time 0.084314    
2022-12-29 10:11:54,515 - Epoch: [124][   20/  113]    Overall Loss 1.342816    Objective Loss 1.342816                                        LR 0.000050    Time 0.074224    
2022-12-29 10:11:55,156 - Epoch: [124][   30/  113]    Overall Loss 1.358123    Objective Loss 1.358123                                        LR 0.000050    Time 0.070836    
2022-12-29 10:11:55,795 - Epoch: [124][   40/  113]    Overall Loss 1.339502    Objective Loss 1.339502                                        LR 0.000050    Time 0.069089    
2022-12-29 10:11:56,423 - Epoch: [124][   50/  113]    Overall Loss 1.321774    Objective Loss 1.321774                                        LR 0.000050    Time 0.067812    
2022-12-29 10:11:57,057 - Epoch: [124][   60/  113]    Overall Loss 1.337179    Objective Loss 1.337179                                        LR 0.000050    Time 0.067075    
2022-12-29 10:11:57,693 - Epoch: [124][   70/  113]    Overall Loss 1.343097    Objective Loss 1.343097                                        LR 0.000050    Time 0.066569    
2022-12-29 10:11:58,327 - Epoch: [124][   80/  113]    Overall Loss 1.341746    Objective Loss 1.341746                                        LR 0.000050    Time 0.066170    
2022-12-29 10:11:58,963 - Epoch: [124][   90/  113]    Overall Loss 1.345632    Objective Loss 1.345632                                        LR 0.000050    Time 0.065878    
2022-12-29 10:11:59,597 - Epoch: [124][  100/  113]    Overall Loss 1.349927    Objective Loss 1.349927                                        LR 0.000050    Time 0.065623    
2022-12-29 10:12:00,230 - Epoch: [124][  110/  113]    Overall Loss 1.362828    Objective Loss 1.362828                                        LR 0.000050    Time 0.065407    
2022-12-29 10:12:00,405 - Epoch: [124][  113/  113]    Overall Loss 1.363376    Objective Loss 1.363376    Top1 50.000000    Top5 100.000000    LR 0.000050    Time 0.065216    
2022-12-29 10:12:00,465 - --- validate (epoch=124)-----------
2022-12-29 10:12:00,465 - 200 samples (16 per mini-batch)
2022-12-29 10:12:01,012 - Epoch: [124][   10/   13]    Loss 1.340102    Top1 43.750000    Top5 96.875000    
2022-12-29 10:12:01,097 - Epoch: [124][   13/   13]    Loss 1.333866    Top1 47.000000    Top5 97.000000    
2022-12-29 10:12:01,150 - ==> Top1: 47.000    Top5: 97.000    Loss: 1.334

2022-12-29 10:12:01,151 - ==> Confusion:
[[18  6  0  6  4  1]
 [ 3 24  0  7 16  0]
 [ 0  9  8  1  5  1]
 [ 5  8  2 29  7  0]
 [ 3  9  2  6 15  0]
 [ 3  1  0  0  1  0]]

2022-12-29 10:12:01,153 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:01,154 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:01,177 - 

2022-12-29 10:12:01,177 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:02,033 - Epoch: [125][   10/  113]    Overall Loss 1.399539    Objective Loss 1.399539                                        LR 0.000025    Time 0.085455    
2022-12-29 10:12:02,674 - Epoch: [125][   20/  113]    Overall Loss 1.332842    Objective Loss 1.332842                                        LR 0.000025    Time 0.074732    
2022-12-29 10:12:03,315 - Epoch: [125][   30/  113]    Overall Loss 1.298413    Objective Loss 1.298413                                        LR 0.000025    Time 0.071170    
2022-12-29 10:12:03,956 - Epoch: [125][   40/  113]    Overall Loss 1.305736    Objective Loss 1.305736                                        LR 0.000025    Time 0.069405    
2022-12-29 10:12:04,589 - Epoch: [125][   50/  113]    Overall Loss 1.320280    Objective Loss 1.320280                                        LR 0.000025    Time 0.068160    
2022-12-29 10:12:05,225 - Epoch: [125][   60/  113]    Overall Loss 1.316041    Objective Loss 1.316041                                        LR 0.000025    Time 0.067390    
2022-12-29 10:12:05,861 - Epoch: [125][   70/  113]    Overall Loss 1.307114    Objective Loss 1.307114                                        LR 0.000025    Time 0.066843    
2022-12-29 10:12:06,493 - Epoch: [125][   80/  113]    Overall Loss 1.305938    Objective Loss 1.305938                                        LR 0.000025    Time 0.066390    
2022-12-29 10:12:07,126 - Epoch: [125][   90/  113]    Overall Loss 1.317402    Objective Loss 1.317402                                        LR 0.000025    Time 0.066036    
2022-12-29 10:12:07,754 - Epoch: [125][  100/  113]    Overall Loss 1.317648    Objective Loss 1.317648                                        LR 0.000025    Time 0.065706    
2022-12-29 10:12:08,382 - Epoch: [125][  110/  113]    Overall Loss 1.327069    Objective Loss 1.327069                                        LR 0.000025    Time 0.065439    
2022-12-29 10:12:08,557 - Epoch: [125][  113/  113]    Overall Loss 1.335627    Objective Loss 1.335627    Top1 29.166667    Top5 100.000000    LR 0.000025    Time 0.065251    
2022-12-29 10:12:08,621 - --- validate (epoch=125)-----------
2022-12-29 10:12:08,622 - 200 samples (16 per mini-batch)
2022-12-29 10:12:09,166 - Epoch: [125][   10/   13]    Loss 1.293070    Top1 48.750000    Top5 96.875000    
2022-12-29 10:12:09,251 - Epoch: [125][   13/   13]    Loss 1.299273    Top1 47.500000    Top5 97.000000    
2022-12-29 10:12:09,313 - ==> Top1: 47.500    Top5: 97.000    Loss: 1.299

2022-12-29 10:12:09,314 - ==> Confusion:
[[20  5  0  5  6  0]
 [ 1 18  4  3 13  0]
 [ 0  7  8  3  9  0]
 [ 4  2  1 25 12  1]
 [ 2  6  6  6 24  0]
 [ 2  1  0  2  4  0]]

2022-12-29 10:12:09,318 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:09,318 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:09,341 - 

2022-12-29 10:12:09,341 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:10,170 - Epoch: [126][   10/  113]    Overall Loss 1.261218    Objective Loss 1.261218                                        LR 0.000025    Time 0.082843    
2022-12-29 10:12:10,807 - Epoch: [126][   20/  113]    Overall Loss 1.232817    Objective Loss 1.232817                                        LR 0.000025    Time 0.073208    
2022-12-29 10:12:11,445 - Epoch: [126][   30/  113]    Overall Loss 1.254900    Objective Loss 1.254900                                        LR 0.000025    Time 0.070046    
2022-12-29 10:12:12,077 - Epoch: [126][   40/  113]    Overall Loss 1.247163    Objective Loss 1.247163                                        LR 0.000025    Time 0.068334    
2022-12-29 10:12:12,711 - Epoch: [126][   50/  113]    Overall Loss 1.269524    Objective Loss 1.269524                                        LR 0.000025    Time 0.067333    
2022-12-29 10:12:13,342 - Epoch: [126][   60/  113]    Overall Loss 1.319925    Objective Loss 1.319925                                        LR 0.000025    Time 0.066618    
2022-12-29 10:12:13,979 - Epoch: [126][   70/  113]    Overall Loss 1.335171    Objective Loss 1.335171                                        LR 0.000025    Time 0.066193    
2022-12-29 10:12:14,613 - Epoch: [126][   80/  113]    Overall Loss 1.336890    Objective Loss 1.336890                                        LR 0.000025    Time 0.065845    
2022-12-29 10:12:15,252 - Epoch: [126][   90/  113]    Overall Loss 1.332872    Objective Loss 1.332872                                        LR 0.000025    Time 0.065619    
2022-12-29 10:12:15,886 - Epoch: [126][  100/  113]    Overall Loss 1.330376    Objective Loss 1.330376                                        LR 0.000025    Time 0.065389    
2022-12-29 10:12:16,517 - Epoch: [126][  110/  113]    Overall Loss 1.337368    Objective Loss 1.337368                                        LR 0.000025    Time 0.065178    
2022-12-29 10:12:16,691 - Epoch: [126][  113/  113]    Overall Loss 1.337030    Objective Loss 1.337030    Top1 37.500000    Top5 100.000000    LR 0.000025    Time 0.064982    
2022-12-29 10:12:16,738 - --- validate (epoch=126)-----------
2022-12-29 10:12:16,738 - 200 samples (16 per mini-batch)
2022-12-29 10:12:17,300 - Epoch: [126][   10/   13]    Loss 1.309700    Top1 41.875000    Top5 98.125000    
2022-12-29 10:12:17,384 - Epoch: [126][   13/   13]    Loss 1.292071    Top1 44.000000    Top5 97.500000    
2022-12-29 10:12:17,437 - ==> Top1: 44.000    Top5: 97.500    Loss: 1.292

2022-12-29 10:12:17,438 - ==> Confusion:
[[24  0  0  4  2  0]
 [ 4 15  3  4 10  1]
 [ 2  5 11  4  9  0]
 [ 3  5  0 27  9  0]
 [ 1 14  6 15 11  0]
 [ 2  4  2  1  2  0]]

2022-12-29 10:12:17,441 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:17,441 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:17,455 - 

2022-12-29 10:12:17,455 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:18,277 - Epoch: [127][   10/  113]    Overall Loss 1.362938    Objective Loss 1.362938                                        LR 0.000025    Time 0.082080    
2022-12-29 10:12:18,914 - Epoch: [127][   20/  113]    Overall Loss 1.420320    Objective Loss 1.420320                                        LR 0.000025    Time 0.072857    
2022-12-29 10:12:19,549 - Epoch: [127][   30/  113]    Overall Loss 1.389458    Objective Loss 1.389458                                        LR 0.000025    Time 0.069725    
2022-12-29 10:12:20,184 - Epoch: [127][   40/  113]    Overall Loss 1.339899    Objective Loss 1.339899                                        LR 0.000025    Time 0.068166    
2022-12-29 10:12:20,815 - Epoch: [127][   50/  113]    Overall Loss 1.355269    Objective Loss 1.355269                                        LR 0.000025    Time 0.067133    
2022-12-29 10:12:21,447 - Epoch: [127][   60/  113]    Overall Loss 1.358124    Objective Loss 1.358124                                        LR 0.000025    Time 0.066478    
2022-12-29 10:12:22,085 - Epoch: [127][   70/  113]    Overall Loss 1.365393    Objective Loss 1.365393                                        LR 0.000025    Time 0.066088    
2022-12-29 10:12:22,720 - Epoch: [127][   80/  113]    Overall Loss 1.363533    Objective Loss 1.363533                                        LR 0.000025    Time 0.065750    
2022-12-29 10:12:23,353 - Epoch: [127][   90/  113]    Overall Loss 1.360207    Objective Loss 1.360207                                        LR 0.000025    Time 0.065477    
2022-12-29 10:12:23,985 - Epoch: [127][  100/  113]    Overall Loss 1.353053    Objective Loss 1.353053                                        LR 0.000025    Time 0.065240    
2022-12-29 10:12:24,611 - Epoch: [127][  110/  113]    Overall Loss 1.353236    Objective Loss 1.353236                                        LR 0.000025    Time 0.064995    
2022-12-29 10:12:24,784 - Epoch: [127][  113/  113]    Overall Loss 1.350867    Objective Loss 1.350867    Top1 50.000000    Top5 100.000000    LR 0.000025    Time 0.064805    
2022-12-29 10:12:24,841 - --- validate (epoch=127)-----------
2022-12-29 10:12:24,842 - 200 samples (16 per mini-batch)
2022-12-29 10:12:25,384 - Epoch: [127][   10/   13]    Loss 1.359582    Top1 40.625000    Top5 96.250000    
2022-12-29 10:12:25,468 - Epoch: [127][   13/   13]    Loss 1.341200    Top1 41.500000    Top5 96.500000    
2022-12-29 10:12:25,516 - ==> Top1: 41.500    Top5: 96.500    Loss: 1.341

2022-12-29 10:12:25,516 - ==> Confusion:
[[12  4  0  3  3  1]
 [ 2 18  3  8 10  0]
 [ 1 11 10  4 14  1]
 [ 3  5  5 21  7  0]
 [ 0  8  5  8 21  0]
 [ 2  4  0  1  4  1]]

2022-12-29 10:12:25,518 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:25,518 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:25,538 - 

2022-12-29 10:12:25,538 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:26,368 - Epoch: [128][   10/  113]    Overall Loss 1.360616    Objective Loss 1.360616                                        LR 0.000025    Time 0.082758    
2022-12-29 10:12:27,009 - Epoch: [128][   20/  113]    Overall Loss 1.336394    Objective Loss 1.336394                                        LR 0.000025    Time 0.073400    
2022-12-29 10:12:27,648 - Epoch: [128][   30/  113]    Overall Loss 1.315473    Objective Loss 1.315473                                        LR 0.000025    Time 0.070238    
2022-12-29 10:12:28,288 - Epoch: [128][   40/  113]    Overall Loss 1.312698    Objective Loss 1.312698                                        LR 0.000025    Time 0.068669    
2022-12-29 10:12:28,921 - Epoch: [128][   50/  113]    Overall Loss 1.306424    Objective Loss 1.306424                                        LR 0.000025    Time 0.067588    
2022-12-29 10:12:29,558 - Epoch: [128][   60/  113]    Overall Loss 1.315162    Objective Loss 1.315162                                        LR 0.000025    Time 0.066930    
2022-12-29 10:12:30,191 - Epoch: [128][   70/  113]    Overall Loss 1.318883    Objective Loss 1.318883                                        LR 0.000025    Time 0.066403    
2022-12-29 10:12:30,822 - Epoch: [128][   80/  113]    Overall Loss 1.322366    Objective Loss 1.322366                                        LR 0.000025    Time 0.065984    
2022-12-29 10:12:31,457 - Epoch: [128][   90/  113]    Overall Loss 1.322800    Objective Loss 1.322800                                        LR 0.000025    Time 0.065703    
2022-12-29 10:12:32,097 - Epoch: [128][  100/  113]    Overall Loss 1.327857    Objective Loss 1.327857                                        LR 0.000025    Time 0.065525    
2022-12-29 10:12:32,723 - Epoch: [128][  110/  113]    Overall Loss 1.330480    Objective Loss 1.330480                                        LR 0.000025    Time 0.065258    
2022-12-29 10:12:32,895 - Epoch: [128][  113/  113]    Overall Loss 1.337301    Objective Loss 1.337301    Top1 41.666667    Top5 87.500000    LR 0.000025    Time 0.065045    
2022-12-29 10:12:32,954 - --- validate (epoch=128)-----------
2022-12-29 10:12:32,954 - 200 samples (16 per mini-batch)
2022-12-29 10:12:33,500 - Epoch: [128][   10/   13]    Loss 1.396230    Top1 39.375000    Top5 98.750000    
2022-12-29 10:12:33,586 - Epoch: [128][   13/   13]    Loss 1.360226    Top1 42.500000    Top5 98.000000    
2022-12-29 10:12:33,649 - ==> Top1: 42.500    Top5: 98.000    Loss: 1.360

2022-12-29 10:12:33,650 - ==> Confusion:
[[23  6  0 12  2  0]
 [ 0 11  6  6 12  0]
 [ 1  4  6  5 13  0]
 [ 6  5  1 28  9  0]
 [ 1  2  5 11 17  1]
 [ 0  2  1  1  3  0]]

2022-12-29 10:12:33,653 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:33,653 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:33,677 - 

2022-12-29 10:12:33,677 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:34,513 - Epoch: [129][   10/  113]    Overall Loss 1.242336    Objective Loss 1.242336                                        LR 0.000025    Time 0.083489    
2022-12-29 10:12:35,152 - Epoch: [129][   20/  113]    Overall Loss 1.228329    Objective Loss 1.228329                                        LR 0.000025    Time 0.073657    
2022-12-29 10:12:35,785 - Epoch: [129][   30/  113]    Overall Loss 1.270998    Objective Loss 1.270998                                        LR 0.000025    Time 0.070201    
2022-12-29 10:12:36,419 - Epoch: [129][   40/  113]    Overall Loss 1.281718    Objective Loss 1.281718                                        LR 0.000025    Time 0.068493    
2022-12-29 10:12:37,051 - Epoch: [129][   50/  113]    Overall Loss 1.291315    Objective Loss 1.291315                                        LR 0.000025    Time 0.067417    
2022-12-29 10:12:37,681 - Epoch: [129][   60/  113]    Overall Loss 1.296277    Objective Loss 1.296277                                        LR 0.000025    Time 0.066684    
2022-12-29 10:12:38,313 - Epoch: [129][   70/  113]    Overall Loss 1.294343    Objective Loss 1.294343                                        LR 0.000025    Time 0.066181    
2022-12-29 10:12:38,947 - Epoch: [129][   80/  113]    Overall Loss 1.297623    Objective Loss 1.297623                                        LR 0.000025    Time 0.065825    
2022-12-29 10:12:39,581 - Epoch: [129][   90/  113]    Overall Loss 1.311839    Objective Loss 1.311839                                        LR 0.000025    Time 0.065550    
2022-12-29 10:12:40,215 - Epoch: [129][  100/  113]    Overall Loss 1.312564    Objective Loss 1.312564                                        LR 0.000025    Time 0.065327    
2022-12-29 10:12:40,839 - Epoch: [129][  110/  113]    Overall Loss 1.314958    Objective Loss 1.314958                                        LR 0.000025    Time 0.065053    
2022-12-29 10:12:41,013 - Epoch: [129][  113/  113]    Overall Loss 1.319365    Objective Loss 1.319365    Top1 41.666667    Top5 100.000000    LR 0.000025    Time 0.064867    
2022-12-29 10:12:41,066 - --- validate (epoch=129)-----------
2022-12-29 10:12:41,066 - 200 samples (16 per mini-batch)
2022-12-29 10:12:41,620 - Epoch: [129][   10/   13]    Loss 1.276991    Top1 50.000000    Top5 96.875000    
2022-12-29 10:12:41,705 - Epoch: [129][   13/   13]    Loss 1.305574    Top1 50.000000    Top5 96.500000    
2022-12-29 10:12:41,759 - ==> Top1: 50.000    Top5: 96.500    Loss: 1.306

2022-12-29 10:12:41,760 - ==> Confusion:
[[25  3  2  5  3  0]
 [ 3 23  5  5  4  0]
 [ 2  9  9 10  6  0]
 [ 4  5  1 28  5  0]
 [ 1 10  5  4 15  0]
 [ 0  3  0  3  2  0]]

2022-12-29 10:12:41,761 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:41,761 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:41,773 - 

2022-12-29 10:12:41,773 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:42,601 - Epoch: [130][   10/  113]    Overall Loss 1.335344    Objective Loss 1.335344                                        LR 0.000025    Time 0.082657    
2022-12-29 10:12:43,236 - Epoch: [130][   20/  113]    Overall Loss 1.372665    Objective Loss 1.372665                                        LR 0.000025    Time 0.073053    
2022-12-29 10:12:43,873 - Epoch: [130][   30/  113]    Overall Loss 1.381942    Objective Loss 1.381942                                        LR 0.000025    Time 0.069904    
2022-12-29 10:12:44,508 - Epoch: [130][   40/  113]    Overall Loss 1.346513    Objective Loss 1.346513                                        LR 0.000025    Time 0.068294    
2022-12-29 10:12:45,144 - Epoch: [130][   50/  113]    Overall Loss 1.337630    Objective Loss 1.337630                                        LR 0.000025    Time 0.067356    
2022-12-29 10:12:45,778 - Epoch: [130][   60/  113]    Overall Loss 1.339754    Objective Loss 1.339754                                        LR 0.000025    Time 0.066676    
2022-12-29 10:12:46,408 - Epoch: [130][   70/  113]    Overall Loss 1.341439    Objective Loss 1.341439                                        LR 0.000025    Time 0.066142    
2022-12-29 10:12:47,036 - Epoch: [130][   80/  113]    Overall Loss 1.326030    Objective Loss 1.326030                                        LR 0.000025    Time 0.065725    
2022-12-29 10:12:47,664 - Epoch: [130][   90/  113]    Overall Loss 1.323567    Objective Loss 1.323567                                        LR 0.000025    Time 0.065390    
2022-12-29 10:12:48,294 - Epoch: [130][  100/  113]    Overall Loss 1.320858    Objective Loss 1.320858                                        LR 0.000025    Time 0.065146    
2022-12-29 10:12:48,915 - Epoch: [130][  110/  113]    Overall Loss 1.319396    Objective Loss 1.319396                                        LR 0.000025    Time 0.064864    
2022-12-29 10:12:49,090 - Epoch: [130][  113/  113]    Overall Loss 1.317673    Objective Loss 1.317673    Top1 45.833333    Top5 100.000000    LR 0.000025    Time 0.064693    
2022-12-29 10:12:49,143 - --- validate (epoch=130)-----------
2022-12-29 10:12:49,144 - 200 samples (16 per mini-batch)
2022-12-29 10:12:49,683 - Epoch: [130][   10/   13]    Loss 1.346122    Top1 46.250000    Top5 96.875000    
2022-12-29 10:12:49,769 - Epoch: [130][   13/   13]    Loss 1.351560    Top1 47.000000    Top5 95.500000    
2022-12-29 10:12:49,818 - ==> Top1: 47.000    Top5: 95.500    Loss: 1.352

2022-12-29 10:12:49,819 - ==> Confusion:
[[13  0  1  3  4  0]
 [ 4 13  5  6 13  0]
 [ 1  2 21  7 14  0]
 [ 7  3  2 29  8  0]
 [ 1  3  2  7 18  0]
 [ 0  2  0  4  7  0]]

2022-12-29 10:12:49,822 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:49,822 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:49,843 - 

2022-12-29 10:12:49,843 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:50,683 - Epoch: [131][   10/  113]    Overall Loss 1.285782    Objective Loss 1.285782                                        LR 0.000025    Time 0.083852    
2022-12-29 10:12:51,324 - Epoch: [131][   20/  113]    Overall Loss 1.277681    Objective Loss 1.277681                                        LR 0.000025    Time 0.073985    
2022-12-29 10:12:51,965 - Epoch: [131][   30/  113]    Overall Loss 1.296095    Objective Loss 1.296095                                        LR 0.000025    Time 0.070673    
2022-12-29 10:12:52,604 - Epoch: [131][   40/  113]    Overall Loss 1.324628    Objective Loss 1.324628                                        LR 0.000025    Time 0.068960    
2022-12-29 10:12:53,243 - Epoch: [131][   50/  113]    Overall Loss 1.326680    Objective Loss 1.326680                                        LR 0.000025    Time 0.067936    
2022-12-29 10:12:53,879 - Epoch: [131][   60/  113]    Overall Loss 1.308969    Objective Loss 1.308969                                        LR 0.000025    Time 0.067205    
2022-12-29 10:12:54,516 - Epoch: [131][   70/  113]    Overall Loss 1.319470    Objective Loss 1.319470                                        LR 0.000025    Time 0.066692    
2022-12-29 10:12:55,150 - Epoch: [131][   80/  113]    Overall Loss 1.320391    Objective Loss 1.320391                                        LR 0.000025    Time 0.066285    
2022-12-29 10:12:55,789 - Epoch: [131][   90/  113]    Overall Loss 1.326900    Objective Loss 1.326900                                        LR 0.000025    Time 0.066014    
2022-12-29 10:12:56,426 - Epoch: [131][  100/  113]    Overall Loss 1.322723    Objective Loss 1.322723                                        LR 0.000025    Time 0.065774    
2022-12-29 10:12:57,049 - Epoch: [131][  110/  113]    Overall Loss 1.330009    Objective Loss 1.330009                                        LR 0.000025    Time 0.065451    
2022-12-29 10:12:57,224 - Epoch: [131][  113/  113]    Overall Loss 1.328536    Objective Loss 1.328536    Top1 45.833333    Top5 100.000000    LR 0.000025    Time 0.065264    
2022-12-29 10:12:57,287 - --- validate (epoch=131)-----------
2022-12-29 10:12:57,287 - 200 samples (16 per mini-batch)
2022-12-29 10:12:57,835 - Epoch: [131][   10/   13]    Loss 1.386871    Top1 44.375000    Top5 98.750000    
2022-12-29 10:12:57,920 - Epoch: [131][   13/   13]    Loss 1.379626    Top1 44.000000    Top5 98.500000    
2022-12-29 10:12:57,968 - ==> Top1: 44.000    Top5: 98.500    Loss: 1.380

2022-12-29 10:12:57,968 - ==> Confusion:
[[22  3  1  3  1  1]
 [ 1 23  3  4 10  0]
 [ 1 11  8  5  9  0]
 [ 1  8  1 27  7  2]
 [ 5 13  0  9  7  1]
 [ 3  5  2  1  1  1]]

2022-12-29 10:12:57,970 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:12:57,971 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:12:57,990 - 

2022-12-29 10:12:57,990 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:12:58,823 - Epoch: [132][   10/  113]    Overall Loss 1.375973    Objective Loss 1.375973                                        LR 0.000025    Time 0.083143    
2022-12-29 10:12:59,460 - Epoch: [132][   20/  113]    Overall Loss 1.301030    Objective Loss 1.301030                                        LR 0.000025    Time 0.073430    
2022-12-29 10:13:00,099 - Epoch: [132][   30/  113]    Overall Loss 1.299699    Objective Loss 1.299699                                        LR 0.000025    Time 0.070215    
2022-12-29 10:13:00,737 - Epoch: [132][   40/  113]    Overall Loss 1.314607    Objective Loss 1.314607                                        LR 0.000025    Time 0.068601    
2022-12-29 10:13:01,370 - Epoch: [132][   50/  113]    Overall Loss 1.283761    Objective Loss 1.283761                                        LR 0.000025    Time 0.067526    
2022-12-29 10:13:02,006 - Epoch: [132][   60/  113]    Overall Loss 1.279834    Objective Loss 1.279834                                        LR 0.000025    Time 0.066876    
2022-12-29 10:13:02,639 - Epoch: [132][   70/  113]    Overall Loss 1.282059    Objective Loss 1.282059                                        LR 0.000025    Time 0.066359    
2022-12-29 10:13:03,268 - Epoch: [132][   80/  113]    Overall Loss 1.277228    Objective Loss 1.277228                                        LR 0.000025    Time 0.065913    
2022-12-29 10:13:03,900 - Epoch: [132][   90/  113]    Overall Loss 1.291210    Objective Loss 1.291210                                        LR 0.000025    Time 0.065608    
2022-12-29 10:13:04,530 - Epoch: [132][  100/  113]    Overall Loss 1.298836    Objective Loss 1.298836                                        LR 0.000025    Time 0.065344    
2022-12-29 10:13:05,158 - Epoch: [132][  110/  113]    Overall Loss 1.298756    Objective Loss 1.298756                                        LR 0.000025    Time 0.065106    
2022-12-29 10:13:05,334 - Epoch: [132][  113/  113]    Overall Loss 1.302858    Objective Loss 1.302858    Top1 41.666667    Top5 100.000000    LR 0.000025    Time 0.064933    
2022-12-29 10:13:05,400 - --- validate (epoch=132)-----------
2022-12-29 10:13:05,400 - 200 samples (16 per mini-batch)
2022-12-29 10:13:05,964 - Epoch: [132][   10/   13]    Loss 1.435483    Top1 41.250000    Top5 96.875000    
2022-12-29 10:13:06,051 - Epoch: [132][   13/   13]    Loss 1.433800    Top1 44.500000    Top5 97.000000    
2022-12-29 10:13:06,112 - ==> Top1: 44.500    Top5: 97.000    Loss: 1.434

2022-12-29 10:13:06,112 - ==> Confusion:
[[24  7  2  4  2  1]
 [ 5 18  3  5  9  0]
 [ 0 11 10  6  5  0]
 [ 2 10  3 15  5  0]
 [ 3 12  4  5 18  0]
 [ 0  0  4  1  2  4]]

2022-12-29 10:13:06,115 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:13:06,115 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:06,130 - 

2022-12-29 10:13:06,130 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:06,981 - Epoch: [133][   10/  113]    Overall Loss 1.317812    Objective Loss 1.317812                                        LR 0.000025    Time 0.084937    
2022-12-29 10:13:07,620 - Epoch: [133][   20/  113]    Overall Loss 1.338758    Objective Loss 1.338758                                        LR 0.000025    Time 0.074418    
2022-12-29 10:13:08,261 - Epoch: [133][   30/  113]    Overall Loss 1.358975    Objective Loss 1.358975                                        LR 0.000025    Time 0.070970    
2022-12-29 10:13:08,902 - Epoch: [133][   40/  113]    Overall Loss 1.331345    Objective Loss 1.331345                                        LR 0.000025    Time 0.069225    
2022-12-29 10:13:09,537 - Epoch: [133][   50/  113]    Overall Loss 1.352904    Objective Loss 1.352904                                        LR 0.000025    Time 0.068074    
2022-12-29 10:13:10,172 - Epoch: [133][   60/  113]    Overall Loss 1.353482    Objective Loss 1.353482                                        LR 0.000025    Time 0.067306    
2022-12-29 10:13:10,810 - Epoch: [133][   70/  113]    Overall Loss 1.343710    Objective Loss 1.343710                                        LR 0.000025    Time 0.066802    
2022-12-29 10:13:11,446 - Epoch: [133][   80/  113]    Overall Loss 1.334502    Objective Loss 1.334502                                        LR 0.000025    Time 0.066386    
2022-12-29 10:13:12,087 - Epoch: [133][   90/  113]    Overall Loss 1.336481    Objective Loss 1.336481                                        LR 0.000025    Time 0.066132    
2022-12-29 10:13:12,721 - Epoch: [133][  100/  113]    Overall Loss 1.332031    Objective Loss 1.332031                                        LR 0.000025    Time 0.065855    
2022-12-29 10:13:13,354 - Epoch: [133][  110/  113]    Overall Loss 1.342129    Objective Loss 1.342129                                        LR 0.000025    Time 0.065614    
2022-12-29 10:13:13,524 - Epoch: [133][  113/  113]    Overall Loss 1.343347    Objective Loss 1.343347    Top1 41.666667    Top5 100.000000    LR 0.000025    Time 0.065375    
2022-12-29 10:13:13,577 - --- validate (epoch=133)-----------
2022-12-29 10:13:13,578 - 200 samples (16 per mini-batch)
2022-12-29 10:13:14,120 - Epoch: [133][   10/   13]    Loss 1.281807    Top1 51.250000    Top5 97.500000    
2022-12-29 10:13:14,206 - Epoch: [133][   13/   13]    Loss 1.276587    Top1 50.500000    Top5 96.500000    
2022-12-29 10:13:14,264 - ==> Top1: 50.500    Top5: 96.500    Loss: 1.277

2022-12-29 10:13:14,265 - ==> Confusion:
[[13  3  0 11  1  0]
 [ 3 15  6  6 11  0]
 [ 0  6 15  8  4  0]
 [ 0  5  0 39  6  1]
 [ 2  6  3 10 18  0]
 [ 2  1  0  1  3  1]]

2022-12-29 10:13:14,266 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:13:14,267 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:14,285 - 

2022-12-29 10:13:14,285 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:15,142 - Epoch: [134][   10/  113]    Overall Loss 1.317393    Objective Loss 1.317393                                        LR 0.000025    Time 0.085551    
2022-12-29 10:13:15,779 - Epoch: [134][   20/  113]    Overall Loss 1.246351    Objective Loss 1.246351                                        LR 0.000025    Time 0.074637    
2022-12-29 10:13:16,410 - Epoch: [134][   30/  113]    Overall Loss 1.254829    Objective Loss 1.254829                                        LR 0.000025    Time 0.070778    
2022-12-29 10:13:17,044 - Epoch: [134][   40/  113]    Overall Loss 1.282623    Objective Loss 1.282623                                        LR 0.000025    Time 0.068903    
2022-12-29 10:13:17,677 - Epoch: [134][   50/  113]    Overall Loss 1.310770    Objective Loss 1.310770                                        LR 0.000025    Time 0.067788    
2022-12-29 10:13:18,310 - Epoch: [134][   60/  113]    Overall Loss 1.323944    Objective Loss 1.323944                                        LR 0.000025    Time 0.067028    
2022-12-29 10:13:18,946 - Epoch: [134][   70/  113]    Overall Loss 1.315457    Objective Loss 1.315457                                        LR 0.000025    Time 0.066527    
2022-12-29 10:13:19,577 - Epoch: [134][   80/  113]    Overall Loss 1.294321    Objective Loss 1.294321                                        LR 0.000025    Time 0.066093    
2022-12-29 10:13:20,212 - Epoch: [134][   90/  113]    Overall Loss 1.286355    Objective Loss 1.286355                                        LR 0.000025    Time 0.065805    
2022-12-29 10:13:20,847 - Epoch: [134][  100/  113]    Overall Loss 1.286481    Objective Loss 1.286481                                        LR 0.000025    Time 0.065562    
2022-12-29 10:13:21,468 - Epoch: [134][  110/  113]    Overall Loss 1.296263    Objective Loss 1.296263                                        LR 0.000025    Time 0.065251    
2022-12-29 10:13:21,645 - Epoch: [134][  113/  113]    Overall Loss 1.301798    Objective Loss 1.301798    Top1 45.833333    Top5 91.666667    LR 0.000025    Time 0.065080    
2022-12-29 10:13:21,702 - --- validate (epoch=134)-----------
2022-12-29 10:13:21,702 - 200 samples (16 per mini-batch)
2022-12-29 10:13:22,246 - Epoch: [134][   10/   13]    Loss 1.358473    Top1 43.125000    Top5 95.625000    
2022-12-29 10:13:22,331 - Epoch: [134][   13/   13]    Loss 1.384998    Top1 41.500000    Top5 96.500000    
2022-12-29 10:13:22,401 - ==> Top1: 41.500    Top5: 96.500    Loss: 1.385

2022-12-29 10:13:22,401 - ==> Confusion:
[[18  3  0  3  4  0]
 [ 1 19  2  2 17  0]
 [ 0 10  7  4 14  0]
 [ 5  6  1 24 16  0]
 [ 1  7  1  4 14  0]
 [ 4  3  1  3  5  1]]

2022-12-29 10:13:22,405 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:13:22,405 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:22,418 - 

2022-12-29 10:13:22,418 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:23,252 - Epoch: [135][   10/  113]    Overall Loss 1.350305    Objective Loss 1.350305                                        LR 0.000025    Time 0.083282    
2022-12-29 10:13:23,892 - Epoch: [135][   20/  113]    Overall Loss 1.283965    Objective Loss 1.283965                                        LR 0.000025    Time 0.073610    
2022-12-29 10:13:24,532 - Epoch: [135][   30/  113]    Overall Loss 1.271995    Objective Loss 1.271995                                        LR 0.000025    Time 0.070382    
2022-12-29 10:13:25,175 - Epoch: [135][   40/  113]    Overall Loss 1.290561    Objective Loss 1.290561                                        LR 0.000025    Time 0.068849    
2022-12-29 10:13:25,810 - Epoch: [135][   50/  113]    Overall Loss 1.261603    Objective Loss 1.261603                                        LR 0.000025    Time 0.067777    
2022-12-29 10:13:26,446 - Epoch: [135][   60/  113]    Overall Loss 1.271079    Objective Loss 1.271079                                        LR 0.000025    Time 0.067068    
2022-12-29 10:13:27,087 - Epoch: [135][   70/  113]    Overall Loss 1.281655    Objective Loss 1.281655                                        LR 0.000025    Time 0.066638    
2022-12-29 10:13:27,719 - Epoch: [135][   80/  113]    Overall Loss 1.283532    Objective Loss 1.283532                                        LR 0.000025    Time 0.066208    
2022-12-29 10:13:28,361 - Epoch: [135][   90/  113]    Overall Loss 1.290261    Objective Loss 1.290261                                        LR 0.000025    Time 0.065974    
2022-12-29 10:13:29,000 - Epoch: [135][  100/  113]    Overall Loss 1.284585    Objective Loss 1.284585                                        LR 0.000025    Time 0.065757    
2022-12-29 10:13:29,626 - Epoch: [135][  110/  113]    Overall Loss 1.284407    Objective Loss 1.284407                                        LR 0.000025    Time 0.065468    
2022-12-29 10:13:29,800 - Epoch: [135][  113/  113]    Overall Loss 1.283827    Objective Loss 1.283827    Top1 50.000000    Top5 95.833333    LR 0.000025    Time 0.065272    
2022-12-29 10:13:29,848 - --- validate (epoch=135)-----------
2022-12-29 10:13:29,849 - 200 samples (16 per mini-batch)
2022-12-29 10:13:30,413 - Epoch: [135][   10/   13]    Loss 1.351446    Top1 50.625000    Top5 95.000000    
2022-12-29 10:13:30,498 - Epoch: [135][   13/   13]    Loss 1.337587    Top1 49.000000    Top5 96.000000    
2022-12-29 10:13:30,546 - ==> Top1: 49.000    Top5: 96.000    Loss: 1.338

2022-12-29 10:13:30,547 - ==> Confusion:
[[14  1  0  8  2  0]
 [ 1 22  2  8 12  0]
 [ 4  4  4 10  4  0]
 [ 3  4  0 32  8  1]
 [ 1  9  1  7 26  0]
 [ 1  3  0  4  4  0]]

2022-12-29 10:13:30,549 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:13:30,549 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:30,572 - 

2022-12-29 10:13:30,572 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:31,423 - Epoch: [136][   10/  113]    Overall Loss 1.238432    Objective Loss 1.238432                                        LR 0.000025    Time 0.085002    
2022-12-29 10:13:32,069 - Epoch: [136][   20/  113]    Overall Loss 1.268926    Objective Loss 1.268926                                        LR 0.000025    Time 0.074745    
2022-12-29 10:13:32,709 - Epoch: [136][   30/  113]    Overall Loss 1.254954    Objective Loss 1.254954                                        LR 0.000025    Time 0.071152    
2022-12-29 10:13:33,343 - Epoch: [136][   40/  113]    Overall Loss 1.241232    Objective Loss 1.241232                                        LR 0.000025    Time 0.069191    
2022-12-29 10:13:33,977 - Epoch: [136][   50/  113]    Overall Loss 1.251872    Objective Loss 1.251872                                        LR 0.000025    Time 0.068024    
2022-12-29 10:13:34,612 - Epoch: [136][   60/  113]    Overall Loss 1.260489    Objective Loss 1.260489                                        LR 0.000025    Time 0.067273    
2022-12-29 10:13:35,253 - Epoch: [136][   70/  113]    Overall Loss 1.264162    Objective Loss 1.264162                                        LR 0.000025    Time 0.066815    
2022-12-29 10:13:35,888 - Epoch: [136][   80/  113]    Overall Loss 1.280278    Objective Loss 1.280278                                        LR 0.000025    Time 0.066392    
2022-12-29 10:13:36,526 - Epoch: [136][   90/  113]    Overall Loss 1.287525    Objective Loss 1.287525                                        LR 0.000025    Time 0.066095    
2022-12-29 10:13:37,164 - Epoch: [136][  100/  113]    Overall Loss 1.285959    Objective Loss 1.285959                                        LR 0.000025    Time 0.065861    
2022-12-29 10:13:37,791 - Epoch: [136][  110/  113]    Overall Loss 1.293575    Objective Loss 1.293575                                        LR 0.000025    Time 0.065567    
2022-12-29 10:13:37,961 - Epoch: [136][  113/  113]    Overall Loss 1.293703    Objective Loss 1.293703    Top1 45.833333    Top5 91.666667    LR 0.000025    Time 0.065325    
2022-12-29 10:13:38,021 - --- validate (epoch=136)-----------
2022-12-29 10:13:38,022 - 200 samples (16 per mini-batch)
2022-12-29 10:13:38,578 - Epoch: [136][   10/   13]    Loss 1.461967    Top1 38.750000    Top5 93.125000    
2022-12-29 10:13:38,663 - Epoch: [136][   13/   13]    Loss 1.402625    Top1 39.500000    Top5 94.000000    
2022-12-29 10:13:38,712 - ==> Top1: 39.500    Top5: 94.000    Loss: 1.403

2022-12-29 10:13:38,713 - ==> Confusion:
[[15  4  0  5  0  0]
 [ 6 24  4  9  6  1]
 [ 3  8  7 13  8  1]
 [ 8  7  0 24  3  0]
 [ 2 12  0  4  6  1]
 [ 1 10  1  3  1  3]]

2022-12-29 10:13:38,715 - ==> Best [Top1: 51.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 115]
2022-12-29 10:13:38,715 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:38,741 - 

2022-12-29 10:13:38,741 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:39,572 - Epoch: [137][   10/  113]    Overall Loss 1.267386    Objective Loss 1.267386                                        LR 0.000025    Time 0.082985    
2022-12-29 10:13:40,215 - Epoch: [137][   20/  113]    Overall Loss 1.246217    Objective Loss 1.246217                                        LR 0.000025    Time 0.073623    
2022-12-29 10:13:40,856 - Epoch: [137][   30/  113]    Overall Loss 1.303679    Objective Loss 1.303679                                        LR 0.000025    Time 0.070429    
2022-12-29 10:13:41,493 - Epoch: [137][   40/  113]    Overall Loss 1.309610    Objective Loss 1.309610                                        LR 0.000025    Time 0.068741    
2022-12-29 10:13:42,136 - Epoch: [137][   50/  113]    Overall Loss 1.314598    Objective Loss 1.314598                                        LR 0.000025    Time 0.067833    
2022-12-29 10:13:42,770 - Epoch: [137][   60/  113]    Overall Loss 1.313280    Objective Loss 1.313280                                        LR 0.000025    Time 0.067093    
2022-12-29 10:13:43,407 - Epoch: [137][   70/  113]    Overall Loss 1.301618    Objective Loss 1.301618                                        LR 0.000025    Time 0.066598    
2022-12-29 10:13:44,038 - Epoch: [137][   80/  113]    Overall Loss 1.289228    Objective Loss 1.289228                                        LR 0.000025    Time 0.066153    
2022-12-29 10:13:44,674 - Epoch: [137][   90/  113]    Overall Loss 1.288539    Objective Loss 1.288539                                        LR 0.000025    Time 0.065862    
2022-12-29 10:13:45,311 - Epoch: [137][  100/  113]    Overall Loss 1.288970    Objective Loss 1.288970                                        LR 0.000025    Time 0.065644    
2022-12-29 10:13:45,935 - Epoch: [137][  110/  113]    Overall Loss 1.298775    Objective Loss 1.298775                                        LR 0.000025    Time 0.065339    
2022-12-29 10:13:46,104 - Epoch: [137][  113/  113]    Overall Loss 1.296198    Objective Loss 1.296198    Top1 50.000000    Top5 95.833333    LR 0.000025    Time 0.065096    
2022-12-29 10:13:46,156 - --- validate (epoch=137)-----------
2022-12-29 10:13:46,156 - 200 samples (16 per mini-batch)
2022-12-29 10:13:46,712 - Epoch: [137][   10/   13]    Loss 1.157997    Top1 55.625000    Top5 98.125000    
2022-12-29 10:13:46,796 - Epoch: [137][   13/   13]    Loss 1.197092    Top1 52.500000    Top5 98.000000    
2022-12-29 10:13:46,860 - ==> Top1: 52.500    Top5: 98.000    Loss: 1.197

2022-12-29 10:13:46,860 - ==> Confusion:
[[20  2  0  4  1  0]
 [ 4 22  5 14  6  0]
 [ 3  8  9  4  5  0]
 [ 5  2  1 43  6  0]
 [ 3 11  3  1 11  0]
 [ 1  1  1  1  3  0]]

2022-12-29 10:13:46,863 - ==> Best [Top1: 52.500   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 137]
2022-12-29 10:13:46,864 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:46,878 - 

2022-12-29 10:13:46,879 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:47,715 - Epoch: [138][   10/  113]    Overall Loss 1.306439    Objective Loss 1.306439                                        LR 0.000025    Time 0.083550    
2022-12-29 10:13:48,353 - Epoch: [138][   20/  113]    Overall Loss 1.346918    Objective Loss 1.346918                                        LR 0.000025    Time 0.073640    
2022-12-29 10:13:48,994 - Epoch: [138][   30/  113]    Overall Loss 1.360115    Objective Loss 1.360115                                        LR 0.000025    Time 0.070443    
2022-12-29 10:13:49,629 - Epoch: [138][   40/  113]    Overall Loss 1.330073    Objective Loss 1.330073                                        LR 0.000025    Time 0.068702    
2022-12-29 10:13:50,264 - Epoch: [138][   50/  113]    Overall Loss 1.330728    Objective Loss 1.330728                                        LR 0.000025    Time 0.067638    
2022-12-29 10:13:50,899 - Epoch: [138][   60/  113]    Overall Loss 1.324210    Objective Loss 1.324210                                        LR 0.000025    Time 0.066944    
2022-12-29 10:13:51,535 - Epoch: [138][   70/  113]    Overall Loss 1.330873    Objective Loss 1.330873                                        LR 0.000025    Time 0.066463    
2022-12-29 10:13:52,172 - Epoch: [138][   80/  113]    Overall Loss 1.307395    Objective Loss 1.307395                                        LR 0.000025    Time 0.066113    
2022-12-29 10:13:52,809 - Epoch: [138][   90/  113]    Overall Loss 1.307254    Objective Loss 1.307254                                        LR 0.000025    Time 0.065834    
2022-12-29 10:13:53,444 - Epoch: [138][  100/  113]    Overall Loss 1.306186    Objective Loss 1.306186                                        LR 0.000025    Time 0.065594    
2022-12-29 10:13:54,074 - Epoch: [138][  110/  113]    Overall Loss 1.312438    Objective Loss 1.312438                                        LR 0.000025    Time 0.065359    
2022-12-29 10:13:54,245 - Epoch: [138][  113/  113]    Overall Loss 1.313312    Objective Loss 1.313312    Top1 45.833333    Top5 95.833333    LR 0.000025    Time 0.065133    
2022-12-29 10:13:54,297 - --- validate (epoch=138)-----------
2022-12-29 10:13:54,297 - 200 samples (16 per mini-batch)
2022-12-29 10:13:54,861 - Epoch: [138][   10/   13]    Loss 1.253039    Top1 54.375000    Top5 98.125000    
2022-12-29 10:13:54,947 - Epoch: [138][   13/   13]    Loss 1.233978    Top1 53.500000    Top5 98.500000    
2022-12-29 10:13:55,013 - ==> Top1: 53.500    Top5: 98.500    Loss: 1.234

2022-12-29 10:13:55,013 - ==> Confusion:
[[15  2  0  4  3  0]
 [ 1 24  3 10  6  0]
 [ 1  4 16  5  9  0]
 [ 4  2  0 31  4  0]
 [ 5 11  1  8 19  0]
 [ 2  3  0  1  4  2]]

2022-12-29 10:13:55,015 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:13:55,015 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:13:55,053 - 

2022-12-29 10:13:55,054 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:13:55,888 - Epoch: [139][   10/  113]    Overall Loss 1.324449    Objective Loss 1.324449                                        LR 0.000025    Time 0.083203    
2022-12-29 10:13:56,523 - Epoch: [139][   20/  113]    Overall Loss 1.367863    Objective Loss 1.367863                                        LR 0.000025    Time 0.073358    
2022-12-29 10:13:57,160 - Epoch: [139][   30/  113]    Overall Loss 1.358649    Objective Loss 1.358649                                        LR 0.000025    Time 0.070104    
2022-12-29 10:13:57,791 - Epoch: [139][   40/  113]    Overall Loss 1.363842    Objective Loss 1.363842                                        LR 0.000025    Time 0.068356    
2022-12-29 10:13:58,424 - Epoch: [139][   50/  113]    Overall Loss 1.334496    Objective Loss 1.334496                                        LR 0.000025    Time 0.067320    
2022-12-29 10:13:59,061 - Epoch: [139][   60/  113]    Overall Loss 1.318351    Objective Loss 1.318351                                        LR 0.000025    Time 0.066706    
2022-12-29 10:13:59,699 - Epoch: [139][   70/  113]    Overall Loss 1.309413    Objective Loss 1.309413                                        LR 0.000025    Time 0.066289    
2022-12-29 10:14:00,335 - Epoch: [139][   80/  113]    Overall Loss 1.309080    Objective Loss 1.309080                                        LR 0.000025    Time 0.065946    
2022-12-29 10:14:00,970 - Epoch: [139][   90/  113]    Overall Loss 1.306926    Objective Loss 1.306926                                        LR 0.000025    Time 0.065667    
2022-12-29 10:14:01,608 - Epoch: [139][  100/  113]    Overall Loss 1.304616    Objective Loss 1.304616                                        LR 0.000025    Time 0.065483    
2022-12-29 10:14:02,238 - Epoch: [139][  110/  113]    Overall Loss 1.318944    Objective Loss 1.318944                                        LR 0.000025    Time 0.065251    
2022-12-29 10:14:02,410 - Epoch: [139][  113/  113]    Overall Loss 1.323533    Objective Loss 1.323533    Top1 29.166667    Top5 87.500000    LR 0.000025    Time 0.065038    
2022-12-29 10:14:02,456 - --- validate (epoch=139)-----------
2022-12-29 10:14:02,456 - 200 samples (16 per mini-batch)
2022-12-29 10:14:03,011 - Epoch: [139][   10/   13]    Loss 1.354255    Top1 48.125000    Top5 97.500000    
2022-12-29 10:14:03,096 - Epoch: [139][   13/   13]    Loss 1.349702    Top1 47.500000    Top5 98.000000    
2022-12-29 10:14:03,151 - ==> Top1: 47.500    Top5: 98.000    Loss: 1.350

2022-12-29 10:14:03,152 - ==> Confusion:
[[13  1  0  5  6  2]
 [ 1 24  4  4  7  0]
 [ 1  3  9  7  8  0]
 [ 4  4  0 31 10  1]
 [ 3  9  3 11 15  0]
 [ 1  4  2  2  2  3]]

2022-12-29 10:14:03,155 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:03,156 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:03,183 - 

2022-12-29 10:14:03,183 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:04,029 - Epoch: [140][   10/  113]    Overall Loss 1.296673    Objective Loss 1.296673                                        LR 0.000013    Time 0.084471    
2022-12-29 10:14:04,672 - Epoch: [140][   20/  113]    Overall Loss 1.359835    Objective Loss 1.359835                                        LR 0.000013    Time 0.074364    
2022-12-29 10:14:05,313 - Epoch: [140][   30/  113]    Overall Loss 1.346956    Objective Loss 1.346956                                        LR 0.000013    Time 0.070911    
2022-12-29 10:14:05,950 - Epoch: [140][   40/  113]    Overall Loss 1.332144    Objective Loss 1.332144                                        LR 0.000013    Time 0.069104    
2022-12-29 10:14:06,587 - Epoch: [140][   50/  113]    Overall Loss 1.324876    Objective Loss 1.324876                                        LR 0.000013    Time 0.068023    
2022-12-29 10:14:07,223 - Epoch: [140][   60/  113]    Overall Loss 1.317764    Objective Loss 1.317764                                        LR 0.000013    Time 0.067276    
2022-12-29 10:14:07,860 - Epoch: [140][   70/  113]    Overall Loss 1.306619    Objective Loss 1.306619                                        LR 0.000013    Time 0.066757    
2022-12-29 10:14:08,495 - Epoch: [140][   80/  113]    Overall Loss 1.309271    Objective Loss 1.309271                                        LR 0.000013    Time 0.066341    
2022-12-29 10:14:09,131 - Epoch: [140][   90/  113]    Overall Loss 1.310166    Objective Loss 1.310166                                        LR 0.000013    Time 0.066031    
2022-12-29 10:14:09,761 - Epoch: [140][  100/  113]    Overall Loss 1.310421    Objective Loss 1.310421                                        LR 0.000013    Time 0.065722    
2022-12-29 10:14:10,380 - Epoch: [140][  110/  113]    Overall Loss 1.303490    Objective Loss 1.303490                                        LR 0.000013    Time 0.065376    
2022-12-29 10:14:10,562 - Epoch: [140][  113/  113]    Overall Loss 1.304236    Objective Loss 1.304236    Top1 45.833333    Top5 100.000000    LR 0.000013    Time 0.065243    
2022-12-29 10:14:10,622 - --- validate (epoch=140)-----------
2022-12-29 10:14:10,622 - 200 samples (16 per mini-batch)
2022-12-29 10:14:11,169 - Epoch: [140][   10/   13]    Loss 1.283725    Top1 50.625000    Top5 97.500000    
2022-12-29 10:14:11,257 - Epoch: [140][   13/   13]    Loss 1.252174    Top1 51.000000    Top5 98.000000    
2022-12-29 10:14:11,315 - ==> Top1: 51.000    Top5: 98.000    Loss: 1.252

2022-12-29 10:14:11,315 - ==> Confusion:
[[20  1  0  4  0  0]
 [ 0 26  5 12  7  0]
 [ 0  6 11  6  5  0]
 [ 3  1  1 28  3  0]
 [ 3 10  8 14 15  0]
 [ 2  2  0  1  4  2]]

2022-12-29 10:14:11,319 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:11,320 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:11,345 - 

2022-12-29 10:14:11,346 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:12,193 - Epoch: [141][   10/  113]    Overall Loss 1.319480    Objective Loss 1.319480                                        LR 0.000013    Time 0.084505    
2022-12-29 10:14:12,837 - Epoch: [141][   20/  113]    Overall Loss 1.303612    Objective Loss 1.303612                                        LR 0.000013    Time 0.074457    
2022-12-29 10:14:13,476 - Epoch: [141][   30/  113]    Overall Loss 1.300076    Objective Loss 1.300076                                        LR 0.000013    Time 0.070912    
2022-12-29 10:14:14,119 - Epoch: [141][   40/  113]    Overall Loss 1.304453    Objective Loss 1.304453                                        LR 0.000013    Time 0.069244    
2022-12-29 10:14:14,751 - Epoch: [141][   50/  113]    Overall Loss 1.320737    Objective Loss 1.320737                                        LR 0.000013    Time 0.068040    
2022-12-29 10:14:15,391 - Epoch: [141][   60/  113]    Overall Loss 1.319268    Objective Loss 1.319268                                        LR 0.000013    Time 0.067353    
2022-12-29 10:14:16,023 - Epoch: [141][   70/  113]    Overall Loss 1.324273    Objective Loss 1.324273                                        LR 0.000013    Time 0.066759    
2022-12-29 10:14:16,657 - Epoch: [141][   80/  113]    Overall Loss 1.320024    Objective Loss 1.320024                                        LR 0.000013    Time 0.066323    
2022-12-29 10:14:17,296 - Epoch: [141][   90/  113]    Overall Loss 1.312009    Objective Loss 1.312009                                        LR 0.000013    Time 0.066054    
2022-12-29 10:14:17,929 - Epoch: [141][  100/  113]    Overall Loss 1.308534    Objective Loss 1.308534                                        LR 0.000013    Time 0.065773    
2022-12-29 10:14:18,557 - Epoch: [141][  110/  113]    Overall Loss 1.304647    Objective Loss 1.304647                                        LR 0.000013    Time 0.065498    
2022-12-29 10:14:18,732 - Epoch: [141][  113/  113]    Overall Loss 1.303339    Objective Loss 1.303339    Top1 45.833333    Top5 100.000000    LR 0.000013    Time 0.065299    
2022-12-29 10:14:18,790 - --- validate (epoch=141)-----------
2022-12-29 10:14:18,790 - 200 samples (16 per mini-batch)
2022-12-29 10:14:19,354 - Epoch: [141][   10/   13]    Loss 1.335980    Top1 47.500000    Top5 95.625000    
2022-12-29 10:14:19,442 - Epoch: [141][   13/   13]    Loss 1.293615    Top1 49.000000    Top5 96.000000    
2022-12-29 10:14:19,486 - ==> Top1: 49.000    Top5: 96.000    Loss: 1.294

2022-12-29 10:14:19,486 - ==> Confusion:
[[18  1  1  2  2  0]
 [ 2 20  1  7  8  0]
 [ 0  8 15  5  9  0]
 [ 4  7  1 31  9  0]
 [ 2 13  2  8 13  0]
 [ 2  4  0  2  2  1]]

2022-12-29 10:14:19,489 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:19,489 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:19,501 - 

2022-12-29 10:14:19,501 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:20,342 - Epoch: [142][   10/  113]    Overall Loss 1.315276    Objective Loss 1.315276                                        LR 0.000013    Time 0.084026    
2022-12-29 10:14:20,974 - Epoch: [142][   20/  113]    Overall Loss 1.268241    Objective Loss 1.268241                                        LR 0.000013    Time 0.073581    
2022-12-29 10:14:21,611 - Epoch: [142][   30/  113]    Overall Loss 1.311526    Objective Loss 1.311526                                        LR 0.000013    Time 0.070271    
2022-12-29 10:14:22,236 - Epoch: [142][   40/  113]    Overall Loss 1.301021    Objective Loss 1.301021                                        LR 0.000013    Time 0.068316    
2022-12-29 10:14:22,863 - Epoch: [142][   50/  113]    Overall Loss 1.316664    Objective Loss 1.316664                                        LR 0.000013    Time 0.067185    
2022-12-29 10:14:23,489 - Epoch: [142][   60/  113]    Overall Loss 1.313158    Objective Loss 1.313158                                        LR 0.000013    Time 0.066413    
2022-12-29 10:14:24,121 - Epoch: [142][   70/  113]    Overall Loss 1.316658    Objective Loss 1.316658                                        LR 0.000013    Time 0.065940    
2022-12-29 10:14:24,745 - Epoch: [142][   80/  113]    Overall Loss 1.315366    Objective Loss 1.315366                                        LR 0.000013    Time 0.065503    
2022-12-29 10:14:25,377 - Epoch: [142][   90/  113]    Overall Loss 1.328687    Objective Loss 1.328687                                        LR 0.000013    Time 0.065236    
2022-12-29 10:14:26,008 - Epoch: [142][  100/  113]    Overall Loss 1.337363    Objective Loss 1.337363                                        LR 0.000013    Time 0.065014    
2022-12-29 10:14:26,636 - Epoch: [142][  110/  113]    Overall Loss 1.336366    Objective Loss 1.336366                                        LR 0.000013    Time 0.064805    
2022-12-29 10:14:26,808 - Epoch: [142][  113/  113]    Overall Loss 1.339964    Objective Loss 1.339964    Top1 37.500000    Top5 95.833333    LR 0.000013    Time 0.064606    
2022-12-29 10:14:26,871 - --- validate (epoch=142)-----------
2022-12-29 10:14:26,871 - 200 samples (16 per mini-batch)
2022-12-29 10:14:27,428 - Epoch: [142][   10/   13]    Loss 1.283912    Top1 43.125000    Top5 98.125000    
2022-12-29 10:14:27,512 - Epoch: [142][   13/   13]    Loss 1.361285    Top1 41.500000    Top5 97.000000    
2022-12-29 10:14:27,560 - ==> Top1: 41.500    Top5: 97.000    Loss: 1.361

2022-12-29 10:14:27,560 - ==> Confusion:
[[19  5  1 10  4  0]
 [ 1 16  3  7  5  0]
 [ 0 10  7  3 11  1]
 [ 5 11  1 26  5  0]
 [ 2 15  4  6 13  0]
 [ 1  5  0  1  0  2]]

2022-12-29 10:14:27,564 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:27,564 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:27,583 - 

2022-12-29 10:14:27,583 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:28,417 - Epoch: [143][   10/  113]    Overall Loss 1.296176    Objective Loss 1.296176                                        LR 0.000013    Time 0.083252    
2022-12-29 10:14:29,055 - Epoch: [143][   20/  113]    Overall Loss 1.281971    Objective Loss 1.281971                                        LR 0.000013    Time 0.073489    
2022-12-29 10:14:29,694 - Epoch: [143][   30/  113]    Overall Loss 1.286535    Objective Loss 1.286535                                        LR 0.000013    Time 0.070280    
2022-12-29 10:14:30,328 - Epoch: [143][   40/  113]    Overall Loss 1.306716    Objective Loss 1.306716                                        LR 0.000013    Time 0.068557    
2022-12-29 10:14:30,961 - Epoch: [143][   50/  113]    Overall Loss 1.299086    Objective Loss 1.299086                                        LR 0.000013    Time 0.067499    
2022-12-29 10:14:31,599 - Epoch: [143][   60/  113]    Overall Loss 1.302942    Objective Loss 1.302942                                        LR 0.000013    Time 0.066861    
2022-12-29 10:14:32,244 - Epoch: [143][   70/  113]    Overall Loss 1.304990    Objective Loss 1.304990                                        LR 0.000013    Time 0.066526    
2022-12-29 10:14:32,883 - Epoch: [143][   80/  113]    Overall Loss 1.320507    Objective Loss 1.320507                                        LR 0.000013    Time 0.066190    
2022-12-29 10:14:33,524 - Epoch: [143][   90/  113]    Overall Loss 1.314874    Objective Loss 1.314874                                        LR 0.000013    Time 0.065955    
2022-12-29 10:14:34,161 - Epoch: [143][  100/  113]    Overall Loss 1.314040    Objective Loss 1.314040                                        LR 0.000013    Time 0.065720    
2022-12-29 10:14:34,791 - Epoch: [143][  110/  113]    Overall Loss 1.310994    Objective Loss 1.310994                                        LR 0.000013    Time 0.065469    
2022-12-29 10:14:34,966 - Epoch: [143][  113/  113]    Overall Loss 1.315617    Objective Loss 1.315617    Top1 37.500000    Top5 100.000000    LR 0.000013    Time 0.065272    
2022-12-29 10:14:35,020 - --- validate (epoch=143)-----------
2022-12-29 10:14:35,021 - 200 samples (16 per mini-batch)
2022-12-29 10:14:35,568 - Epoch: [143][   10/   13]    Loss 1.359444    Top1 45.000000    Top5 98.125000    
2022-12-29 10:14:35,653 - Epoch: [143][   13/   13]    Loss 1.382435    Top1 44.500000    Top5 97.500000    
2022-12-29 10:14:35,699 - ==> Top1: 44.500    Top5: 97.500    Loss: 1.382

2022-12-29 10:14:35,700 - ==> Confusion:
[[15  1  0  3  6  0]
 [ 7 18  3 10 11  0]
 [ 1  9  7  9  9  2]
 [ 3  4  1 25  9  1]
 [ 2  3  1  8 23  0]
 [ 2  4  0  2  0  1]]

2022-12-29 10:14:35,703 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:35,703 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:35,726 - 

2022-12-29 10:14:35,727 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:36,569 - Epoch: [144][   10/  113]    Overall Loss 1.262412    Objective Loss 1.262412                                        LR 0.000013    Time 0.084013    
2022-12-29 10:14:37,205 - Epoch: [144][   20/  113]    Overall Loss 1.289474    Objective Loss 1.289474                                        LR 0.000013    Time 0.073755    
2022-12-29 10:14:37,841 - Epoch: [144][   30/  113]    Overall Loss 1.305917    Objective Loss 1.305917                                        LR 0.000013    Time 0.070360    
2022-12-29 10:14:38,475 - Epoch: [144][   40/  113]    Overall Loss 1.325803    Objective Loss 1.325803                                        LR 0.000013    Time 0.068603    
2022-12-29 10:14:39,109 - Epoch: [144][   50/  113]    Overall Loss 1.315357    Objective Loss 1.315357                                        LR 0.000013    Time 0.067554    
2022-12-29 10:14:39,745 - Epoch: [144][   60/  113]    Overall Loss 1.299549    Objective Loss 1.299549                                        LR 0.000013    Time 0.066898    
2022-12-29 10:14:40,385 - Epoch: [144][   70/  113]    Overall Loss 1.306725    Objective Loss 1.306725                                        LR 0.000013    Time 0.066466    
2022-12-29 10:14:41,019 - Epoch: [144][   80/  113]    Overall Loss 1.311506    Objective Loss 1.311506                                        LR 0.000013    Time 0.066084    
2022-12-29 10:14:41,659 - Epoch: [144][   90/  113]    Overall Loss 1.313180    Objective Loss 1.313180                                        LR 0.000013    Time 0.065842    
2022-12-29 10:14:42,289 - Epoch: [144][  100/  113]    Overall Loss 1.313717    Objective Loss 1.313717                                        LR 0.000013    Time 0.065556    
2022-12-29 10:14:42,911 - Epoch: [144][  110/  113]    Overall Loss 1.313369    Objective Loss 1.313369                                        LR 0.000013    Time 0.065251    
2022-12-29 10:14:43,089 - Epoch: [144][  113/  113]    Overall Loss 1.312750    Objective Loss 1.312750    Top1 58.333333    Top5 100.000000    LR 0.000013    Time 0.065087    
2022-12-29 10:14:43,152 - --- validate (epoch=144)-----------
2022-12-29 10:14:43,153 - 200 samples (16 per mini-batch)
2022-12-29 10:14:43,708 - Epoch: [144][   10/   13]    Loss 1.343682    Top1 49.375000    Top5 96.875000    
2022-12-29 10:14:43,792 - Epoch: [144][   13/   13]    Loss 1.326159    Top1 50.500000    Top5 96.500000    
2022-12-29 10:14:43,835 - ==> Top1: 50.500    Top5: 96.500    Loss: 1.326

2022-12-29 10:14:43,836 - ==> Confusion:
[[19  4  1  5  1  0]
 [ 3 19  1 12  9  0]
 [ 1  6 11  7  6  0]
 [ 4  3  1 35  1  0]
 [ 3  7  3  8 13  0]
 [ 1  2  1  5  4  4]]

2022-12-29 10:14:43,838 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:43,838 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:43,848 - 

2022-12-29 10:14:43,848 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:44,672 - Epoch: [145][   10/  113]    Overall Loss 1.339272    Objective Loss 1.339272                                        LR 0.000013    Time 0.082298    
2022-12-29 10:14:45,313 - Epoch: [145][   20/  113]    Overall Loss 1.313733    Objective Loss 1.313733                                        LR 0.000013    Time 0.073189    
2022-12-29 10:14:45,952 - Epoch: [145][   30/  113]    Overall Loss 1.327240    Objective Loss 1.327240                                        LR 0.000013    Time 0.070066    
2022-12-29 10:14:46,590 - Epoch: [145][   40/  113]    Overall Loss 1.329002    Objective Loss 1.329002                                        LR 0.000013    Time 0.068475    
2022-12-29 10:14:47,221 - Epoch: [145][   50/  113]    Overall Loss 1.327607    Objective Loss 1.327607                                        LR 0.000013    Time 0.067395    
2022-12-29 10:14:47,851 - Epoch: [145][   60/  113]    Overall Loss 1.334103    Objective Loss 1.334103                                        LR 0.000013    Time 0.066654    
2022-12-29 10:14:48,482 - Epoch: [145][   70/  113]    Overall Loss 1.327656    Objective Loss 1.327656                                        LR 0.000013    Time 0.066143    
2022-12-29 10:14:49,107 - Epoch: [145][   80/  113]    Overall Loss 1.313920    Objective Loss 1.313920                                        LR 0.000013    Time 0.065680    
2022-12-29 10:14:49,742 - Epoch: [145][   90/  113]    Overall Loss 1.313175    Objective Loss 1.313175                                        LR 0.000013    Time 0.065436    
2022-12-29 10:14:50,369 - Epoch: [145][  100/  113]    Overall Loss 1.316849    Objective Loss 1.316849                                        LR 0.000013    Time 0.065154    
2022-12-29 10:14:50,995 - Epoch: [145][  110/  113]    Overall Loss 1.323545    Objective Loss 1.323545                                        LR 0.000013    Time 0.064922    
2022-12-29 10:14:51,167 - Epoch: [145][  113/  113]    Overall Loss 1.320783    Objective Loss 1.320783    Top1 58.333333    Top5 100.000000    LR 0.000013    Time 0.064718    
2022-12-29 10:14:51,221 - --- validate (epoch=145)-----------
2022-12-29 10:14:51,222 - 200 samples (16 per mini-batch)
2022-12-29 10:14:51,780 - Epoch: [145][   10/   13]    Loss 1.168253    Top1 53.125000    Top5 98.125000    
2022-12-29 10:14:51,866 - Epoch: [145][   13/   13]    Loss 1.230474    Top1 51.000000    Top5 98.500000    
2022-12-29 10:14:51,923 - ==> Top1: 51.000    Top5: 98.500    Loss: 1.230

2022-12-29 10:14:51,924 - ==> Confusion:
[[27  0  1  6  1  0]
 [ 2 16  4  7  6  0]
 [ 0 11 13  1  7  0]
 [ 5  3  2 30 10  0]
 [ 4 11  2  4 15  0]
 [ 3  3  0  2  3  1]]

2022-12-29 10:14:51,926 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:14:51,926 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:14:51,949 - 

2022-12-29 10:14:51,949 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:14:52,794 - Epoch: [146][   10/  113]    Overall Loss 1.276481    Objective Loss 1.276481                                        LR 0.000013    Time 0.084418    
2022-12-29 10:14:53,437 - Epoch: [146][   20/  113]    Overall Loss 1.236585    Objective Loss 1.236585                                        LR 0.000013    Time 0.074347    
2022-12-29 10:14:54,071 - Epoch: [146][   30/  113]    Overall Loss 1.257391    Objective Loss 1.257391                                        LR 0.000013    Time 0.070675    
2022-12-29 10:14:54,708 - Epoch: [146][   40/  113]    Overall Loss 1.260676    Objective Loss 1.260676                                        LR 0.000013    Time 0.068909    
2022-12-29 10:14:55,344 - Epoch: [146][   50/  113]    Overall Loss 1.266381    Objective Loss 1.266381                                        LR 0.000013    Time 0.067840    
2022-12-29 10:14:55,981 - Epoch: [146][   60/  113]    Overall Loss 1.274455    Objective Loss 1.274455                                        LR 0.000013    Time 0.067141    
2022-12-29 10:14:56,618 - Epoch: [146][   70/  113]    Overall Loss 1.274074    Objective Loss 1.274074                                        LR 0.000013    Time 0.066641    
2022-12-29 10:14:57,251 - Epoch: [146][   80/  113]    Overall Loss 1.261068    Objective Loss 1.261068                                        LR 0.000013    Time 0.066224    
2022-12-29 10:14:57,892 - Epoch: [146][   90/  113]    Overall Loss 1.268493    Objective Loss 1.268493                                        LR 0.000013    Time 0.065975    
2022-12-29 10:14:58,528 - Epoch: [146][  100/  113]    Overall Loss 1.273878    Objective Loss 1.273878                                        LR 0.000013    Time 0.065742    
2022-12-29 10:14:59,161 - Epoch: [146][  110/  113]    Overall Loss 1.272928    Objective Loss 1.272928                                        LR 0.000013    Time 0.065513    
2022-12-29 10:14:59,336 - Epoch: [146][  113/  113]    Overall Loss 1.274124    Objective Loss 1.274124    Top1 37.500000    Top5 100.000000    LR 0.000013    Time 0.065317    
2022-12-29 10:14:59,396 - --- validate (epoch=146)-----------
2022-12-29 10:14:59,396 - 200 samples (16 per mini-batch)
2022-12-29 10:14:59,948 - Epoch: [146][   10/   13]    Loss 1.293746    Top1 46.875000    Top5 96.875000    
2022-12-29 10:15:00,035 - Epoch: [146][   13/   13]    Loss 1.316568    Top1 45.000000    Top5 97.000000    
2022-12-29 10:15:00,086 - ==> Top1: 45.000    Top5: 97.000    Loss: 1.317

2022-12-29 10:15:00,087 - ==> Confusion:
[[17  0  0  7  3  0]
 [ 3 17  2 11  8  0]
 [ 0  8  5  8  8  0]
 [ 6  7  3 39 10  0]
 [ 0  6  3 10 12  0]
 [ 1  2  0  3  1  0]]

2022-12-29 10:15:00,089 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:00,090 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:00,105 - 

2022-12-29 10:15:00,105 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:00,946 - Epoch: [147][   10/  113]    Overall Loss 1.364919    Objective Loss 1.364919                                        LR 0.000013    Time 0.084031    
2022-12-29 10:15:01,586 - Epoch: [147][   20/  113]    Overall Loss 1.338302    Objective Loss 1.338302                                        LR 0.000013    Time 0.073969    
2022-12-29 10:15:02,229 - Epoch: [147][   30/  113]    Overall Loss 1.349114    Objective Loss 1.349114                                        LR 0.000013    Time 0.070742    
2022-12-29 10:15:02,869 - Epoch: [147][   40/  113]    Overall Loss 1.316342    Objective Loss 1.316342                                        LR 0.000013    Time 0.069030    
2022-12-29 10:15:03,511 - Epoch: [147][   50/  113]    Overall Loss 1.285303    Objective Loss 1.285303                                        LR 0.000013    Time 0.068066    
2022-12-29 10:15:04,147 - Epoch: [147][   60/  113]    Overall Loss 1.280752    Objective Loss 1.280752                                        LR 0.000013    Time 0.067314    
2022-12-29 10:15:04,782 - Epoch: [147][   70/  113]    Overall Loss 1.276449    Objective Loss 1.276449                                        LR 0.000013    Time 0.066761    
2022-12-29 10:15:05,415 - Epoch: [147][   80/  113]    Overall Loss 1.268355    Objective Loss 1.268355                                        LR 0.000013    Time 0.066317    
2022-12-29 10:15:06,047 - Epoch: [147][   90/  113]    Overall Loss 1.270975    Objective Loss 1.270975                                        LR 0.000013    Time 0.065965    
2022-12-29 10:15:06,678 - Epoch: [147][  100/  113]    Overall Loss 1.274037    Objective Loss 1.274037                                        LR 0.000013    Time 0.065679    
2022-12-29 10:15:07,310 - Epoch: [147][  110/  113]    Overall Loss 1.277409    Objective Loss 1.277409                                        LR 0.000013    Time 0.065445    
2022-12-29 10:15:07,480 - Epoch: [147][  113/  113]    Overall Loss 1.275197    Objective Loss 1.275197    Top1 45.833333    Top5 95.833333    LR 0.000013    Time 0.065216    
2022-12-29 10:15:07,539 - --- validate (epoch=147)-----------
2022-12-29 10:15:07,539 - 200 samples (16 per mini-batch)
2022-12-29 10:15:08,093 - Epoch: [147][   10/   13]    Loss 1.286294    Top1 52.500000    Top5 95.625000    
2022-12-29 10:15:08,178 - Epoch: [147][   13/   13]    Loss 1.275582    Top1 52.000000    Top5 96.500000    
2022-12-29 10:15:08,237 - ==> Top1: 52.000    Top5: 96.500    Loss: 1.276

2022-12-29 10:15:08,238 - ==> Confusion:
[[29  4  1  8  1  1]
 [ 1 17  2 13  4  1]
 [ 1  9  6  2  8  0]
 [ 3  3  1 33  5  1]
 [ 2  8  3  6 17  0]
 [ 1  1  1  2  3  2]]

2022-12-29 10:15:08,242 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:08,242 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:08,266 - 

2022-12-29 10:15:08,266 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:09,104 - Epoch: [148][   10/  113]    Overall Loss 1.216469    Objective Loss 1.216469                                        LR 0.000013    Time 0.083693    
2022-12-29 10:15:09,744 - Epoch: [148][   20/  113]    Overall Loss 1.223715    Objective Loss 1.223715                                        LR 0.000013    Time 0.073793    
2022-12-29 10:15:10,383 - Epoch: [148][   30/  113]    Overall Loss 1.241483    Objective Loss 1.241483                                        LR 0.000013    Time 0.070503    
2022-12-29 10:15:11,016 - Epoch: [148][   40/  113]    Overall Loss 1.239662    Objective Loss 1.239662                                        LR 0.000013    Time 0.068698    
2022-12-29 10:15:11,657 - Epoch: [148][   50/  113]    Overall Loss 1.236052    Objective Loss 1.236052                                        LR 0.000013    Time 0.067762    
2022-12-29 10:15:12,293 - Epoch: [148][   60/  113]    Overall Loss 1.250534    Objective Loss 1.250534                                        LR 0.000013    Time 0.067058    
2022-12-29 10:15:12,928 - Epoch: [148][   70/  113]    Overall Loss 1.270449    Objective Loss 1.270449                                        LR 0.000013    Time 0.066536    
2022-12-29 10:15:13,565 - Epoch: [148][   80/  113]    Overall Loss 1.275301    Objective Loss 1.275301                                        LR 0.000013    Time 0.066179    
2022-12-29 10:15:14,204 - Epoch: [148][   90/  113]    Overall Loss 1.266654    Objective Loss 1.266654                                        LR 0.000013    Time 0.065926    
2022-12-29 10:15:14,839 - Epoch: [148][  100/  113]    Overall Loss 1.270108    Objective Loss 1.270108                                        LR 0.000013    Time 0.065679    
2022-12-29 10:15:15,470 - Epoch: [148][  110/  113]    Overall Loss 1.283127    Objective Loss 1.283127                                        LR 0.000013    Time 0.065437    
2022-12-29 10:15:15,644 - Epoch: [148][  113/  113]    Overall Loss 1.289206    Objective Loss 1.289206    Top1 45.833333    Top5 91.666667    LR 0.000013    Time 0.065232    
2022-12-29 10:15:15,694 - --- validate (epoch=148)-----------
2022-12-29 10:15:15,695 - 200 samples (16 per mini-batch)
2022-12-29 10:15:16,239 - Epoch: [148][   10/   13]    Loss 1.336048    Top1 48.125000    Top5 97.500000    
2022-12-29 10:15:16,324 - Epoch: [148][   13/   13]    Loss 1.314248    Top1 49.500000    Top5 97.000000    
2022-12-29 10:15:16,382 - ==> Top1: 49.500    Top5: 97.000    Loss: 1.314

2022-12-29 10:15:16,383 - ==> Confusion:
[[25  4  0  8  2  0]
 [ 1 22  1  5  9  0]
 [ 2  6 10  5  9  0]
 [ 5  3  1 29  0  0]
 [ 2 10  2 11 13  0]
 [ 1  5  3  3  3  0]]

2022-12-29 10:15:16,385 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:16,385 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:16,406 - 

2022-12-29 10:15:16,407 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:17,259 - Epoch: [149][   10/  113]    Overall Loss 1.379619    Objective Loss 1.379619                                        LR 0.000013    Time 0.085019    
2022-12-29 10:15:17,901 - Epoch: [149][   20/  113]    Overall Loss 1.359088    Objective Loss 1.359088                                        LR 0.000013    Time 0.074571    
2022-12-29 10:15:18,540 - Epoch: [149][   30/  113]    Overall Loss 1.306695    Objective Loss 1.306695                                        LR 0.000013    Time 0.071012    
2022-12-29 10:15:19,177 - Epoch: [149][   40/  113]    Overall Loss 1.267106    Objective Loss 1.267106                                        LR 0.000013    Time 0.069166    
2022-12-29 10:15:19,816 - Epoch: [149][   50/  113]    Overall Loss 1.276338    Objective Loss 1.276338                                        LR 0.000013    Time 0.068113    
2022-12-29 10:15:20,456 - Epoch: [149][   60/  113]    Overall Loss 1.277495    Objective Loss 1.277495                                        LR 0.000013    Time 0.067415    
2022-12-29 10:15:21,099 - Epoch: [149][   70/  113]    Overall Loss 1.272356    Objective Loss 1.272356                                        LR 0.000013    Time 0.066954    
2022-12-29 10:15:21,741 - Epoch: [149][   80/  113]    Overall Loss 1.278158    Objective Loss 1.278158                                        LR 0.000013    Time 0.066604    
2022-12-29 10:15:22,384 - Epoch: [149][   90/  113]    Overall Loss 1.270865    Objective Loss 1.270865                                        LR 0.000013    Time 0.066345    
2022-12-29 10:15:23,024 - Epoch: [149][  100/  113]    Overall Loss 1.274700    Objective Loss 1.274700                                        LR 0.000013    Time 0.066109    
2022-12-29 10:15:23,658 - Epoch: [149][  110/  113]    Overall Loss 1.278545    Objective Loss 1.278545                                        LR 0.000013    Time 0.065859    
2022-12-29 10:15:23,832 - Epoch: [149][  113/  113]    Overall Loss 1.273167    Objective Loss 1.273167    Top1 41.666667    Top5 100.000000    LR 0.000013    Time 0.065643    
2022-12-29 10:15:23,880 - --- validate (epoch=149)-----------
2022-12-29 10:15:23,881 - 200 samples (16 per mini-batch)
2022-12-29 10:15:24,426 - Epoch: [149][   10/   13]    Loss 1.307332    Top1 50.000000    Top5 98.125000    
2022-12-29 10:15:24,512 - Epoch: [149][   13/   13]    Loss 1.263777    Top1 51.000000    Top5 98.000000    
2022-12-29 10:15:24,565 - ==> Top1: 51.000    Top5: 98.000    Loss: 1.264

2022-12-29 10:15:24,566 - ==> Confusion:
[[24  0  0  5  2  0]
 [ 4 18  2 10  6  0]
 [ 1  5  8  5  6  0]
 [ 5  4  1 32  3  2]
 [ 3 10  2 10 19  0]
 [ 0  0  0  5  7  1]]

2022-12-29 10:15:24,568 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:24,568 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:24,582 - 

2022-12-29 10:15:24,582 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:25,433 - Epoch: [150][   10/  113]    Overall Loss 1.295505    Objective Loss 1.295505                                        LR 0.000013    Time 0.084969    
2022-12-29 10:15:26,077 - Epoch: [150][   20/  113]    Overall Loss 1.310925    Objective Loss 1.310925                                        LR 0.000013    Time 0.074652    
2022-12-29 10:15:26,718 - Epoch: [150][   30/  113]    Overall Loss 1.302947    Objective Loss 1.302947                                        LR 0.000013    Time 0.071113    
2022-12-29 10:15:27,355 - Epoch: [150][   40/  113]    Overall Loss 1.301552    Objective Loss 1.301552                                        LR 0.000013    Time 0.069260    
2022-12-29 10:15:27,994 - Epoch: [150][   50/  113]    Overall Loss 1.298766    Objective Loss 1.298766                                        LR 0.000013    Time 0.068179    
2022-12-29 10:15:28,636 - Epoch: [150][   60/  113]    Overall Loss 1.311136    Objective Loss 1.311136                                        LR 0.000013    Time 0.067505    
2022-12-29 10:15:29,279 - Epoch: [150][   70/  113]    Overall Loss 1.316381    Objective Loss 1.316381                                        LR 0.000013    Time 0.067029    
2022-12-29 10:15:29,917 - Epoch: [150][   80/  113]    Overall Loss 1.298926    Objective Loss 1.298926                                        LR 0.000013    Time 0.066622    
2022-12-29 10:15:30,554 - Epoch: [150][   90/  113]    Overall Loss 1.308326    Objective Loss 1.308326                                        LR 0.000013    Time 0.066297    
2022-12-29 10:15:31,191 - Epoch: [150][  100/  113]    Overall Loss 1.308481    Objective Loss 1.308481                                        LR 0.000013    Time 0.066031    
2022-12-29 10:15:31,827 - Epoch: [150][  110/  113]    Overall Loss 1.302136    Objective Loss 1.302136                                        LR 0.000013    Time 0.065808    
2022-12-29 10:15:32,002 - Epoch: [150][  113/  113]    Overall Loss 1.297187    Objective Loss 1.297187    Top1 66.666667    Top5 100.000000    LR 0.000013    Time 0.065607    
2022-12-29 10:15:32,063 - --- validate (epoch=150)-----------
2022-12-29 10:15:32,063 - 200 samples (16 per mini-batch)
2022-12-29 10:15:32,617 - Epoch: [150][   10/   13]    Loss 1.358152    Top1 46.875000    Top5 100.000000    
2022-12-29 10:15:32,701 - Epoch: [150][   13/   13]    Loss 1.356180    Top1 44.000000    Top5 100.000000    
2022-12-29 10:15:32,752 - ==> Top1: 44.000    Top5: 100.000    Loss: 1.356

2022-12-29 10:15:32,752 - ==> Confusion:
[[24  4  1  7  0  0]
 [ 3 17  0 13  8  0]
 [ 3  4  6 10  8  1]
 [ 6  6  3 30  4  2]
 [ 0  7  5  6 10  1]
 [ 4  2  2  1  1  1]]

2022-12-29 10:15:32,755 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:32,755 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:32,777 - 

2022-12-29 10:15:32,777 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:33,605 - Epoch: [151][   10/  113]    Overall Loss 1.297011    Objective Loss 1.297011                                        LR 0.000013    Time 0.082689    
2022-12-29 10:15:34,243 - Epoch: [151][   20/  113]    Overall Loss 1.256570    Objective Loss 1.256570                                        LR 0.000013    Time 0.073205    
2022-12-29 10:15:34,879 - Epoch: [151][   30/  113]    Overall Loss 1.239039    Objective Loss 1.239039                                        LR 0.000013    Time 0.070004    
2022-12-29 10:15:35,516 - Epoch: [151][   40/  113]    Overall Loss 1.251872    Objective Loss 1.251872                                        LR 0.000013    Time 0.068413    
2022-12-29 10:15:36,152 - Epoch: [151][   50/  113]    Overall Loss 1.269781    Objective Loss 1.269781                                        LR 0.000013    Time 0.067447    
2022-12-29 10:15:36,779 - Epoch: [151][   60/  113]    Overall Loss 1.254427    Objective Loss 1.254427                                        LR 0.000013    Time 0.066645    
2022-12-29 10:15:37,409 - Epoch: [151][   70/  113]    Overall Loss 1.259796    Objective Loss 1.259796                                        LR 0.000013    Time 0.066109    
2022-12-29 10:15:38,044 - Epoch: [151][   80/  113]    Overall Loss 1.253453    Objective Loss 1.253453                                        LR 0.000013    Time 0.065775    
2022-12-29 10:15:38,684 - Epoch: [151][   90/  113]    Overall Loss 1.257873    Objective Loss 1.257873                                        LR 0.000013    Time 0.065580    
2022-12-29 10:15:39,318 - Epoch: [151][  100/  113]    Overall Loss 1.260126    Objective Loss 1.260126                                        LR 0.000013    Time 0.065351    
2022-12-29 10:15:39,951 - Epoch: [151][  110/  113]    Overall Loss 1.268928    Objective Loss 1.268928                                        LR 0.000013    Time 0.065163    
2022-12-29 10:15:40,121 - Epoch: [151][  113/  113]    Overall Loss 1.263255    Objective Loss 1.263255    Top1 75.000000    Top5 100.000000    LR 0.000013    Time 0.064934    
2022-12-29 10:15:40,182 - --- validate (epoch=151)-----------
2022-12-29 10:15:40,183 - 200 samples (16 per mini-batch)
2022-12-29 10:15:40,727 - Epoch: [151][   10/   13]    Loss 1.374711    Top1 42.500000    Top5 96.875000    
2022-12-29 10:15:40,816 - Epoch: [151][   13/   13]    Loss 1.385207    Top1 41.000000    Top5 96.000000    
2022-12-29 10:15:40,877 - ==> Top1: 41.000    Top5: 96.000    Loss: 1.385

2022-12-29 10:15:40,878 - ==> Confusion:
[[17  6  1  5  8  0]
 [ 4 19  2  3 13  1]
 [ 0  6 11  1 13  0]
 [ 4  4  0 23  6  0]
 [ 4  9  4  6 11  0]
 [ 4  4  3  2  5  1]]

2022-12-29 10:15:40,882 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:40,882 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:40,905 - 

2022-12-29 10:15:40,906 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:41,747 - Epoch: [152][   10/  113]    Overall Loss 1.368883    Objective Loss 1.368883                                        LR 0.000013    Time 0.084071    
2022-12-29 10:15:42,393 - Epoch: [152][   20/  113]    Overall Loss 1.282872    Objective Loss 1.282872                                        LR 0.000013    Time 0.074314    
2022-12-29 10:15:43,036 - Epoch: [152][   30/  113]    Overall Loss 1.291355    Objective Loss 1.291355                                        LR 0.000013    Time 0.070954    
2022-12-29 10:15:43,674 - Epoch: [152][   40/  113]    Overall Loss 1.322447    Objective Loss 1.322447                                        LR 0.000013    Time 0.069149    
2022-12-29 10:15:44,313 - Epoch: [152][   50/  113]    Overall Loss 1.342021    Objective Loss 1.342021                                        LR 0.000013    Time 0.068086    
2022-12-29 10:15:44,952 - Epoch: [152][   60/  113]    Overall Loss 1.329458    Objective Loss 1.329458                                        LR 0.000013    Time 0.067392    
2022-12-29 10:15:45,586 - Epoch: [152][   70/  113]    Overall Loss 1.315971    Objective Loss 1.315971                                        LR 0.000013    Time 0.066811    
2022-12-29 10:15:46,223 - Epoch: [152][   80/  113]    Overall Loss 1.303026    Objective Loss 1.303026                                        LR 0.000013    Time 0.066410    
2022-12-29 10:15:46,858 - Epoch: [152][   90/  113]    Overall Loss 1.302789    Objective Loss 1.302789                                        LR 0.000013    Time 0.066089    
2022-12-29 10:15:47,493 - Epoch: [152][  100/  113]    Overall Loss 1.297340    Objective Loss 1.297340                                        LR 0.000013    Time 0.065824    
2022-12-29 10:15:48,126 - Epoch: [152][  110/  113]    Overall Loss 1.303383    Objective Loss 1.303383                                        LR 0.000013    Time 0.065584    
2022-12-29 10:15:48,299 - Epoch: [152][  113/  113]    Overall Loss 1.304481    Objective Loss 1.304481    Top1 54.166667    Top5 95.833333    LR 0.000013    Time 0.065373    
2022-12-29 10:15:48,352 - --- validate (epoch=152)-----------
2022-12-29 10:15:48,353 - 200 samples (16 per mini-batch)
2022-12-29 10:15:48,896 - Epoch: [152][   10/   13]    Loss 1.310561    Top1 51.250000    Top5 98.125000    
2022-12-29 10:15:48,979 - Epoch: [152][   13/   13]    Loss 1.334732    Top1 50.000000    Top5 98.000000    
2022-12-29 10:15:49,029 - ==> Top1: 50.000    Top5: 98.000    Loss: 1.335

2022-12-29 10:15:49,030 - ==> Confusion:
[[21  3  0  4  2  0]
 [ 1 17  4 15  6  0]
 [ 2  6  9 11  9  0]
 [ 6  3  1 30  4  0]
 [ 2  5  0 11 20  0]
 [ 1  1  0  0  3  3]]

2022-12-29 10:15:49,032 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:49,033 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:49,056 - 

2022-12-29 10:15:49,056 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:49,894 - Epoch: [153][   10/  113]    Overall Loss 1.392499    Objective Loss 1.392499                                        LR 0.000013    Time 0.083606    
2022-12-29 10:15:50,533 - Epoch: [153][   20/  113]    Overall Loss 1.324598    Objective Loss 1.324598                                        LR 0.000013    Time 0.073734    
2022-12-29 10:15:51,170 - Epoch: [153][   30/  113]    Overall Loss 1.311140    Objective Loss 1.311140                                        LR 0.000013    Time 0.070388    
2022-12-29 10:15:51,813 - Epoch: [153][   40/  113]    Overall Loss 1.304593    Objective Loss 1.304593                                        LR 0.000013    Time 0.068842    
2022-12-29 10:15:52,454 - Epoch: [153][   50/  113]    Overall Loss 1.302116    Objective Loss 1.302116                                        LR 0.000013    Time 0.067885    
2022-12-29 10:15:53,091 - Epoch: [153][   60/  113]    Overall Loss 1.295811    Objective Loss 1.295811                                        LR 0.000013    Time 0.067184    
2022-12-29 10:15:53,730 - Epoch: [153][   70/  113]    Overall Loss 1.298923    Objective Loss 1.298923                                        LR 0.000013    Time 0.066705    
2022-12-29 10:15:54,369 - Epoch: [153][   80/  113]    Overall Loss 1.292583    Objective Loss 1.292583                                        LR 0.000013    Time 0.066352    
2022-12-29 10:15:55,011 - Epoch: [153][   90/  113]    Overall Loss 1.282881    Objective Loss 1.282881                                        LR 0.000013    Time 0.066111    
2022-12-29 10:15:55,648 - Epoch: [153][  100/  113]    Overall Loss 1.287440    Objective Loss 1.287440                                        LR 0.000013    Time 0.065857    
2022-12-29 10:15:56,274 - Epoch: [153][  110/  113]    Overall Loss 1.293849    Objective Loss 1.293849                                        LR 0.000013    Time 0.065560    
2022-12-29 10:15:56,448 - Epoch: [153][  113/  113]    Overall Loss 1.296918    Objective Loss 1.296918    Top1 54.166667    Top5 100.000000    LR 0.000013    Time 0.065353    
2022-12-29 10:15:56,512 - --- validate (epoch=153)-----------
2022-12-29 10:15:56,513 - 200 samples (16 per mini-batch)
2022-12-29 10:15:57,074 - Epoch: [153][   10/   13]    Loss 1.349792    Top1 47.500000    Top5 96.250000    
2022-12-29 10:15:57,159 - Epoch: [153][   13/   13]    Loss 1.329373    Top1 49.000000    Top5 97.000000    
2022-12-29 10:15:57,208 - ==> Top1: 49.000    Top5: 97.000    Loss: 1.329

2022-12-29 10:15:57,208 - ==> Confusion:
[[12  0  2  6  1  0]
 [ 1 28  3  0 10  1]
 [ 0 11  8  8  5  0]
 [ 5 11  2 28  7  1]
 [ 0 16  0  5 20  0]
 [ 2  4  0  0  1  2]]

2022-12-29 10:15:57,211 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:15:57,211 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:15:57,224 - 

2022-12-29 10:15:57,224 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:15:58,056 - Epoch: [154][   10/  113]    Overall Loss 1.304119    Objective Loss 1.304119                                        LR 0.000013    Time 0.083077    
2022-12-29 10:15:58,693 - Epoch: [154][   20/  113]    Overall Loss 1.288627    Objective Loss 1.288627                                        LR 0.000013    Time 0.073366    
2022-12-29 10:15:59,332 - Epoch: [154][   30/  113]    Overall Loss 1.289280    Objective Loss 1.289280                                        LR 0.000013    Time 0.070177    
2022-12-29 10:15:59,971 - Epoch: [154][   40/  113]    Overall Loss 1.285277    Objective Loss 1.285277                                        LR 0.000013    Time 0.068586    
2022-12-29 10:16:00,607 - Epoch: [154][   50/  113]    Overall Loss 1.268182    Objective Loss 1.268182                                        LR 0.000013    Time 0.067585    
2022-12-29 10:16:01,244 - Epoch: [154][   60/  113]    Overall Loss 1.259862    Objective Loss 1.259862                                        LR 0.000013    Time 0.066933    
2022-12-29 10:16:01,886 - Epoch: [154][   70/  113]    Overall Loss 1.260936    Objective Loss 1.260936                                        LR 0.000013    Time 0.066536    
2022-12-29 10:16:02,521 - Epoch: [154][   80/  113]    Overall Loss 1.260919    Objective Loss 1.260919                                        LR 0.000013    Time 0.066145    
2022-12-29 10:16:03,162 - Epoch: [154][   90/  113]    Overall Loss 1.272990    Objective Loss 1.272990                                        LR 0.000013    Time 0.065921    
2022-12-29 10:16:03,797 - Epoch: [154][  100/  113]    Overall Loss 1.271334    Objective Loss 1.271334                                        LR 0.000013    Time 0.065673    
2022-12-29 10:16:04,430 - Epoch: [154][  110/  113]    Overall Loss 1.264770    Objective Loss 1.264770                                        LR 0.000013    Time 0.065445    
2022-12-29 10:16:04,602 - Epoch: [154][  113/  113]    Overall Loss 1.263220    Objective Loss 1.263220    Top1 58.333333    Top5 100.000000    LR 0.000013    Time 0.065236    
2022-12-29 10:16:04,662 - --- validate (epoch=154)-----------
2022-12-29 10:16:04,663 - 200 samples (16 per mini-batch)
2022-12-29 10:16:05,224 - Epoch: [154][   10/   13]    Loss 1.381520    Top1 49.375000    Top5 95.000000    
2022-12-29 10:16:05,310 - Epoch: [154][   13/   13]    Loss 1.402545    Top1 47.500000    Top5 96.000000    
2022-12-29 10:16:05,375 - ==> Top1: 47.500    Top5: 96.000    Loss: 1.403

2022-12-29 10:16:05,376 - ==> Confusion:
[[19  4  2  5  2  1]
 [ 1 26  4  4  5  0]
 [ 0 12 10 10  4  1]
 [ 6  7  0 28  3  0]
 [ 2 13  0  8 11  1]
 [ 1  4  2  2  1  1]]

2022-12-29 10:16:05,378 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:05,378 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:05,401 - 

2022-12-29 10:16:05,402 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:06,235 - Epoch: [155][   10/  113]    Overall Loss 1.227387    Objective Loss 1.227387                                        LR 0.000013    Time 0.083195    
2022-12-29 10:16:06,872 - Epoch: [155][   20/  113]    Overall Loss 1.273400    Objective Loss 1.273400                                        LR 0.000013    Time 0.073393    
2022-12-29 10:16:07,511 - Epoch: [155][   30/  113]    Overall Loss 1.279321    Objective Loss 1.279321                                        LR 0.000013    Time 0.070241    
2022-12-29 10:16:08,155 - Epoch: [155][   40/  113]    Overall Loss 1.281537    Objective Loss 1.281537                                        LR 0.000013    Time 0.068745    
2022-12-29 10:16:08,788 - Epoch: [155][   50/  113]    Overall Loss 1.287830    Objective Loss 1.287830                                        LR 0.000013    Time 0.067653    
2022-12-29 10:16:09,422 - Epoch: [155][   60/  113]    Overall Loss 1.288668    Objective Loss 1.288668                                        LR 0.000013    Time 0.066940    
2022-12-29 10:16:10,057 - Epoch: [155][   70/  113]    Overall Loss 1.280109    Objective Loss 1.280109                                        LR 0.000013    Time 0.066440    
2022-12-29 10:16:10,684 - Epoch: [155][   80/  113]    Overall Loss 1.271233    Objective Loss 1.271233                                        LR 0.000013    Time 0.065973    
2022-12-29 10:16:11,314 - Epoch: [155][   90/  113]    Overall Loss 1.279564    Objective Loss 1.279564                                        LR 0.000013    Time 0.065629    
2022-12-29 10:16:11,945 - Epoch: [155][  100/  113]    Overall Loss 1.294842    Objective Loss 1.294842                                        LR 0.000013    Time 0.065372    
2022-12-29 10:16:12,573 - Epoch: [155][  110/  113]    Overall Loss 1.293053    Objective Loss 1.293053                                        LR 0.000013    Time 0.065132    
2022-12-29 10:16:12,744 - Epoch: [155][  113/  113]    Overall Loss 1.297871    Objective Loss 1.297871    Top1 29.166667    Top5 100.000000    LR 0.000013    Time 0.064914    
2022-12-29 10:16:12,803 - --- validate (epoch=155)-----------
2022-12-29 10:16:12,804 - 200 samples (16 per mini-batch)
2022-12-29 10:16:13,361 - Epoch: [155][   10/   13]    Loss 1.154344    Top1 56.875000    Top5 96.875000    
2022-12-29 10:16:13,446 - Epoch: [155][   13/   13]    Loss 1.214996    Top1 52.000000    Top5 97.500000    
2022-12-29 10:16:13,509 - ==> Top1: 52.000    Top5: 97.500    Loss: 1.215

2022-12-29 10:16:13,509 - ==> Confusion:
[[21  3  0  6  4  0]
 [ 0 27  1  6  7  0]
 [ 1  8  5  5  8  0]
 [ 4  6  2 37  3  0]
 [ 2 12  3  8 12  0]
 [ 0  2  0  1  4  2]]

2022-12-29 10:16:13,512 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:13,512 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:13,527 - 

2022-12-29 10:16:13,528 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:14,359 - Epoch: [156][   10/  113]    Overall Loss 1.376797    Objective Loss 1.376797                                        LR 0.000013    Time 0.083075    
2022-12-29 10:16:14,997 - Epoch: [156][   20/  113]    Overall Loss 1.362009    Objective Loss 1.362009                                        LR 0.000013    Time 0.073409    
2022-12-29 10:16:15,636 - Epoch: [156][   30/  113]    Overall Loss 1.326044    Objective Loss 1.326044                                        LR 0.000013    Time 0.070219    
2022-12-29 10:16:16,275 - Epoch: [156][   40/  113]    Overall Loss 1.335041    Objective Loss 1.335041                                        LR 0.000013    Time 0.068615    
2022-12-29 10:16:16,910 - Epoch: [156][   50/  113]    Overall Loss 1.323738    Objective Loss 1.323738                                        LR 0.000013    Time 0.067594    
2022-12-29 10:16:17,542 - Epoch: [156][   60/  113]    Overall Loss 1.335912    Objective Loss 1.335912                                        LR 0.000013    Time 0.066844    
2022-12-29 10:16:18,180 - Epoch: [156][   70/  113]    Overall Loss 1.324021    Objective Loss 1.324021                                        LR 0.000013    Time 0.066406    
2022-12-29 10:16:18,812 - Epoch: [156][   80/  113]    Overall Loss 1.320309    Objective Loss 1.320309                                        LR 0.000013    Time 0.065995    
2022-12-29 10:16:19,445 - Epoch: [156][   90/  113]    Overall Loss 1.312765    Objective Loss 1.312765                                        LR 0.000013    Time 0.065691    
2022-12-29 10:16:20,074 - Epoch: [156][  100/  113]    Overall Loss 1.320235    Objective Loss 1.320235                                        LR 0.000013    Time 0.065412    
2022-12-29 10:16:20,701 - Epoch: [156][  110/  113]    Overall Loss 1.320718    Objective Loss 1.320718                                        LR 0.000013    Time 0.065157    
2022-12-29 10:16:20,873 - Epoch: [156][  113/  113]    Overall Loss 1.319702    Objective Loss 1.319702    Top1 50.000000    Top5 95.833333    LR 0.000013    Time 0.064943    
2022-12-29 10:16:20,929 - --- validate (epoch=156)-----------
2022-12-29 10:16:20,930 - 200 samples (16 per mini-batch)
2022-12-29 10:16:21,486 - Epoch: [156][   10/   13]    Loss 1.381927    Top1 40.625000    Top5 95.000000    
2022-12-29 10:16:21,571 - Epoch: [156][   13/   13]    Loss 1.326867    Top1 43.500000    Top5 95.000000    
2022-12-29 10:16:21,631 - ==> Top1: 43.500    Top5: 95.000    Loss: 1.327

2022-12-29 10:16:21,632 - ==> Confusion:
[[17  1  1  6  5  0]
 [ 4 16  7  5 15  1]
 [ 0  5  7 10  8  0]
 [ 4  4  3 26  7  0]
 [ 4  2  5  7 20  0]
 [ 2  2  2  2  1  1]]

2022-12-29 10:16:21,634 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:21,634 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:21,656 - 

2022-12-29 10:16:21,656 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:22,492 - Epoch: [157][   10/  113]    Overall Loss 1.330988    Objective Loss 1.330988                                        LR 0.000013    Time 0.083462    
2022-12-29 10:16:23,132 - Epoch: [157][   20/  113]    Overall Loss 1.312503    Objective Loss 1.312503                                        LR 0.000013    Time 0.073694    
2022-12-29 10:16:23,773 - Epoch: [157][   30/  113]    Overall Loss 1.274791    Objective Loss 1.274791                                        LR 0.000013    Time 0.070487    
2022-12-29 10:16:24,417 - Epoch: [157][   40/  113]    Overall Loss 1.253670    Objective Loss 1.253670                                        LR 0.000013    Time 0.068962    
2022-12-29 10:16:25,056 - Epoch: [157][   50/  113]    Overall Loss 1.250408    Objective Loss 1.250408                                        LR 0.000013    Time 0.067933    
2022-12-29 10:16:25,697 - Epoch: [157][   60/  113]    Overall Loss 1.254557    Objective Loss 1.254557                                        LR 0.000013    Time 0.067279    
2022-12-29 10:16:26,336 - Epoch: [157][   70/  113]    Overall Loss 1.265984    Objective Loss 1.265984                                        LR 0.000013    Time 0.066790    
2022-12-29 10:16:26,970 - Epoch: [157][   80/  113]    Overall Loss 1.262410    Objective Loss 1.262410                                        LR 0.000013    Time 0.066367    
2022-12-29 10:16:27,611 - Epoch: [157][   90/  113]    Overall Loss 1.272334    Objective Loss 1.272334                                        LR 0.000013    Time 0.066103    
2022-12-29 10:16:28,252 - Epoch: [157][  100/  113]    Overall Loss 1.283700    Objective Loss 1.283700                                        LR 0.000013    Time 0.065899    
2022-12-29 10:16:28,875 - Epoch: [157][  110/  113]    Overall Loss 1.286818    Objective Loss 1.286818                                        LR 0.000013    Time 0.065564    
2022-12-29 10:16:29,053 - Epoch: [157][  113/  113]    Overall Loss 1.294293    Objective Loss 1.294293    Top1 37.500000    Top5 100.000000    LR 0.000013    Time 0.065349    
2022-12-29 10:16:29,111 - --- validate (epoch=157)-----------
2022-12-29 10:16:29,111 - 200 samples (16 per mini-batch)
2022-12-29 10:16:29,656 - Epoch: [157][   10/   13]    Loss 1.431978    Top1 42.500000    Top5 92.500000    
2022-12-29 10:16:29,742 - Epoch: [157][   13/   13]    Loss 1.462650    Top1 42.000000    Top5 93.000000    
2022-12-29 10:16:29,798 - ==> Top1: 42.000    Top5: 93.000    Loss: 1.463

2022-12-29 10:16:29,798 - ==> Confusion:
[[16  3  0  3  5  2]
 [ 1 19  5  8  9  0]
 [ 1  9  8  6 12  0]
 [ 2  4  1 21 12  0]
 [ 1 10  3  7 19  1]
 [ 2  1  0  3  5  1]]

2022-12-29 10:16:29,801 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:29,801 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:29,820 - 

2022-12-29 10:16:29,821 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:30,673 - Epoch: [158][   10/  113]    Overall Loss 1.318345    Objective Loss 1.318345                                        LR 0.000013    Time 0.085067    
2022-12-29 10:16:31,308 - Epoch: [158][   20/  113]    Overall Loss 1.268153    Objective Loss 1.268153                                        LR 0.000013    Time 0.074259    
2022-12-29 10:16:31,950 - Epoch: [158][   30/  113]    Overall Loss 1.291708    Objective Loss 1.291708                                        LR 0.000013    Time 0.070886    
2022-12-29 10:16:32,589 - Epoch: [158][   40/  113]    Overall Loss 1.288542    Objective Loss 1.288542                                        LR 0.000013    Time 0.069123    
2022-12-29 10:16:33,226 - Epoch: [158][   50/  113]    Overall Loss 1.289767    Objective Loss 1.289767                                        LR 0.000013    Time 0.068031    
2022-12-29 10:16:33,861 - Epoch: [158][   60/  113]    Overall Loss 1.283246    Objective Loss 1.283246                                        LR 0.000013    Time 0.067264    
2022-12-29 10:16:34,500 - Epoch: [158][   70/  113]    Overall Loss 1.283439    Objective Loss 1.283439                                        LR 0.000013    Time 0.066776    
2022-12-29 10:16:35,137 - Epoch: [158][   80/  113]    Overall Loss 1.283384    Objective Loss 1.283384                                        LR 0.000013    Time 0.066391    
2022-12-29 10:16:35,770 - Epoch: [158][   90/  113]    Overall Loss 1.283059    Objective Loss 1.283059                                        LR 0.000013    Time 0.066038    
2022-12-29 10:16:36,401 - Epoch: [158][  100/  113]    Overall Loss 1.281469    Objective Loss 1.281469                                        LR 0.000013    Time 0.065746    
2022-12-29 10:16:37,026 - Epoch: [158][  110/  113]    Overall Loss 1.296578    Objective Loss 1.296578                                        LR 0.000013    Time 0.065443    
2022-12-29 10:16:37,203 - Epoch: [158][  113/  113]    Overall Loss 1.298576    Objective Loss 1.298576    Top1 41.666667    Top5 95.833333    LR 0.000013    Time 0.065270    
2022-12-29 10:16:37,257 - --- validate (epoch=158)-----------
2022-12-29 10:16:37,257 - 200 samples (16 per mini-batch)
2022-12-29 10:16:37,817 - Epoch: [158][   10/   13]    Loss 1.424092    Top1 40.000000    Top5 96.250000    
2022-12-29 10:16:37,903 - Epoch: [158][   13/   13]    Loss 1.452240    Top1 39.000000    Top5 96.000000    
2022-12-29 10:16:37,949 - ==> Top1: 39.000    Top5: 96.000    Loss: 1.452

2022-12-29 10:16:37,949 - ==> Confusion:
[[14  6  0  3  3  1]
 [ 4 14  4  8 12  1]
 [ 2 10  9  6  6  0]
 [ 3  5  2 27  7  0]
 [ 2  8  7  6 12  0]
 [ 3  3  4  2  4  2]]

2022-12-29 10:16:37,952 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:37,952 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:37,975 - 

2022-12-29 10:16:37,975 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:38,811 - Epoch: [159][   10/  113]    Overall Loss 1.329282    Objective Loss 1.329282                                        LR 0.000013    Time 0.083485    
2022-12-29 10:16:39,449 - Epoch: [159][   20/  113]    Overall Loss 1.334429    Objective Loss 1.334429                                        LR 0.000013    Time 0.073607    
2022-12-29 10:16:40,088 - Epoch: [159][   30/  113]    Overall Loss 1.330309    Objective Loss 1.330309                                        LR 0.000013    Time 0.070351    
2022-12-29 10:16:40,731 - Epoch: [159][   40/  113]    Overall Loss 1.328374    Objective Loss 1.328374                                        LR 0.000013    Time 0.068819    
2022-12-29 10:16:41,364 - Epoch: [159][   50/  113]    Overall Loss 1.318986    Objective Loss 1.318986                                        LR 0.000013    Time 0.067705    
2022-12-29 10:16:42,014 - Epoch: [159][   60/  113]    Overall Loss 1.319130    Objective Loss 1.319130                                        LR 0.000013    Time 0.067260    
2022-12-29 10:16:42,650 - Epoch: [159][   70/  113]    Overall Loss 1.315933    Objective Loss 1.315933                                        LR 0.000013    Time 0.066721    
2022-12-29 10:16:43,283 - Epoch: [159][   80/  113]    Overall Loss 1.305623    Objective Loss 1.305623                                        LR 0.000013    Time 0.066291    
2022-12-29 10:16:43,922 - Epoch: [159][   90/  113]    Overall Loss 1.299547    Objective Loss 1.299547                                        LR 0.000013    Time 0.066019    
2022-12-29 10:16:44,558 - Epoch: [159][  100/  113]    Overall Loss 1.302912    Objective Loss 1.302912                                        LR 0.000013    Time 0.065774    
2022-12-29 10:16:45,193 - Epoch: [159][  110/  113]    Overall Loss 1.312219    Objective Loss 1.312219                                        LR 0.000013    Time 0.065562    
2022-12-29 10:16:45,367 - Epoch: [159][  113/  113]    Overall Loss 1.315524    Objective Loss 1.315524    Top1 33.333333    Top5 100.000000    LR 0.000013    Time 0.065362    
2022-12-29 10:16:45,420 - --- validate (epoch=159)-----------
2022-12-29 10:16:45,421 - 200 samples (16 per mini-batch)
2022-12-29 10:16:45,972 - Epoch: [159][   10/   13]    Loss 1.444126    Top1 43.750000    Top5 95.625000    
2022-12-29 10:16:46,057 - Epoch: [159][   13/   13]    Loss 1.427538    Top1 44.000000    Top5 96.000000    
2022-12-29 10:16:46,111 - ==> Top1: 44.000    Top5: 96.000    Loss: 1.428

2022-12-29 10:16:46,112 - ==> Confusion:
[[ 7  7  1  2  4  0]
 [ 2 23  6  7  6  1]
 [ 1  6 10  4 10  0]
 [ 2  5  3 25  6  0]
 [ 2 13  5  5 19  0]
 [ 2  4  2  1  5  4]]

2022-12-29 10:16:46,115 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:46,115 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:46,129 - 

2022-12-29 10:16:46,130 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:46,961 - Epoch: [160][   10/  113]    Overall Loss 1.352986    Objective Loss 1.352986                                        LR 0.000013    Time 0.083036    
2022-12-29 10:16:47,600 - Epoch: [160][   20/  113]    Overall Loss 1.304349    Objective Loss 1.304349                                        LR 0.000013    Time 0.073428    
2022-12-29 10:16:48,236 - Epoch: [160][   30/  113]    Overall Loss 1.298171    Objective Loss 1.298171                                        LR 0.000013    Time 0.070123    
2022-12-29 10:16:48,869 - Epoch: [160][   40/  113]    Overall Loss 1.298477    Objective Loss 1.298477                                        LR 0.000013    Time 0.068420    
2022-12-29 10:16:49,504 - Epoch: [160][   50/  113]    Overall Loss 1.295614    Objective Loss 1.295614                                        LR 0.000013    Time 0.067430    
2022-12-29 10:16:50,139 - Epoch: [160][   60/  113]    Overall Loss 1.312548    Objective Loss 1.312548                                        LR 0.000013    Time 0.066767    
2022-12-29 10:16:50,780 - Epoch: [160][   70/  113]    Overall Loss 1.308819    Objective Loss 1.308819                                        LR 0.000013    Time 0.066371    
2022-12-29 10:16:51,412 - Epoch: [160][   80/  113]    Overall Loss 1.296444    Objective Loss 1.296444                                        LR 0.000013    Time 0.065966    
2022-12-29 10:16:52,047 - Epoch: [160][   90/  113]    Overall Loss 1.307742    Objective Loss 1.307742                                        LR 0.000013    Time 0.065694    
2022-12-29 10:16:52,680 - Epoch: [160][  100/  113]    Overall Loss 1.304441    Objective Loss 1.304441                                        LR 0.000013    Time 0.065446    
2022-12-29 10:16:53,313 - Epoch: [160][  110/  113]    Overall Loss 1.323668    Objective Loss 1.323668                                        LR 0.000013    Time 0.065245    
2022-12-29 10:16:53,482 - Epoch: [160][  113/  113]    Overall Loss 1.320975    Objective Loss 1.320975    Top1 54.166667    Top5 95.833333    LR 0.000013    Time 0.065006    
2022-12-29 10:16:53,544 - --- validate (epoch=160)-----------
2022-12-29 10:16:53,544 - 200 samples (16 per mini-batch)
2022-12-29 10:16:54,092 - Epoch: [160][   10/   13]    Loss 1.236482    Top1 50.625000    Top5 97.500000    
2022-12-29 10:16:54,176 - Epoch: [160][   13/   13]    Loss 1.310567    Top1 48.000000    Top5 97.500000    
2022-12-29 10:16:54,227 - ==> Top1: 48.000    Top5: 97.500    Loss: 1.311

2022-12-29 10:16:54,228 - ==> Confusion:
[[17  3  0  6  3  0]
 [10 12  1  8  9  0]
 [ 2  5 10  3  8  0]
 [ 2  3  2 31 12  0]
 [ 3  7  2  8 25  0]
 [ 2  1  1  0  3  1]]

2022-12-29 10:16:54,232 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:16:54,232 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:16:54,254 - 

2022-12-29 10:16:54,254 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:16:55,092 - Epoch: [161][   10/  113]    Overall Loss 1.268527    Objective Loss 1.268527                                        LR 0.000013    Time 0.083758    
2022-12-29 10:16:55,731 - Epoch: [161][   20/  113]    Overall Loss 1.322729    Objective Loss 1.322729                                        LR 0.000013    Time 0.073788    
2022-12-29 10:16:56,371 - Epoch: [161][   30/  113]    Overall Loss 1.323751    Objective Loss 1.323751                                        LR 0.000013    Time 0.070512    
2022-12-29 10:16:57,015 - Epoch: [161][   40/  113]    Overall Loss 1.308417    Objective Loss 1.308417                                        LR 0.000013    Time 0.068974    
2022-12-29 10:16:57,655 - Epoch: [161][   50/  113]    Overall Loss 1.297071    Objective Loss 1.297071                                        LR 0.000013    Time 0.067956    
2022-12-29 10:16:58,292 - Epoch: [161][   60/  113]    Overall Loss 1.315789    Objective Loss 1.315789                                        LR 0.000013    Time 0.067242    
2022-12-29 10:16:58,933 - Epoch: [161][   70/  113]    Overall Loss 1.315598    Objective Loss 1.315598                                        LR 0.000013    Time 0.066792    
2022-12-29 10:16:59,569 - Epoch: [161][   80/  113]    Overall Loss 1.316130    Objective Loss 1.316130                                        LR 0.000013    Time 0.066381    
2022-12-29 10:17:00,210 - Epoch: [161][   90/  113]    Overall Loss 1.325168    Objective Loss 1.325168                                        LR 0.000013    Time 0.066126    
2022-12-29 10:17:00,844 - Epoch: [161][  100/  113]    Overall Loss 1.327017    Objective Loss 1.327017                                        LR 0.000013    Time 0.065854    
2022-12-29 10:17:01,475 - Epoch: [161][  110/  113]    Overall Loss 1.331671    Objective Loss 1.331671                                        LR 0.000013    Time 0.065592    
2022-12-29 10:17:01,650 - Epoch: [161][  113/  113]    Overall Loss 1.333323    Objective Loss 1.333323    Top1 41.666667    Top5 100.000000    LR 0.000013    Time 0.065397    
2022-12-29 10:17:01,712 - --- validate (epoch=161)-----------
2022-12-29 10:17:01,712 - 200 samples (16 per mini-batch)
2022-12-29 10:17:02,258 - Epoch: [161][   10/   13]    Loss 1.353711    Top1 42.500000    Top5 98.125000    
2022-12-29 10:17:02,343 - Epoch: [161][   13/   13]    Loss 1.329297    Top1 44.000000    Top5 97.500000    
2022-12-29 10:17:02,397 - ==> Top1: 44.000    Top5: 97.500    Loss: 1.329

2022-12-29 10:17:02,398 - ==> Confusion:
[[15  4  0  9  6  0]
 [ 2 15  3  6  9  0]
 [ 1  6  7  4  8  1]
 [ 2  4  3 33 10  0]
 [ 2  8  4  6 16  0]
 [ 1  7  2  1  3  2]]

2022-12-29 10:17:02,400 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:17:02,400 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:02,410 - 

2022-12-29 10:17:02,410 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:03,249 - Epoch: [162][   10/  113]    Overall Loss 1.312323    Objective Loss 1.312323                                        LR 0.000013    Time 0.083763    
2022-12-29 10:17:03,886 - Epoch: [162][   20/  113]    Overall Loss 1.291642    Objective Loss 1.291642                                        LR 0.000013    Time 0.073690    
2022-12-29 10:17:04,527 - Epoch: [162][   30/  113]    Overall Loss 1.291875    Objective Loss 1.291875                                        LR 0.000013    Time 0.070479    
2022-12-29 10:17:05,163 - Epoch: [162][   40/  113]    Overall Loss 1.281626    Objective Loss 1.281626                                        LR 0.000013    Time 0.068749    
2022-12-29 10:17:05,797 - Epoch: [162][   50/  113]    Overall Loss 1.289800    Objective Loss 1.289800                                        LR 0.000013    Time 0.067678    
2022-12-29 10:17:06,436 - Epoch: [162][   60/  113]    Overall Loss 1.292861    Objective Loss 1.292861                                        LR 0.000013    Time 0.067031    
2022-12-29 10:17:07,073 - Epoch: [162][   70/  113]    Overall Loss 1.291100    Objective Loss 1.291100                                        LR 0.000013    Time 0.066551    
2022-12-29 10:17:07,704 - Epoch: [162][   80/  113]    Overall Loss 1.291852    Objective Loss 1.291852                                        LR 0.000013    Time 0.066121    
2022-12-29 10:17:08,343 - Epoch: [162][   90/  113]    Overall Loss 1.286858    Objective Loss 1.286858                                        LR 0.000013    Time 0.065863    
2022-12-29 10:17:08,973 - Epoch: [162][  100/  113]    Overall Loss 1.289187    Objective Loss 1.289187                                        LR 0.000013    Time 0.065571    
2022-12-29 10:17:09,598 - Epoch: [162][  110/  113]    Overall Loss 1.286566    Objective Loss 1.286566                                        LR 0.000013    Time 0.065285    
2022-12-29 10:17:09,771 - Epoch: [162][  113/  113]    Overall Loss 1.287736    Objective Loss 1.287736    Top1 58.333333    Top5 100.000000    LR 0.000013    Time 0.065081    
2022-12-29 10:17:09,826 - --- validate (epoch=162)-----------
2022-12-29 10:17:09,826 - 200 samples (16 per mini-batch)
2022-12-29 10:17:10,371 - Epoch: [162][   10/   13]    Loss 1.291584    Top1 50.625000    Top5 96.875000    
2022-12-29 10:17:10,458 - Epoch: [162][   13/   13]    Loss 1.300896    Top1 50.000000    Top5 97.000000    
2022-12-29 10:17:10,508 - ==> Top1: 50.000    Top5: 97.000    Loss: 1.301

2022-12-29 10:17:10,508 - ==> Confusion:
[[13  0  0 10  1  0]
 [ 5 30  0  4  7  0]
 [ 3  5  6  5  6  0]
 [ 5  2  1 34  7  0]
 [ 5  9  4  5 15  0]
 [ 5  3  3  2  3  2]]

2022-12-29 10:17:10,511 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:17:10,511 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:10,524 - 

2022-12-29 10:17:10,524 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:11,351 - Epoch: [163][   10/  113]    Overall Loss 1.375634    Objective Loss 1.375634                                        LR 0.000013    Time 0.082577    
2022-12-29 10:17:11,990 - Epoch: [163][   20/  113]    Overall Loss 1.338675    Objective Loss 1.338675                                        LR 0.000013    Time 0.073219    
2022-12-29 10:17:12,629 - Epoch: [163][   30/  113]    Overall Loss 1.275647    Objective Loss 1.275647                                        LR 0.000013    Time 0.070068    
2022-12-29 10:17:13,262 - Epoch: [163][   40/  113]    Overall Loss 1.268746    Objective Loss 1.268746                                        LR 0.000013    Time 0.068378    
2022-12-29 10:17:13,896 - Epoch: [163][   50/  113]    Overall Loss 1.288477    Objective Loss 1.288477                                        LR 0.000013    Time 0.067370    
2022-12-29 10:17:14,529 - Epoch: [163][   60/  113]    Overall Loss 1.265331    Objective Loss 1.265331                                        LR 0.000013    Time 0.066684    
2022-12-29 10:17:15,164 - Epoch: [163][   70/  113]    Overall Loss 1.261970    Objective Loss 1.261970                                        LR 0.000013    Time 0.066212    
2022-12-29 10:17:15,803 - Epoch: [163][   80/  113]    Overall Loss 1.267283    Objective Loss 1.267283                                        LR 0.000013    Time 0.065920    
2022-12-29 10:17:16,437 - Epoch: [163][   90/  113]    Overall Loss 1.279165    Objective Loss 1.279165                                        LR 0.000013    Time 0.065634    
2022-12-29 10:17:17,076 - Epoch: [163][  100/  113]    Overall Loss 1.287838    Objective Loss 1.287838                                        LR 0.000013    Time 0.065455    
2022-12-29 10:17:17,704 - Epoch: [163][  110/  113]    Overall Loss 1.288226    Objective Loss 1.288226                                        LR 0.000013    Time 0.065209    
2022-12-29 10:17:17,876 - Epoch: [163][  113/  113]    Overall Loss 1.294656    Objective Loss 1.294656    Top1 37.500000    Top5 95.833333    LR 0.000013    Time 0.065001    
2022-12-29 10:17:17,928 - --- validate (epoch=163)-----------
2022-12-29 10:17:17,928 - 200 samples (16 per mini-batch)
2022-12-29 10:17:18,472 - Epoch: [163][   10/   13]    Loss 1.255625    Top1 50.625000    Top5 98.125000    
2022-12-29 10:17:18,558 - Epoch: [163][   13/   13]    Loss 1.256901    Top1 48.500000    Top5 97.500000    
2022-12-29 10:17:18,605 - ==> Top1: 48.500    Top5: 97.500    Loss: 1.257

2022-12-29 10:17:18,605 - ==> Confusion:
[[18  4  1  9  3  0]
 [ 3 21  2  4  3  1]
 [ 3  9 11  5  1  0]
 [ 4  6  2 35  8  1]
 [ 2 16  2  6 12  0]
 [ 1  1  1  3  2  0]]

2022-12-29 10:17:18,608 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:17:18,609 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:18,624 - 

2022-12-29 10:17:18,624 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:19,467 - Epoch: [164][   10/  113]    Overall Loss 1.371051    Objective Loss 1.371051                                        LR 0.000013    Time 0.084208    
2022-12-29 10:17:20,101 - Epoch: [164][   20/  113]    Overall Loss 1.329276    Objective Loss 1.329276                                        LR 0.000013    Time 0.073757    
2022-12-29 10:17:20,739 - Epoch: [164][   30/  113]    Overall Loss 1.293085    Objective Loss 1.293085                                        LR 0.000013    Time 0.070443    
2022-12-29 10:17:21,376 - Epoch: [164][   40/  113]    Overall Loss 1.264761    Objective Loss 1.264761                                        LR 0.000013    Time 0.068730    
2022-12-29 10:17:22,015 - Epoch: [164][   50/  113]    Overall Loss 1.285537    Objective Loss 1.285537                                        LR 0.000013    Time 0.067752    
2022-12-29 10:17:22,645 - Epoch: [164][   60/  113]    Overall Loss 1.291966    Objective Loss 1.291966                                        LR 0.000013    Time 0.066954    
2022-12-29 10:17:23,274 - Epoch: [164][   70/  113]    Overall Loss 1.284883    Objective Loss 1.284883                                        LR 0.000013    Time 0.066377    
2022-12-29 10:17:23,901 - Epoch: [164][   80/  113]    Overall Loss 1.267038    Objective Loss 1.267038                                        LR 0.000013    Time 0.065908    
2022-12-29 10:17:24,531 - Epoch: [164][   90/  113]    Overall Loss 1.265589    Objective Loss 1.265589                                        LR 0.000013    Time 0.065584    
2022-12-29 10:17:25,163 - Epoch: [164][  100/  113]    Overall Loss 1.268201    Objective Loss 1.268201                                        LR 0.000013    Time 0.065339    
2022-12-29 10:17:25,787 - Epoch: [164][  110/  113]    Overall Loss 1.273995    Objective Loss 1.273995                                        LR 0.000013    Time 0.065062    
2022-12-29 10:17:25,963 - Epoch: [164][  113/  113]    Overall Loss 1.270527    Objective Loss 1.270527    Top1 50.000000    Top5 95.833333    LR 0.000013    Time 0.064893    
2022-12-29 10:17:26,010 - --- validate (epoch=164)-----------
2022-12-29 10:17:26,011 - 200 samples (16 per mini-batch)
2022-12-29 10:17:26,563 - Epoch: [164][   10/   13]    Loss 1.358401    Top1 45.000000    Top5 96.250000    
2022-12-29 10:17:26,653 - Epoch: [164][   13/   13]    Loss 1.326956    Top1 45.000000    Top5 96.500000    
2022-12-29 10:17:26,714 - ==> Top1: 45.000    Top5: 96.500    Loss: 1.327

2022-12-29 10:17:26,714 - ==> Confusion:
[[21  1  1 10  3  0]
 [ 3 14  1 15  5  1]
 [ 2  5  6 12  5  0]
 [ 4  2  0 37  0  0]
 [ 3  9  1 16 10  0]
 [ 1  3  1  4  2  2]]

2022-12-29 10:17:26,717 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:17:26,717 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:26,736 - 

2022-12-29 10:17:26,736 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:27,578 - Epoch: [165][   10/  113]    Overall Loss 1.504315    Objective Loss 1.504315                                        LR 0.000013    Time 0.083981    
2022-12-29 10:17:28,216 - Epoch: [165][   20/  113]    Overall Loss 1.415644    Objective Loss 1.415644                                        LR 0.000013    Time 0.073878    
2022-12-29 10:17:28,852 - Epoch: [165][   30/  113]    Overall Loss 1.344603    Objective Loss 1.344603                                        LR 0.000013    Time 0.070438    
2022-12-29 10:17:29,487 - Epoch: [165][   40/  113]    Overall Loss 1.309817    Objective Loss 1.309817                                        LR 0.000013    Time 0.068690    
2022-12-29 10:17:30,119 - Epoch: [165][   50/  113]    Overall Loss 1.303187    Objective Loss 1.303187                                        LR 0.000013    Time 0.067580    
2022-12-29 10:17:30,757 - Epoch: [165][   60/  113]    Overall Loss 1.310619    Objective Loss 1.310619                                        LR 0.000013    Time 0.066939    
2022-12-29 10:17:31,389 - Epoch: [165][   70/  113]    Overall Loss 1.300140    Objective Loss 1.300140                                        LR 0.000013    Time 0.066406    
2022-12-29 10:17:32,025 - Epoch: [165][   80/  113]    Overall Loss 1.297188    Objective Loss 1.297188                                        LR 0.000013    Time 0.066045    
2022-12-29 10:17:32,657 - Epoch: [165][   90/  113]    Overall Loss 1.293509    Objective Loss 1.293509                                        LR 0.000013    Time 0.065722    
2022-12-29 10:17:33,291 - Epoch: [165][  100/  113]    Overall Loss 1.289059    Objective Loss 1.289059                                        LR 0.000013    Time 0.065485    
2022-12-29 10:17:33,916 - Epoch: [165][  110/  113]    Overall Loss 1.296702    Objective Loss 1.296702                                        LR 0.000013    Time 0.065206    
2022-12-29 10:17:34,087 - Epoch: [165][  113/  113]    Overall Loss 1.301324    Objective Loss 1.301324    Top1 62.500000    Top5 100.000000    LR 0.000013    Time 0.064993    
2022-12-29 10:17:34,146 - --- validate (epoch=165)-----------
2022-12-29 10:17:34,147 - 200 samples (16 per mini-batch)
2022-12-29 10:17:34,707 - Epoch: [165][   10/   13]    Loss 1.313879    Top1 48.750000    Top5 96.875000    
2022-12-29 10:17:34,792 - Epoch: [165][   13/   13]    Loss 1.352059    Top1 46.500000    Top5 96.500000    
2022-12-29 10:17:34,835 - ==> Top1: 46.500    Top5: 96.500    Loss: 1.352

2022-12-29 10:17:34,835 - ==> Confusion:
[[22  5  1  4  3  0]
 [ 4 22  4  6 11  0]
 [ 0  8  8  5  9  0]
 [ 3  2  3 31  7  1]
 [ 1  6  3 11 10  0]
 [ 1  6  2  1  0  0]]

2022-12-29 10:17:34,838 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:17:34,838 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:34,860 - 

2022-12-29 10:17:34,860 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:35,697 - Epoch: [166][   10/  113]    Overall Loss 1.301354    Objective Loss 1.301354                                        LR 0.000013    Time 0.083596    
2022-12-29 10:17:36,337 - Epoch: [166][   20/  113]    Overall Loss 1.263246    Objective Loss 1.263246                                        LR 0.000013    Time 0.073741    
2022-12-29 10:17:36,975 - Epoch: [166][   30/  113]    Overall Loss 1.259679    Objective Loss 1.259679                                        LR 0.000013    Time 0.070420    
2022-12-29 10:17:37,613 - Epoch: [166][   40/  113]    Overall Loss 1.259050    Objective Loss 1.259050                                        LR 0.000013    Time 0.068745    
2022-12-29 10:17:38,245 - Epoch: [166][   50/  113]    Overall Loss 1.274987    Objective Loss 1.274987                                        LR 0.000013    Time 0.067625    
2022-12-29 10:17:38,880 - Epoch: [166][   60/  113]    Overall Loss 1.284370    Objective Loss 1.284370                                        LR 0.000013    Time 0.066934    
2022-12-29 10:17:39,516 - Epoch: [166][   70/  113]    Overall Loss 1.290860    Objective Loss 1.290860                                        LR 0.000013    Time 0.066458    
2022-12-29 10:17:40,151 - Epoch: [166][   80/  113]    Overall Loss 1.282046    Objective Loss 1.282046                                        LR 0.000013    Time 0.066078    
2022-12-29 10:17:40,783 - Epoch: [166][   90/  113]    Overall Loss 1.286470    Objective Loss 1.286470                                        LR 0.000013    Time 0.065753    
2022-12-29 10:17:41,417 - Epoch: [166][  100/  113]    Overall Loss 1.277214    Objective Loss 1.277214                                        LR 0.000013    Time 0.065517    
2022-12-29 10:17:42,052 - Epoch: [166][  110/  113]    Overall Loss 1.281666    Objective Loss 1.281666                                        LR 0.000013    Time 0.065326    
2022-12-29 10:17:42,227 - Epoch: [166][  113/  113]    Overall Loss 1.278921    Objective Loss 1.278921    Top1 50.000000    Top5 100.000000    LR 0.000013    Time 0.065135    
2022-12-29 10:17:42,277 - --- validate (epoch=166)-----------
2022-12-29 10:17:42,277 - 200 samples (16 per mini-batch)
2022-12-29 10:17:42,820 - Epoch: [166][   10/   13]    Loss 1.214786    Top1 51.250000    Top5 100.000000    
2022-12-29 10:17:42,905 - Epoch: [166][   13/   13]    Loss 1.214721    Top1 51.500000    Top5 99.500000    
2022-12-29 10:17:42,960 - ==> Top1: 51.500    Top5: 99.500    Loss: 1.215

2022-12-29 10:17:42,960 - ==> Confusion:
[[22  2  0  9  5  0]
 [ 0 20  2  3  4  0]
 [ 0 13 15  7  5  1]
 [ 3  4  3 27  3  1]
 [ 2 15  2  6 18  0]
 [ 0  1  0  2  4  1]]

2022-12-29 10:17:42,962 - ==> Best [Top1: 53.500   Top5: 98.500   Sparsity:0.00   Params: 289216 on epoch: 138]
2022-12-29 10:17:42,963 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:42,990 - 

2022-12-29 10:17:42,990 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:43,807 - Epoch: [167][   10/  113]    Overall Loss 1.418451    Objective Loss 1.418451                                        LR 0.000013    Time 0.081594    
2022-12-29 10:17:44,437 - Epoch: [167][   20/  113]    Overall Loss 1.394263    Objective Loss 1.394263                                        LR 0.000013    Time 0.072303    
2022-12-29 10:17:45,073 - Epoch: [167][   30/  113]    Overall Loss 1.383265    Objective Loss 1.383265                                        LR 0.000013    Time 0.069358    
2022-12-29 10:17:45,699 - Epoch: [167][   40/  113]    Overall Loss 1.367661    Objective Loss 1.367661                                        LR 0.000013    Time 0.067659    
2022-12-29 10:17:46,328 - Epoch: [167][   50/  113]    Overall Loss 1.358341    Objective Loss 1.358341                                        LR 0.000013    Time 0.066714    
2022-12-29 10:17:46,960 - Epoch: [167][   60/  113]    Overall Loss 1.348565    Objective Loss 1.348565                                        LR 0.000013    Time 0.066117    
2022-12-29 10:17:47,590 - Epoch: [167][   70/  113]    Overall Loss 1.346436    Objective Loss 1.346436                                        LR 0.000013    Time 0.065666    
2022-12-29 10:17:48,221 - Epoch: [167][   80/  113]    Overall Loss 1.335462    Objective Loss 1.335462                                        LR 0.000013    Time 0.065340    
2022-12-29 10:17:48,855 - Epoch: [167][   90/  113]    Overall Loss 1.344697    Objective Loss 1.344697                                        LR 0.000013    Time 0.065120    
2022-12-29 10:17:49,485 - Epoch: [167][  100/  113]    Overall Loss 1.344066    Objective Loss 1.344066                                        LR 0.000013    Time 0.064896    
2022-12-29 10:17:50,101 - Epoch: [167][  110/  113]    Overall Loss 1.339038    Objective Loss 1.339038                                        LR 0.000013    Time 0.064597    
2022-12-29 10:17:50,274 - Epoch: [167][  113/  113]    Overall Loss 1.339829    Objective Loss 1.339829    Top1 54.166667    Top5 91.666667    LR 0.000013    Time 0.064409    
2022-12-29 10:17:50,335 - --- validate (epoch=167)-----------
2022-12-29 10:17:50,335 - 200 samples (16 per mini-batch)
2022-12-29 10:17:50,893 - Epoch: [167][   10/   13]    Loss 1.219584    Top1 54.375000    Top5 97.500000    
2022-12-29 10:17:50,981 - Epoch: [167][   13/   13]    Loss 1.228109    Top1 55.000000    Top5 98.000000    
2022-12-29 10:17:51,033 - ==> Top1: 55.000    Top5: 98.000    Loss: 1.228

2022-12-29 10:17:51,034 - ==> Confusion:
[[21  0  1  6  2  0]
 [ 2 20  1  7 11  0]
 [ 0  6 11  8  9  0]
 [ 3  0  0 34  1  1]
 [ 3  6  0 11 21  0]
 [ 2  1  1  1  7  3]]

2022-12-29 10:17:51,037 - ==> Best [Top1: 55.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 167]
2022-12-29 10:17:51,037 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:51,054 - 

2022-12-29 10:17:51,054 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:17:51,877 - Epoch: [168][   10/  113]    Overall Loss 1.201341    Objective Loss 1.201341                                        LR 0.000013    Time 0.082226    
2022-12-29 10:17:52,517 - Epoch: [168][   20/  113]    Overall Loss 1.230396    Objective Loss 1.230396                                        LR 0.000013    Time 0.073088    
2022-12-29 10:17:53,158 - Epoch: [168][   30/  113]    Overall Loss 1.259568    Objective Loss 1.259568                                        LR 0.000013    Time 0.070066    
2022-12-29 10:17:53,794 - Epoch: [168][   40/  113]    Overall Loss 1.252570    Objective Loss 1.252570                                        LR 0.000013    Time 0.068432    
2022-12-29 10:17:54,425 - Epoch: [168][   50/  113]    Overall Loss 1.268377    Objective Loss 1.268377                                        LR 0.000013    Time 0.067352    
2022-12-29 10:17:55,062 - Epoch: [168][   60/  113]    Overall Loss 1.272253    Objective Loss 1.272253                                        LR 0.000013    Time 0.066731    
2022-12-29 10:17:55,696 - Epoch: [168][   70/  113]    Overall Loss 1.250800    Objective Loss 1.250800                                        LR 0.000013    Time 0.066257    
2022-12-29 10:17:56,331 - Epoch: [168][   80/  113]    Overall Loss 1.236230    Objective Loss 1.236230                                        LR 0.000013    Time 0.065901    
2022-12-29 10:17:56,971 - Epoch: [168][   90/  113]    Overall Loss 1.238420    Objective Loss 1.238420                                        LR 0.000013    Time 0.065687    
2022-12-29 10:17:57,611 - Epoch: [168][  100/  113]    Overall Loss 1.241984    Objective Loss 1.241984                                        LR 0.000013    Time 0.065512    
2022-12-29 10:17:58,240 - Epoch: [168][  110/  113]    Overall Loss 1.245449    Objective Loss 1.245449                                        LR 0.000013    Time 0.065270    
2022-12-29 10:17:58,413 - Epoch: [168][  113/  113]    Overall Loss 1.238848    Objective Loss 1.238848    Top1 54.166667    Top5 100.000000    LR 0.000013    Time 0.065068    
2022-12-29 10:17:58,477 - --- validate (epoch=168)-----------
2022-12-29 10:17:58,477 - 200 samples (16 per mini-batch)
2022-12-29 10:17:59,033 - Epoch: [168][   10/   13]    Loss 1.180550    Top1 58.125000    Top5 98.125000    
2022-12-29 10:17:59,119 - Epoch: [168][   13/   13]    Loss 1.221294    Top1 55.500000    Top5 98.500000    
2022-12-29 10:17:59,176 - ==> Top1: 55.500    Top5: 98.500    Loss: 1.221

2022-12-29 10:17:59,177 - ==> Confusion:
[[30  6  1 10  2  0]
 [ 1 29  2 10  4  0]
 [ 0  7 13  9  9  0]
 [ 2  3  1 26  2  0]
 [ 3  6  1  2 11  0]
 [ 1  4  1  1  1  2]]

2022-12-29 10:17:59,179 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:17:59,180 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:17:59,207 - 

2022-12-29 10:17:59,208 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:00,036 - Epoch: [169][   10/  113]    Overall Loss 1.175570    Objective Loss 1.175570                                        LR 0.000013    Time 0.082593    
2022-12-29 10:18:00,675 - Epoch: [169][   20/  113]    Overall Loss 1.235808    Objective Loss 1.235808                                        LR 0.000013    Time 0.073254    
2022-12-29 10:18:01,313 - Epoch: [169][   30/  113]    Overall Loss 1.251755    Objective Loss 1.251755                                        LR 0.000013    Time 0.070069    
2022-12-29 10:18:01,951 - Epoch: [169][   40/  113]    Overall Loss 1.244680    Objective Loss 1.244680                                        LR 0.000013    Time 0.068493    
2022-12-29 10:18:02,583 - Epoch: [169][   50/  113]    Overall Loss 1.270687    Objective Loss 1.270687                                        LR 0.000013    Time 0.067433    
2022-12-29 10:18:03,221 - Epoch: [169][   60/  113]    Overall Loss 1.279433    Objective Loss 1.279433                                        LR 0.000013    Time 0.066811    
2022-12-29 10:18:03,855 - Epoch: [169][   70/  113]    Overall Loss 1.301885    Objective Loss 1.301885                                        LR 0.000013    Time 0.066316    
2022-12-29 10:18:04,486 - Epoch: [169][   80/  113]    Overall Loss 1.285410    Objective Loss 1.285410                                        LR 0.000013    Time 0.065911    
2022-12-29 10:18:05,126 - Epoch: [169][   90/  113]    Overall Loss 1.284501    Objective Loss 1.284501                                        LR 0.000013    Time 0.065689    
2022-12-29 10:18:05,757 - Epoch: [169][  100/  113]    Overall Loss 1.271814    Objective Loss 1.271814                                        LR 0.000013    Time 0.065434    
2022-12-29 10:18:06,382 - Epoch: [169][  110/  113]    Overall Loss 1.265941    Objective Loss 1.265941                                        LR 0.000013    Time 0.065156    
2022-12-29 10:18:06,558 - Epoch: [169][  113/  113]    Overall Loss 1.268292    Objective Loss 1.268292    Top1 54.166667    Top5 91.666667    LR 0.000013    Time 0.064985    
2022-12-29 10:18:06,608 - --- validate (epoch=169)-----------
2022-12-29 10:18:06,608 - 200 samples (16 per mini-batch)
2022-12-29 10:18:07,151 - Epoch: [169][   10/   13]    Loss 1.380941    Top1 44.375000    Top5 96.250000    
2022-12-29 10:18:07,233 - Epoch: [169][   13/   13]    Loss 1.362700    Top1 43.500000    Top5 96.500000    
2022-12-29 10:18:07,294 - ==> Top1: 43.500    Top5: 96.500    Loss: 1.363

2022-12-29 10:18:07,294 - ==> Confusion:
[[14  3  2  1  4  0]
 [ 0 12  6 13 10  0]
 [ 2  6 15  5 17  0]
 [ 6  5  1 32  8  0]
 [ 2  4  3  7 13  0]
 [ 1  3  1  3  0  1]]

2022-12-29 10:18:07,297 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:07,297 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:07,316 - 

2022-12-29 10:18:07,317 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:08,141 - Epoch: [170][   10/  113]    Overall Loss 1.329803    Objective Loss 1.329803                                        LR 0.000013    Time 0.082209    
2022-12-29 10:18:08,767 - Epoch: [170][   20/  113]    Overall Loss 1.336509    Objective Loss 1.336509                                        LR 0.000013    Time 0.072400    
2022-12-29 10:18:09,395 - Epoch: [170][   30/  113]    Overall Loss 1.338361    Objective Loss 1.338361                                        LR 0.000013    Time 0.069171    
2022-12-29 10:18:10,023 - Epoch: [170][   40/  113]    Overall Loss 1.307083    Objective Loss 1.307083                                        LR 0.000013    Time 0.067564    
2022-12-29 10:18:10,653 - Epoch: [170][   50/  113]    Overall Loss 1.292470    Objective Loss 1.292470                                        LR 0.000013    Time 0.066643    
2022-12-29 10:18:11,282 - Epoch: [170][   60/  113]    Overall Loss 1.314777    Objective Loss 1.314777                                        LR 0.000013    Time 0.066017    
2022-12-29 10:18:11,912 - Epoch: [170][   70/  113]    Overall Loss 1.315590    Objective Loss 1.315590                                        LR 0.000013    Time 0.065573    
2022-12-29 10:18:12,541 - Epoch: [170][   80/  113]    Overall Loss 1.311383    Objective Loss 1.311383                                        LR 0.000013    Time 0.065232    
2022-12-29 10:18:13,173 - Epoch: [170][   90/  113]    Overall Loss 1.300382    Objective Loss 1.300382                                        LR 0.000013    Time 0.065001    
2022-12-29 10:18:13,799 - Epoch: [170][  100/  113]    Overall Loss 1.289271    Objective Loss 1.289271                                        LR 0.000013    Time 0.064755    
2022-12-29 10:18:14,425 - Epoch: [170][  110/  113]    Overall Loss 1.290365    Objective Loss 1.290365                                        LR 0.000013    Time 0.064557    
2022-12-29 10:18:14,596 - Epoch: [170][  113/  113]    Overall Loss 1.288165    Objective Loss 1.288165    Top1 50.000000    Top5 100.000000    LR 0.000013    Time 0.064354    
2022-12-29 10:18:14,652 - --- validate (epoch=170)-----------
2022-12-29 10:18:14,653 - 200 samples (16 per mini-batch)
2022-12-29 10:18:15,197 - Epoch: [170][   10/   13]    Loss 1.285715    Top1 49.375000    Top5 98.125000    
2022-12-29 10:18:15,284 - Epoch: [170][   13/   13]    Loss 1.261258    Top1 48.000000    Top5 98.500000    
2022-12-29 10:18:15,352 - ==> Top1: 48.000    Top5: 98.500    Loss: 1.261

2022-12-29 10:18:15,353 - ==> Confusion:
[[18  3  2  4  4  0]
 [ 3 11  5  4 11  0]
 [ 2  8 13  6  8  0]
 [ 4  6  4 40  5  0]
 [ 1  5  5  5 13  0]
 [ 0  3  2  1  3  1]]

2022-12-29 10:18:15,357 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:15,357 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:15,374 - 

2022-12-29 10:18:15,374 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:16,210 - Epoch: [171][   10/  113]    Overall Loss 1.432988    Objective Loss 1.432988                                        LR 0.000013    Time 0.083446    
2022-12-29 10:18:16,847 - Epoch: [171][   20/  113]    Overall Loss 1.411140    Objective Loss 1.411140                                        LR 0.000013    Time 0.073542    
2022-12-29 10:18:17,480 - Epoch: [171][   30/  113]    Overall Loss 1.352899    Objective Loss 1.352899                                        LR 0.000013    Time 0.070110    
2022-12-29 10:18:18,115 - Epoch: [171][   40/  113]    Overall Loss 1.315684    Objective Loss 1.315684                                        LR 0.000013    Time 0.068458    
2022-12-29 10:18:18,750 - Epoch: [171][   50/  113]    Overall Loss 1.313409    Objective Loss 1.313409                                        LR 0.000013    Time 0.067464    
2022-12-29 10:18:19,386 - Epoch: [171][   60/  113]    Overall Loss 1.293601    Objective Loss 1.293601                                        LR 0.000013    Time 0.066808    
2022-12-29 10:18:20,021 - Epoch: [171][   70/  113]    Overall Loss 1.292972    Objective Loss 1.292972                                        LR 0.000013    Time 0.066335    
2022-12-29 10:18:20,655 - Epoch: [171][   80/  113]    Overall Loss 1.290204    Objective Loss 1.290204                                        LR 0.000013    Time 0.065952    
2022-12-29 10:18:21,289 - Epoch: [171][   90/  113]    Overall Loss 1.291961    Objective Loss 1.291961                                        LR 0.000013    Time 0.065666    
2022-12-29 10:18:21,929 - Epoch: [171][  100/  113]    Overall Loss 1.291997    Objective Loss 1.291997                                        LR 0.000013    Time 0.065497    
2022-12-29 10:18:22,557 - Epoch: [171][  110/  113]    Overall Loss 1.289961    Objective Loss 1.289961                                        LR 0.000013    Time 0.065242    
2022-12-29 10:18:22,728 - Epoch: [171][  113/  113]    Overall Loss 1.294545    Objective Loss 1.294545    Top1 45.833333    Top5 100.000000    LR 0.000013    Time 0.065023    
2022-12-29 10:18:22,787 - --- validate (epoch=171)-----------
2022-12-29 10:18:22,787 - 200 samples (16 per mini-batch)
2022-12-29 10:18:23,333 - Epoch: [171][   10/   13]    Loss 1.328891    Top1 47.500000    Top5 97.500000    
2022-12-29 10:18:23,417 - Epoch: [171][   13/   13]    Loss 1.253939    Top1 49.500000    Top5 97.500000    
2022-12-29 10:18:23,467 - ==> Top1: 49.500    Top5: 97.500    Loss: 1.254

2022-12-29 10:18:23,468 - ==> Confusion:
[[22  6  0  5  5  0]
 [ 2 25  3  7  4  0]
 [ 0  9 10  5  7  0]
 [ 5  7  0 27  7  0]
 [ 2  9  7  6 15  0]
 [ 0  1  1  1  2  0]]

2022-12-29 10:18:23,469 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:23,470 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:23,492 - 

2022-12-29 10:18:23,493 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:24,321 - Epoch: [172][   10/  113]    Overall Loss 1.250613    Objective Loss 1.250613                                        LR 0.000013    Time 0.082618    
2022-12-29 10:18:24,961 - Epoch: [172][   20/  113]    Overall Loss 1.199569    Objective Loss 1.199569                                        LR 0.000013    Time 0.073285    
2022-12-29 10:18:25,598 - Epoch: [172][   30/  113]    Overall Loss 1.197513    Objective Loss 1.197513                                        LR 0.000013    Time 0.070087    
2022-12-29 10:18:26,235 - Epoch: [172][   40/  113]    Overall Loss 1.193017    Objective Loss 1.193017                                        LR 0.000013    Time 0.068478    
2022-12-29 10:18:26,869 - Epoch: [172][   50/  113]    Overall Loss 1.223572    Objective Loss 1.223572                                        LR 0.000013    Time 0.067446    
2022-12-29 10:18:27,505 - Epoch: [172][   60/  113]    Overall Loss 1.224750    Objective Loss 1.224750                                        LR 0.000013    Time 0.066793    
2022-12-29 10:18:28,139 - Epoch: [172][   70/  113]    Overall Loss 1.213304    Objective Loss 1.213304                                        LR 0.000013    Time 0.066305    
2022-12-29 10:18:28,767 - Epoch: [172][   80/  113]    Overall Loss 1.228236    Objective Loss 1.228236                                        LR 0.000013    Time 0.065856    
2022-12-29 10:18:29,405 - Epoch: [172][   90/  113]    Overall Loss 1.230205    Objective Loss 1.230205                                        LR 0.000013    Time 0.065632    
2022-12-29 10:18:30,035 - Epoch: [172][  100/  113]    Overall Loss 1.226643    Objective Loss 1.226643                                        LR 0.000013    Time 0.065356    
2022-12-29 10:18:30,660 - Epoch: [172][  110/  113]    Overall Loss 1.230394    Objective Loss 1.230394                                        LR 0.000013    Time 0.065095    
2022-12-29 10:18:30,835 - Epoch: [172][  113/  113]    Overall Loss 1.229375    Objective Loss 1.229375    Top1 75.000000    Top5 100.000000    LR 0.000013    Time 0.064913    
2022-12-29 10:18:30,894 - --- validate (epoch=172)-----------
2022-12-29 10:18:30,894 - 200 samples (16 per mini-batch)
2022-12-29 10:18:31,445 - Epoch: [172][   10/   13]    Loss 1.303192    Top1 48.125000    Top5 96.875000    
2022-12-29 10:18:31,531 - Epoch: [172][   13/   13]    Loss 1.220023    Top1 52.500000    Top5 97.500000    
2022-12-29 10:18:31,589 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.220

2022-12-29 10:18:31,590 - ==> Confusion:
[[20  0  1  4  1  0]
 [ 3 27  4  7  3  0]
 [ 1  6 13  4 13  0]
 [ 7  3  1 30  7  0]
 [ 5  2  6  9 14  0]
 [ 1  2  1  2  2  1]]

2022-12-29 10:18:31,592 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:31,593 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:31,607 - 

2022-12-29 10:18:31,608 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:32,443 - Epoch: [173][   10/  113]    Overall Loss 1.296305    Objective Loss 1.296305                                        LR 0.000013    Time 0.083448    
2022-12-29 10:18:33,076 - Epoch: [173][   20/  113]    Overall Loss 1.240726    Objective Loss 1.240726                                        LR 0.000013    Time 0.073325    
2022-12-29 10:18:33,706 - Epoch: [173][   30/  113]    Overall Loss 1.274719    Objective Loss 1.274719                                        LR 0.000013    Time 0.069885    
2022-12-29 10:18:34,336 - Epoch: [173][   40/  113]    Overall Loss 1.261034    Objective Loss 1.261034                                        LR 0.000013    Time 0.068140    
2022-12-29 10:18:34,968 - Epoch: [173][   50/  113]    Overall Loss 1.266370    Objective Loss 1.266370                                        LR 0.000013    Time 0.067143    
2022-12-29 10:18:35,603 - Epoch: [173][   60/  113]    Overall Loss 1.256561    Objective Loss 1.256561                                        LR 0.000013    Time 0.066523    
2022-12-29 10:18:36,231 - Epoch: [173][   70/  113]    Overall Loss 1.257930    Objective Loss 1.257930                                        LR 0.000013    Time 0.065995    
2022-12-29 10:18:36,860 - Epoch: [173][   80/  113]    Overall Loss 1.267817    Objective Loss 1.267817                                        LR 0.000013    Time 0.065599    
2022-12-29 10:18:37,492 - Epoch: [173][   90/  113]    Overall Loss 1.279388    Objective Loss 1.279388                                        LR 0.000013    Time 0.065324    
2022-12-29 10:18:38,122 - Epoch: [173][  100/  113]    Overall Loss 1.277215    Objective Loss 1.277215                                        LR 0.000013    Time 0.065088    
2022-12-29 10:18:38,751 - Epoch: [173][  110/  113]    Overall Loss 1.281607    Objective Loss 1.281607                                        LR 0.000013    Time 0.064888    
2022-12-29 10:18:38,925 - Epoch: [173][  113/  113]    Overall Loss 1.286916    Objective Loss 1.286916    Top1 41.666667    Top5 91.666667    LR 0.000013    Time 0.064701    
2022-12-29 10:18:38,975 - --- validate (epoch=173)-----------
2022-12-29 10:18:38,976 - 200 samples (16 per mini-batch)
2022-12-29 10:18:39,522 - Epoch: [173][   10/   13]    Loss 1.289265    Top1 51.250000    Top5 95.625000    
2022-12-29 10:18:39,608 - Epoch: [173][   13/   13]    Loss 1.266733    Top1 50.000000    Top5 96.500000    
2022-12-29 10:18:39,662 - ==> Top1: 50.000    Top5: 96.500    Loss: 1.267

2022-12-29 10:18:39,662 - ==> Confusion:
[[19  3  0  7  3  0]
 [ 2 20  4  8 12  0]
 [ 1  5 12  7  7  1]
 [ 4  0  0 33  7  1]
 [ 4  2  4 10 15  2]
 [ 1  1  0  2  2  1]]

2022-12-29 10:18:39,664 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:39,664 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:39,683 - 

2022-12-29 10:18:39,684 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:40,528 - Epoch: [174][   10/  113]    Overall Loss 1.203517    Objective Loss 1.203517                                        LR 0.000013    Time 0.084211    
2022-12-29 10:18:41,164 - Epoch: [174][   20/  113]    Overall Loss 1.200978    Objective Loss 1.200978                                        LR 0.000013    Time 0.073879    
2022-12-29 10:18:41,805 - Epoch: [174][   30/  113]    Overall Loss 1.254625    Objective Loss 1.254625                                        LR 0.000013    Time 0.070631    
2022-12-29 10:18:42,449 - Epoch: [174][   40/  113]    Overall Loss 1.278257    Objective Loss 1.278257                                        LR 0.000013    Time 0.069050    
2022-12-29 10:18:43,086 - Epoch: [174][   50/  113]    Overall Loss 1.291318    Objective Loss 1.291318                                        LR 0.000013    Time 0.067962    
2022-12-29 10:18:43,719 - Epoch: [174][   60/  113]    Overall Loss 1.280398    Objective Loss 1.280398                                        LR 0.000013    Time 0.067188    
2022-12-29 10:18:44,355 - Epoch: [174][   70/  113]    Overall Loss 1.278285    Objective Loss 1.278285                                        LR 0.000013    Time 0.066668    
2022-12-29 10:18:44,990 - Epoch: [174][   80/  113]    Overall Loss 1.271436    Objective Loss 1.271436                                        LR 0.000013    Time 0.066272    
2022-12-29 10:18:45,628 - Epoch: [174][   90/  113]    Overall Loss 1.266801    Objective Loss 1.266801                                        LR 0.000013    Time 0.065982    
2022-12-29 10:18:46,259 - Epoch: [174][  100/  113]    Overall Loss 1.278603    Objective Loss 1.278603                                        LR 0.000013    Time 0.065692    
2022-12-29 10:18:46,888 - Epoch: [174][  110/  113]    Overall Loss 1.263379    Objective Loss 1.263379                                        LR 0.000013    Time 0.065433    
2022-12-29 10:18:47,064 - Epoch: [174][  113/  113]    Overall Loss 1.261001    Objective Loss 1.261001    Top1 50.000000    Top5 100.000000    LR 0.000013    Time 0.065249    
2022-12-29 10:18:47,113 - --- validate (epoch=174)-----------
2022-12-29 10:18:47,113 - 200 samples (16 per mini-batch)
2022-12-29 10:18:47,654 - Epoch: [174][   10/   13]    Loss 1.304663    Top1 47.500000    Top5 98.125000    
2022-12-29 10:18:47,740 - Epoch: [174][   13/   13]    Loss 1.333530    Top1 47.500000    Top5 97.000000    
2022-12-29 10:18:47,801 - ==> Top1: 47.500    Top5: 97.000    Loss: 1.334

2022-12-29 10:18:47,802 - ==> Confusion:
[[24  0  0 12  1  0]
 [ 7 12  4 11  7  0]
 [ 2  5 13  6  4  0]
 [ 7  2  3 31  2  0]
 [ 7  4  2 10 14  0]
 [ 1  2  0  3  3  1]]

2022-12-29 10:18:47,804 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:47,805 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:47,828 - 

2022-12-29 10:18:47,828 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:48,675 - Epoch: [175][   10/  113]    Overall Loss 1.352149    Objective Loss 1.352149                                        LR 0.000006    Time 0.084610    
2022-12-29 10:18:49,315 - Epoch: [175][   20/  113]    Overall Loss 1.278031    Objective Loss 1.278031                                        LR 0.000006    Time 0.074233    
2022-12-29 10:18:49,949 - Epoch: [175][   30/  113]    Overall Loss 1.285596    Objective Loss 1.285596                                        LR 0.000006    Time 0.070632    
2022-12-29 10:18:50,586 - Epoch: [175][   40/  113]    Overall Loss 1.254568    Objective Loss 1.254568                                        LR 0.000006    Time 0.068873    
2022-12-29 10:18:51,222 - Epoch: [175][   50/  113]    Overall Loss 1.248817    Objective Loss 1.248817                                        LR 0.000006    Time 0.067809    
2022-12-29 10:18:51,861 - Epoch: [175][   60/  113]    Overall Loss 1.264627    Objective Loss 1.264627                                        LR 0.000006    Time 0.067156    
2022-12-29 10:18:52,494 - Epoch: [175][   70/  113]    Overall Loss 1.274858    Objective Loss 1.274858                                        LR 0.000006    Time 0.066589    
2022-12-29 10:18:53,124 - Epoch: [175][   80/  113]    Overall Loss 1.271313    Objective Loss 1.271313                                        LR 0.000006    Time 0.066147    
2022-12-29 10:18:53,761 - Epoch: [175][   90/  113]    Overall Loss 1.279573    Objective Loss 1.279573                                        LR 0.000006    Time 0.065863    
2022-12-29 10:18:54,396 - Epoch: [175][  100/  113]    Overall Loss 1.277404    Objective Loss 1.277404                                        LR 0.000006    Time 0.065621    
2022-12-29 10:18:55,023 - Epoch: [175][  110/  113]    Overall Loss 1.283974    Objective Loss 1.283974                                        LR 0.000006    Time 0.065351    
2022-12-29 10:18:55,195 - Epoch: [175][  113/  113]    Overall Loss 1.279588    Objective Loss 1.279588    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065141    
2022-12-29 10:18:55,256 - --- validate (epoch=175)-----------
2022-12-29 10:18:55,257 - 200 samples (16 per mini-batch)
2022-12-29 10:18:55,802 - Epoch: [175][   10/   13]    Loss 1.297472    Top1 51.875000    Top5 97.500000    
2022-12-29 10:18:55,886 - Epoch: [175][   13/   13]    Loss 1.320226    Top1 50.500000    Top5 97.500000    
2022-12-29 10:18:55,944 - ==> Top1: 50.500    Top5: 97.500    Loss: 1.320

2022-12-29 10:18:55,945 - ==> Confusion:
[[17  0  3 12  1  0]
 [ 4 16  1 12  2  0]
 [ 1  2 17  5  8  0]
 [ 9  4  1 39  5  0]
 [ 5  3  3 10 11  0]
 [ 1  2  2  0  3  1]]

2022-12-29 10:18:55,948 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:18:55,949 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:18:55,970 - 

2022-12-29 10:18:55,971 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:18:56,809 - Epoch: [176][   10/  113]    Overall Loss 1.264950    Objective Loss 1.264950                                        LR 0.000006    Time 0.083567    
2022-12-29 10:18:57,451 - Epoch: [176][   20/  113]    Overall Loss 1.286721    Objective Loss 1.286721                                        LR 0.000006    Time 0.073857    
2022-12-29 10:18:58,090 - Epoch: [176][   30/  113]    Overall Loss 1.261617    Objective Loss 1.261617                                        LR 0.000006    Time 0.070537    
2022-12-29 10:18:58,728 - Epoch: [176][   40/  113]    Overall Loss 1.263250    Objective Loss 1.263250                                        LR 0.000006    Time 0.068836    
2022-12-29 10:18:59,361 - Epoch: [176][   50/  113]    Overall Loss 1.229610    Objective Loss 1.229610                                        LR 0.000006    Time 0.067711    
2022-12-29 10:18:59,995 - Epoch: [176][   60/  113]    Overall Loss 1.219059    Objective Loss 1.219059                                        LR 0.000006    Time 0.066990    
2022-12-29 10:19:00,633 - Epoch: [176][   70/  113]    Overall Loss 1.229748    Objective Loss 1.229748                                        LR 0.000006    Time 0.066521    
2022-12-29 10:19:01,263 - Epoch: [176][   80/  113]    Overall Loss 1.235404    Objective Loss 1.235404                                        LR 0.000006    Time 0.066076    
2022-12-29 10:19:01,904 - Epoch: [176][   90/  113]    Overall Loss 1.240470    Objective Loss 1.240470                                        LR 0.000006    Time 0.065852    
2022-12-29 10:19:02,537 - Epoch: [176][  100/  113]    Overall Loss 1.228974    Objective Loss 1.228974                                        LR 0.000006    Time 0.065592    
2022-12-29 10:19:03,167 - Epoch: [176][  110/  113]    Overall Loss 1.230236    Objective Loss 1.230236                                        LR 0.000006    Time 0.065354    
2022-12-29 10:19:03,343 - Epoch: [176][  113/  113]    Overall Loss 1.231392    Objective Loss 1.231392    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065174    
2022-12-29 10:19:03,405 - --- validate (epoch=176)-----------
2022-12-29 10:19:03,406 - 200 samples (16 per mini-batch)
2022-12-29 10:19:03,961 - Epoch: [176][   10/   13]    Loss 1.282035    Top1 46.875000    Top5 98.750000    
2022-12-29 10:19:04,047 - Epoch: [176][   13/   13]    Loss 1.297866    Top1 47.000000    Top5 98.000000    
2022-12-29 10:19:04,090 - ==> Top1: 47.000    Top5: 98.000    Loss: 1.298

2022-12-29 10:19:04,090 - ==> Confusion:
[[21  2  1  6  1  0]
 [ 3 13  4  5  5  0]
 [ 1  7  8  6  5  0]
 [ 1  5  3 33  8  1]
 [ 2  8  6 14 17  0]
 [ 0  5  2  4  1  2]]

2022-12-29 10:19:04,093 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:04,093 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:04,117 - 

2022-12-29 10:19:04,118 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:04,969 - Epoch: [177][   10/  113]    Overall Loss 1.306873    Objective Loss 1.306873                                        LR 0.000006    Time 0.084931    
2022-12-29 10:19:05,608 - Epoch: [177][   20/  113]    Overall Loss 1.256105    Objective Loss 1.256105                                        LR 0.000006    Time 0.074362    
2022-12-29 10:19:06,243 - Epoch: [177][   30/  113]    Overall Loss 1.278056    Objective Loss 1.278056                                        LR 0.000006    Time 0.070737    
2022-12-29 10:19:06,878 - Epoch: [177][   40/  113]    Overall Loss 1.296834    Objective Loss 1.296834                                        LR 0.000006    Time 0.068911    
2022-12-29 10:19:07,509 - Epoch: [177][   50/  113]    Overall Loss 1.282384    Objective Loss 1.282384                                        LR 0.000006    Time 0.067750    
2022-12-29 10:19:08,139 - Epoch: [177][   60/  113]    Overall Loss 1.288442    Objective Loss 1.288442                                        LR 0.000006    Time 0.066940    
2022-12-29 10:19:08,770 - Epoch: [177][   70/  113]    Overall Loss 1.285377    Objective Loss 1.285377                                        LR 0.000006    Time 0.066397    
2022-12-29 10:19:09,402 - Epoch: [177][   80/  113]    Overall Loss 1.302688    Objective Loss 1.302688                                        LR 0.000006    Time 0.065989    
2022-12-29 10:19:10,037 - Epoch: [177][   90/  113]    Overall Loss 1.289517    Objective Loss 1.289517                                        LR 0.000006    Time 0.065706    
2022-12-29 10:19:10,667 - Epoch: [177][  100/  113]    Overall Loss 1.297490    Objective Loss 1.297490                                        LR 0.000006    Time 0.065427    
2022-12-29 10:19:11,295 - Epoch: [177][  110/  113]    Overall Loss 1.291300    Objective Loss 1.291300                                        LR 0.000006    Time 0.065184    
2022-12-29 10:19:11,470 - Epoch: [177][  113/  113]    Overall Loss 1.288772    Objective Loss 1.288772    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.065002    
2022-12-29 10:19:11,527 - --- validate (epoch=177)-----------
2022-12-29 10:19:11,528 - 200 samples (16 per mini-batch)
2022-12-29 10:19:12,073 - Epoch: [177][   10/   13]    Loss 1.329726    Top1 46.250000    Top5 96.250000    
2022-12-29 10:19:12,159 - Epoch: [177][   13/   13]    Loss 1.370189    Top1 45.500000    Top5 96.000000    
2022-12-29 10:19:12,214 - ==> Top1: 45.500    Top5: 96.000    Loss: 1.370

2022-12-29 10:19:12,215 - ==> Confusion:
[[16  4  0  7  4  0]
 [ 4 13  6  9 10  0]
 [ 3  9 14  4  3  0]
 [ 2  2  2 34  2  1]
 [ 1  6  7 11 12  0]
 [ 3  5  0  3  1  2]]

2022-12-29 10:19:12,217 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:12,218 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:12,245 - 

2022-12-29 10:19:12,246 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:13,077 - Epoch: [178][   10/  113]    Overall Loss 1.295195    Objective Loss 1.295195                                        LR 0.000006    Time 0.083010    
2022-12-29 10:19:13,717 - Epoch: [178][   20/  113]    Overall Loss 1.209357    Objective Loss 1.209357                                        LR 0.000006    Time 0.073470    
2022-12-29 10:19:14,356 - Epoch: [178][   30/  113]    Overall Loss 1.212121    Objective Loss 1.212121                                        LR 0.000006    Time 0.070246    
2022-12-29 10:19:15,002 - Epoch: [178][   40/  113]    Overall Loss 1.223444    Objective Loss 1.223444                                        LR 0.000006    Time 0.068829    
2022-12-29 10:19:15,642 - Epoch: [178][   50/  113]    Overall Loss 1.203886    Objective Loss 1.203886                                        LR 0.000006    Time 0.067830    
2022-12-29 10:19:16,277 - Epoch: [178][   60/  113]    Overall Loss 1.215275    Objective Loss 1.215275                                        LR 0.000006    Time 0.067101    
2022-12-29 10:19:16,920 - Epoch: [178][   70/  113]    Overall Loss 1.220824    Objective Loss 1.220824                                        LR 0.000006    Time 0.066694    
2022-12-29 10:19:17,556 - Epoch: [178][   80/  113]    Overall Loss 1.224462    Objective Loss 1.224462                                        LR 0.000006    Time 0.066306    
2022-12-29 10:19:18,197 - Epoch: [178][   90/  113]    Overall Loss 1.235088    Objective Loss 1.235088                                        LR 0.000006    Time 0.066047    
2022-12-29 10:19:18,833 - Epoch: [178][  100/  113]    Overall Loss 1.229521    Objective Loss 1.229521                                        LR 0.000006    Time 0.065802    
2022-12-29 10:19:19,457 - Epoch: [178][  110/  113]    Overall Loss 1.232703    Objective Loss 1.232703                                        LR 0.000006    Time 0.065491    
2022-12-29 10:19:19,634 - Epoch: [178][  113/  113]    Overall Loss 1.237026    Objective Loss 1.237026    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.065314    
2022-12-29 10:19:19,691 - --- validate (epoch=178)-----------
2022-12-29 10:19:19,691 - 200 samples (16 per mini-batch)
2022-12-29 10:19:20,249 - Epoch: [178][   10/   13]    Loss 1.336420    Top1 46.250000    Top5 96.250000    
2022-12-29 10:19:20,333 - Epoch: [178][   13/   13]    Loss 1.353636    Top1 45.500000    Top5 95.500000    
2022-12-29 10:19:20,381 - ==> Top1: 45.500    Top5: 95.500    Loss: 1.354

2022-12-29 10:19:20,382 - ==> Confusion:
[[13  4  3  7  4  1]
 [ 1 15  7 10  6  0]
 [ 2 11 19  2  8  0]
 [ 6  2  2 33  2  0]
 [ 3  6  1  9 11  0]
 [ 4  3  1  1  3  0]]

2022-12-29 10:19:20,385 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:20,385 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:20,399 - 

2022-12-29 10:19:20,399 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:21,235 - Epoch: [179][   10/  113]    Overall Loss 1.273342    Objective Loss 1.273342                                        LR 0.000006    Time 0.083436    
2022-12-29 10:19:21,877 - Epoch: [179][   20/  113]    Overall Loss 1.217874    Objective Loss 1.217874                                        LR 0.000006    Time 0.073797    
2022-12-29 10:19:22,514 - Epoch: [179][   30/  113]    Overall Loss 1.211600    Objective Loss 1.211600                                        LR 0.000006    Time 0.070411    
2022-12-29 10:19:23,154 - Epoch: [179][   40/  113]    Overall Loss 1.228238    Objective Loss 1.228238                                        LR 0.000006    Time 0.068808    
2022-12-29 10:19:23,789 - Epoch: [179][   50/  113]    Overall Loss 1.253858    Objective Loss 1.253858                                        LR 0.000006    Time 0.067731    
2022-12-29 10:19:24,422 - Epoch: [179][   60/  113]    Overall Loss 1.259416    Objective Loss 1.259416                                        LR 0.000006    Time 0.066986    
2022-12-29 10:19:25,056 - Epoch: [179][   70/  113]    Overall Loss 1.254948    Objective Loss 1.254948                                        LR 0.000006    Time 0.066468    
2022-12-29 10:19:25,687 - Epoch: [179][   80/  113]    Overall Loss 1.245490    Objective Loss 1.245490                                        LR 0.000006    Time 0.066045    
2022-12-29 10:19:26,322 - Epoch: [179][   90/  113]    Overall Loss 1.246712    Objective Loss 1.246712                                        LR 0.000006    Time 0.065750    
2022-12-29 10:19:26,957 - Epoch: [179][  100/  113]    Overall Loss 1.249545    Objective Loss 1.249545                                        LR 0.000006    Time 0.065524    
2022-12-29 10:19:27,581 - Epoch: [179][  110/  113]    Overall Loss 1.252184    Objective Loss 1.252184                                        LR 0.000006    Time 0.065237    
2022-12-29 10:19:27,755 - Epoch: [179][  113/  113]    Overall Loss 1.250496    Objective Loss 1.250496    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065040    
2022-12-29 10:19:27,811 - --- validate (epoch=179)-----------
2022-12-29 10:19:27,811 - 200 samples (16 per mini-batch)
2022-12-29 10:19:28,357 - Epoch: [179][   10/   13]    Loss 1.383749    Top1 45.000000    Top5 95.625000    
2022-12-29 10:19:28,441 - Epoch: [179][   13/   13]    Loss 1.395314    Top1 44.000000    Top5 96.500000    
2022-12-29 10:19:28,500 - ==> Top1: 44.000    Top5: 96.500    Loss: 1.395

2022-12-29 10:19:28,501 - ==> Confusion:
[[19  1  2  8  1  0]
 [ 4 20  7  8  5  1]
 [ 1  4 14 18  4  1]
 [11  3  1 27  2  0]
 [ 2  8  3  5  8  1]
 [ 3  3  0  4  1  0]]

2022-12-29 10:19:28,503 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:28,504 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:28,529 - 

2022-12-29 10:19:28,529 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:29,373 - Epoch: [180][   10/  113]    Overall Loss 1.278811    Objective Loss 1.278811                                        LR 0.000006    Time 0.084295    
2022-12-29 10:19:30,011 - Epoch: [180][   20/  113]    Overall Loss 1.255873    Objective Loss 1.255873                                        LR 0.000006    Time 0.074030    
2022-12-29 10:19:30,647 - Epoch: [180][   30/  113]    Overall Loss 1.284801    Objective Loss 1.284801                                        LR 0.000006    Time 0.070524    
2022-12-29 10:19:31,288 - Epoch: [180][   40/  113]    Overall Loss 1.251758    Objective Loss 1.251758                                        LR 0.000006    Time 0.068900    
2022-12-29 10:19:31,925 - Epoch: [180][   50/  113]    Overall Loss 1.269660    Objective Loss 1.269660                                        LR 0.000006    Time 0.067858    
2022-12-29 10:19:32,560 - Epoch: [180][   60/  113]    Overall Loss 1.292896    Objective Loss 1.292896                                        LR 0.000006    Time 0.067120    
2022-12-29 10:19:33,196 - Epoch: [180][   70/  113]    Overall Loss 1.294256    Objective Loss 1.294256                                        LR 0.000006    Time 0.066619    
2022-12-29 10:19:33,831 - Epoch: [180][   80/  113]    Overall Loss 1.296200    Objective Loss 1.296200                                        LR 0.000006    Time 0.066222    
2022-12-29 10:19:34,469 - Epoch: [180][   90/  113]    Overall Loss 1.296665    Objective Loss 1.296665                                        LR 0.000006    Time 0.065940    
2022-12-29 10:19:35,103 - Epoch: [180][  100/  113]    Overall Loss 1.299409    Objective Loss 1.299409                                        LR 0.000006    Time 0.065686    
2022-12-29 10:19:35,726 - Epoch: [180][  110/  113]    Overall Loss 1.300197    Objective Loss 1.300197                                        LR 0.000006    Time 0.065376    
2022-12-29 10:19:35,901 - Epoch: [180][  113/  113]    Overall Loss 1.295452    Objective Loss 1.295452    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065182    
2022-12-29 10:19:35,952 - --- validate (epoch=180)-----------
2022-12-29 10:19:35,952 - 200 samples (16 per mini-batch)
2022-12-29 10:19:36,493 - Epoch: [180][   10/   13]    Loss 1.339978    Top1 45.000000    Top5 97.500000    
2022-12-29 10:19:36,579 - Epoch: [180][   13/   13]    Loss 1.335737    Top1 45.500000    Top5 97.000000    
2022-12-29 10:19:36,632 - ==> Top1: 45.500    Top5: 97.000    Loss: 1.336

2022-12-29 10:19:36,632 - ==> Confusion:
[[22  4  0  8  1  0]
 [ 3 16  2  8  6  2]
 [ 1  7  8  6  6  0]
 [ 7  6  1 26  6  2]
 [ 4  4  3 10 17  0]
 [ 2  3  2  2  3  2]]

2022-12-29 10:19:36,635 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:36,635 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:36,657 - 

2022-12-29 10:19:36,658 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:37,489 - Epoch: [181][   10/  113]    Overall Loss 1.169778    Objective Loss 1.169778                                        LR 0.000006    Time 0.082936    
2022-12-29 10:19:38,121 - Epoch: [181][   20/  113]    Overall Loss 1.226920    Objective Loss 1.226920                                        LR 0.000006    Time 0.073035    
2022-12-29 10:19:38,757 - Epoch: [181][   30/  113]    Overall Loss 1.247590    Objective Loss 1.247590                                        LR 0.000006    Time 0.069868    
2022-12-29 10:19:39,387 - Epoch: [181][   40/  113]    Overall Loss 1.232254    Objective Loss 1.232254                                        LR 0.000006    Time 0.068151    
2022-12-29 10:19:40,018 - Epoch: [181][   50/  113]    Overall Loss 1.231816    Objective Loss 1.231816                                        LR 0.000006    Time 0.067120    
2022-12-29 10:19:40,646 - Epoch: [181][   60/  113]    Overall Loss 1.236559    Objective Loss 1.236559                                        LR 0.000006    Time 0.066393    
2022-12-29 10:19:41,281 - Epoch: [181][   70/  113]    Overall Loss 1.250568    Objective Loss 1.250568                                        LR 0.000006    Time 0.065970    
2022-12-29 10:19:41,927 - Epoch: [181][   80/  113]    Overall Loss 1.250905    Objective Loss 1.250905                                        LR 0.000006    Time 0.065793    
2022-12-29 10:19:42,561 - Epoch: [181][   90/  113]    Overall Loss 1.251072    Objective Loss 1.251072                                        LR 0.000006    Time 0.065526    
2022-12-29 10:19:43,194 - Epoch: [181][  100/  113]    Overall Loss 1.256990    Objective Loss 1.256990                                        LR 0.000006    Time 0.065303    
2022-12-29 10:19:43,824 - Epoch: [181][  110/  113]    Overall Loss 1.268726    Objective Loss 1.268726                                        LR 0.000006    Time 0.065081    
2022-12-29 10:19:43,998 - Epoch: [181][  113/  113]    Overall Loss 1.269125    Objective Loss 1.269125    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.064893    
2022-12-29 10:19:44,058 - --- validate (epoch=181)-----------
2022-12-29 10:19:44,059 - 200 samples (16 per mini-batch)
2022-12-29 10:19:44,608 - Epoch: [181][   10/   13]    Loss 1.281819    Top1 46.875000    Top5 98.750000    
2022-12-29 10:19:44,693 - Epoch: [181][   13/   13]    Loss 1.281593    Top1 48.000000    Top5 98.500000    
2022-12-29 10:19:44,743 - ==> Top1: 48.000    Top5: 98.500    Loss: 1.282

2022-12-29 10:19:44,744 - ==> Confusion:
[[17  0  0  4  1  1]
 [ 3 19  5  5 12  0]
 [ 2  8 10  7  4  0]
 [ 5  4  0 30  3  0]
 [ 5 11  1 13 18  0]
 [ 2  1  1  3  3  2]]

2022-12-29 10:19:44,746 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:44,747 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:44,768 - 

2022-12-29 10:19:44,768 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:45,607 - Epoch: [182][   10/  113]    Overall Loss 1.359997    Objective Loss 1.359997                                        LR 0.000006    Time 0.083769    
2022-12-29 10:19:46,249 - Epoch: [182][   20/  113]    Overall Loss 1.420944    Objective Loss 1.420944                                        LR 0.000006    Time 0.073950    
2022-12-29 10:19:46,890 - Epoch: [182][   30/  113]    Overall Loss 1.390627    Objective Loss 1.390627                                        LR 0.000006    Time 0.070641    
2022-12-29 10:19:47,529 - Epoch: [182][   40/  113]    Overall Loss 1.374467    Objective Loss 1.374467                                        LR 0.000006    Time 0.068966    
2022-12-29 10:19:48,170 - Epoch: [182][   50/  113]    Overall Loss 1.357427    Objective Loss 1.357427                                        LR 0.000006    Time 0.067977    
2022-12-29 10:19:48,808 - Epoch: [182][   60/  113]    Overall Loss 1.353617    Objective Loss 1.353617                                        LR 0.000006    Time 0.067269    
2022-12-29 10:19:49,449 - Epoch: [182][   70/  113]    Overall Loss 1.343344    Objective Loss 1.343344                                        LR 0.000006    Time 0.066805    
2022-12-29 10:19:50,087 - Epoch: [182][   80/  113]    Overall Loss 1.319872    Objective Loss 1.319872                                        LR 0.000006    Time 0.066424    
2022-12-29 10:19:50,725 - Epoch: [182][   90/  113]    Overall Loss 1.316930    Objective Loss 1.316930                                        LR 0.000006    Time 0.066134    
2022-12-29 10:19:51,361 - Epoch: [182][  100/  113]    Overall Loss 1.305804    Objective Loss 1.305804                                        LR 0.000006    Time 0.065876    
2022-12-29 10:19:52,005 - Epoch: [182][  110/  113]    Overall Loss 1.305927    Objective Loss 1.305927                                        LR 0.000006    Time 0.065737    
2022-12-29 10:19:52,178 - Epoch: [182][  113/  113]    Overall Loss 1.312892    Objective Loss 1.312892    Top1 33.333333    Top5 91.666667    LR 0.000006    Time 0.065518    
2022-12-29 10:19:52,238 - --- validate (epoch=182)-----------
2022-12-29 10:19:52,238 - 200 samples (16 per mini-batch)
2022-12-29 10:19:52,788 - Epoch: [182][   10/   13]    Loss 1.342560    Top1 49.375000    Top5 96.875000    
2022-12-29 10:19:52,872 - Epoch: [182][   13/   13]    Loss 1.273047    Top1 52.500000    Top5 97.500000    
2022-12-29 10:19:52,935 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.273

2022-12-29 10:19:52,936 - ==> Confusion:
[[19  4  1  8  4  1]
 [ 1 21  5  4  4  0]
 [ 1  9 11  7 10  0]
 [ 5  2  3 25  4  0]
 [ 2  5  3  6 28  0]
 [ 0  3  1  1  1  1]]

2022-12-29 10:19:52,939 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:19:52,940 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:19:52,962 - 

2022-12-29 10:19:52,962 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:19:53,787 - Epoch: [183][   10/  113]    Overall Loss 1.259586    Objective Loss 1.259586                                        LR 0.000006    Time 0.082349    
2022-12-29 10:19:54,426 - Epoch: [183][   20/  113]    Overall Loss 1.277891    Objective Loss 1.277891                                        LR 0.000006    Time 0.073114    
2022-12-29 10:19:55,064 - Epoch: [183][   30/  113]    Overall Loss 1.293559    Objective Loss 1.293559                                        LR 0.000006    Time 0.069983    
2022-12-29 10:19:55,706 - Epoch: [183][   40/  113]    Overall Loss 1.294943    Objective Loss 1.294943                                        LR 0.000006    Time 0.068520    
2022-12-29 10:19:56,342 - Epoch: [183][   50/  113]    Overall Loss 1.288827    Objective Loss 1.288827                                        LR 0.000006    Time 0.067537    
2022-12-29 10:19:56,980 - Epoch: [183][   60/  113]    Overall Loss 1.294562    Objective Loss 1.294562                                        LR 0.000006    Time 0.066911    
2022-12-29 10:19:57,617 - Epoch: [183][   70/  113]    Overall Loss 1.285726    Objective Loss 1.285726                                        LR 0.000006    Time 0.066432    
2022-12-29 10:19:58,252 - Epoch: [183][   80/  113]    Overall Loss 1.280555    Objective Loss 1.280555                                        LR 0.000006    Time 0.066062    
2022-12-29 10:19:58,891 - Epoch: [183][   90/  113]    Overall Loss 1.274122    Objective Loss 1.274122                                        LR 0.000006    Time 0.065822    
2022-12-29 10:19:59,525 - Epoch: [183][  100/  113]    Overall Loss 1.283158    Objective Loss 1.283158                                        LR 0.000006    Time 0.065570    
2022-12-29 10:20:00,150 - Epoch: [183][  110/  113]    Overall Loss 1.275037    Objective Loss 1.275037                                        LR 0.000006    Time 0.065290    
2022-12-29 10:20:00,321 - Epoch: [183][  113/  113]    Overall Loss 1.276841    Objective Loss 1.276841    Top1 41.666667    Top5 91.666667    LR 0.000006    Time 0.065068    
2022-12-29 10:20:00,365 - --- validate (epoch=183)-----------
2022-12-29 10:20:00,365 - 200 samples (16 per mini-batch)
2022-12-29 10:20:00,920 - Epoch: [183][   10/   13]    Loss 1.282590    Top1 53.750000    Top5 96.875000    
2022-12-29 10:20:01,008 - Epoch: [183][   13/   13]    Loss 1.256136    Top1 53.500000    Top5 97.500000    
2022-12-29 10:20:01,060 - ==> Top1: 53.500    Top5: 97.500    Loss: 1.256

2022-12-29 10:20:01,060 - ==> Confusion:
[[19  2  0  6  3  0]
 [ 2 28  2 10  5  0]
 [ 1  5 11  6  4  0]
 [ 4  6  3 41  0  1]
 [ 3 14  2  3  8  2]
 [ 0  5  1  2  1  0]]

2022-12-29 10:20:01,063 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:01,063 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:01,075 - 

2022-12-29 10:20:01,075 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:01,922 - Epoch: [184][   10/  113]    Overall Loss 1.351676    Objective Loss 1.351676                                        LR 0.000006    Time 0.084663    
2022-12-29 10:20:02,563 - Epoch: [184][   20/  113]    Overall Loss 1.325401    Objective Loss 1.325401                                        LR 0.000006    Time 0.074314    
2022-12-29 10:20:03,201 - Epoch: [184][   30/  113]    Overall Loss 1.319235    Objective Loss 1.319235                                        LR 0.000006    Time 0.070809    
2022-12-29 10:20:03,840 - Epoch: [184][   40/  113]    Overall Loss 1.313081    Objective Loss 1.313081                                        LR 0.000006    Time 0.069062    
2022-12-29 10:20:04,475 - Epoch: [184][   50/  113]    Overall Loss 1.295210    Objective Loss 1.295210                                        LR 0.000006    Time 0.067946    
2022-12-29 10:20:05,111 - Epoch: [184][   60/  113]    Overall Loss 1.302969    Objective Loss 1.302969                                        LR 0.000006    Time 0.067211    
2022-12-29 10:20:05,744 - Epoch: [184][   70/  113]    Overall Loss 1.305911    Objective Loss 1.305911                                        LR 0.000006    Time 0.066648    
2022-12-29 10:20:06,380 - Epoch: [184][   80/  113]    Overall Loss 1.292937    Objective Loss 1.292937                                        LR 0.000006    Time 0.066259    
2022-12-29 10:20:07,018 - Epoch: [184][   90/  113]    Overall Loss 1.284191    Objective Loss 1.284191                                        LR 0.000006    Time 0.065975    
2022-12-29 10:20:07,655 - Epoch: [184][  100/  113]    Overall Loss 1.278940    Objective Loss 1.278940                                        LR 0.000006    Time 0.065751    
2022-12-29 10:20:08,287 - Epoch: [184][  110/  113]    Overall Loss 1.287661    Objective Loss 1.287661                                        LR 0.000006    Time 0.065511    
2022-12-29 10:20:08,461 - Epoch: [184][  113/  113]    Overall Loss 1.287289    Objective Loss 1.287289    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.065304    
2022-12-29 10:20:08,526 - --- validate (epoch=184)-----------
2022-12-29 10:20:08,526 - 200 samples (16 per mini-batch)
2022-12-29 10:20:09,080 - Epoch: [184][   10/   13]    Loss 1.350853    Top1 49.375000    Top5 97.500000    
2022-12-29 10:20:09,166 - Epoch: [184][   13/   13]    Loss 1.362485    Top1 47.500000    Top5 97.500000    
2022-12-29 10:20:09,230 - ==> Top1: 47.500    Top5: 97.500    Loss: 1.362

2022-12-29 10:20:09,230 - ==> Confusion:
[[18 12  1  3  3  0]
 [ 2 25  4  6  5  0]
 [ 0 13  7  4  6  0]
 [ 3  7  1 29  3  0]
 [ 1 10  2  9 15  0]
 [ 3  2  1  2  2  1]]

2022-12-29 10:20:09,233 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:09,234 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:09,256 - 

2022-12-29 10:20:09,257 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:10,090 - Epoch: [185][   10/  113]    Overall Loss 1.309198    Objective Loss 1.309198                                        LR 0.000006    Time 0.083261    
2022-12-29 10:20:10,725 - Epoch: [185][   20/  113]    Overall Loss 1.297871    Objective Loss 1.297871                                        LR 0.000006    Time 0.073354    
2022-12-29 10:20:11,369 - Epoch: [185][   30/  113]    Overall Loss 1.286122    Objective Loss 1.286122                                        LR 0.000006    Time 0.070327    
2022-12-29 10:20:12,005 - Epoch: [185][   40/  113]    Overall Loss 1.244435    Objective Loss 1.244435                                        LR 0.000006    Time 0.068630    
2022-12-29 10:20:12,637 - Epoch: [185][   50/  113]    Overall Loss 1.243148    Objective Loss 1.243148                                        LR 0.000006    Time 0.067542    
2022-12-29 10:20:13,268 - Epoch: [185][   60/  113]    Overall Loss 1.267667    Objective Loss 1.267667                                        LR 0.000006    Time 0.066794    
2022-12-29 10:20:13,899 - Epoch: [185][   70/  113]    Overall Loss 1.259081    Objective Loss 1.259081                                        LR 0.000006    Time 0.066262    
2022-12-29 10:20:14,534 - Epoch: [185][   80/  113]    Overall Loss 1.255539    Objective Loss 1.255539                                        LR 0.000006    Time 0.065904    
2022-12-29 10:20:15,171 - Epoch: [185][   90/  113]    Overall Loss 1.261202    Objective Loss 1.261202                                        LR 0.000006    Time 0.065653    
2022-12-29 10:20:15,807 - Epoch: [185][  100/  113]    Overall Loss 1.243326    Objective Loss 1.243326                                        LR 0.000006    Time 0.065444    
2022-12-29 10:20:16,433 - Epoch: [185][  110/  113]    Overall Loss 1.253975    Objective Loss 1.253975                                        LR 0.000006    Time 0.065188    
2022-12-29 10:20:16,612 - Epoch: [185][  113/  113]    Overall Loss 1.252407    Objective Loss 1.252407    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065036    
2022-12-29 10:20:16,664 - --- validate (epoch=185)-----------
2022-12-29 10:20:16,664 - 200 samples (16 per mini-batch)
2022-12-29 10:20:17,210 - Epoch: [185][   10/   13]    Loss 1.211519    Top1 53.125000    Top5 98.750000    
2022-12-29 10:20:17,296 - Epoch: [185][   13/   13]    Loss 1.251890    Top1 50.000000    Top5 97.500000    
2022-12-29 10:20:17,355 - ==> Top1: 50.000    Top5: 97.500    Loss: 1.252

2022-12-29 10:20:17,355 - ==> Confusion:
[[15  3  1  4  4  0]
 [ 1 27  1  4  9  0]
 [ 1 10 13  4 12  0]
 [ 0  5  3 34  6  0]
 [ 2  9  6  3 11  0]
 [ 1  1  2  2  6  0]]

2022-12-29 10:20:17,359 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:17,359 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:17,372 - 

2022-12-29 10:20:17,372 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:18,215 - Epoch: [186][   10/  113]    Overall Loss 1.390484    Objective Loss 1.390484                                        LR 0.000006    Time 0.084206    
2022-12-29 10:20:18,849 - Epoch: [186][   20/  113]    Overall Loss 1.310879    Objective Loss 1.310879                                        LR 0.000006    Time 0.073748    
2022-12-29 10:20:19,482 - Epoch: [186][   30/  113]    Overall Loss 1.299531    Objective Loss 1.299531                                        LR 0.000006    Time 0.070244    
2022-12-29 10:20:20,115 - Epoch: [186][   40/  113]    Overall Loss 1.300076    Objective Loss 1.300076                                        LR 0.000006    Time 0.068492    
2022-12-29 10:20:20,744 - Epoch: [186][   50/  113]    Overall Loss 1.294613    Objective Loss 1.294613                                        LR 0.000006    Time 0.067369    
2022-12-29 10:20:21,380 - Epoch: [186][   60/  113]    Overall Loss 1.284681    Objective Loss 1.284681                                        LR 0.000006    Time 0.066737    
2022-12-29 10:20:22,019 - Epoch: [186][   70/  113]    Overall Loss 1.287323    Objective Loss 1.287323                                        LR 0.000006    Time 0.066316    
2022-12-29 10:20:22,648 - Epoch: [186][   80/  113]    Overall Loss 1.300704    Objective Loss 1.300704                                        LR 0.000006    Time 0.065885    
2022-12-29 10:20:23,278 - Epoch: [186][   90/  113]    Overall Loss 1.306850    Objective Loss 1.306850                                        LR 0.000006    Time 0.065563    
2022-12-29 10:20:23,909 - Epoch: [186][  100/  113]    Overall Loss 1.311343    Objective Loss 1.311343                                        LR 0.000006    Time 0.065311    
2022-12-29 10:20:24,540 - Epoch: [186][  110/  113]    Overall Loss 1.313967    Objective Loss 1.313967                                        LR 0.000006    Time 0.065104    
2022-12-29 10:20:24,712 - Epoch: [186][  113/  113]    Overall Loss 1.315192    Objective Loss 1.315192    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064898    
2022-12-29 10:20:24,772 - --- validate (epoch=186)-----------
2022-12-29 10:20:24,773 - 200 samples (16 per mini-batch)
2022-12-29 10:20:25,319 - Epoch: [186][   10/   13]    Loss 1.318838    Top1 45.000000    Top5 98.750000    
2022-12-29 10:20:25,404 - Epoch: [186][   13/   13]    Loss 1.353748    Top1 46.000000    Top5 98.000000    
2022-12-29 10:20:25,468 - ==> Top1: 46.000    Top5: 98.000    Loss: 1.354

2022-12-29 10:20:25,468 - ==> Confusion:
[[16  5  0  6  2  0]
 [ 2 23  0  9  7  1]
 [ 1  8 15  5 10  0]
 [ 6  2  1 23  6  0]
 [ 3 14  0  8 14  0]
 [ 1  3  0  4  4  1]]

2022-12-29 10:20:25,470 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:25,471 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:25,493 - 

2022-12-29 10:20:25,494 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:26,341 - Epoch: [187][   10/  113]    Overall Loss 1.189382    Objective Loss 1.189382                                        LR 0.000006    Time 0.084560    
2022-12-29 10:20:26,979 - Epoch: [187][   20/  113]    Overall Loss 1.199475    Objective Loss 1.199475                                        LR 0.000006    Time 0.074137    
2022-12-29 10:20:27,617 - Epoch: [187][   30/  113]    Overall Loss 1.238242    Objective Loss 1.238242                                        LR 0.000006    Time 0.070673    
2022-12-29 10:20:28,254 - Epoch: [187][   40/  113]    Overall Loss 1.257051    Objective Loss 1.257051                                        LR 0.000006    Time 0.068925    
2022-12-29 10:20:28,890 - Epoch: [187][   50/  113]    Overall Loss 1.260467    Objective Loss 1.260467                                        LR 0.000006    Time 0.067856    
2022-12-29 10:20:29,527 - Epoch: [187][   60/  113]    Overall Loss 1.257710    Objective Loss 1.257710                                        LR 0.000006    Time 0.067147    
2022-12-29 10:20:30,163 - Epoch: [187][   70/  113]    Overall Loss 1.258736    Objective Loss 1.258736                                        LR 0.000006    Time 0.066639    
2022-12-29 10:20:30,797 - Epoch: [187][   80/  113]    Overall Loss 1.257529    Objective Loss 1.257529                                        LR 0.000006    Time 0.066226    
2022-12-29 10:20:31,435 - Epoch: [187][   90/  113]    Overall Loss 1.248263    Objective Loss 1.248263                                        LR 0.000006    Time 0.065948    
2022-12-29 10:20:32,071 - Epoch: [187][  100/  113]    Overall Loss 1.252248    Objective Loss 1.252248                                        LR 0.000006    Time 0.065713    
2022-12-29 10:20:32,699 - Epoch: [187][  110/  113]    Overall Loss 1.250746    Objective Loss 1.250746                                        LR 0.000006    Time 0.065443    
2022-12-29 10:20:32,878 - Epoch: [187][  113/  113]    Overall Loss 1.251280    Objective Loss 1.251280    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065282    
2022-12-29 10:20:32,935 - --- validate (epoch=187)-----------
2022-12-29 10:20:32,935 - 200 samples (16 per mini-batch)
2022-12-29 10:20:33,485 - Epoch: [187][   10/   13]    Loss 1.421977    Top1 38.750000    Top5 98.125000    
2022-12-29 10:20:33,569 - Epoch: [187][   13/   13]    Loss 1.332559    Top1 43.500000    Top5 98.500000    
2022-12-29 10:20:33,617 - ==> Top1: 43.500    Top5: 98.500    Loss: 1.333

2022-12-29 10:20:33,618 - ==> Confusion:
[[14  3  1 11  3  0]
 [ 3 13  3 11 12  0]
 [ 0  7  7  4  8  0]
 [ 5  2  5 37  4  1]
 [ 3  4  1 15 16  0]
 [ 1  2  1  0  3  0]]

2022-12-29 10:20:33,620 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:33,621 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:33,643 - 

2022-12-29 10:20:33,643 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:34,496 - Epoch: [188][   10/  113]    Overall Loss 1.237190    Objective Loss 1.237190                                        LR 0.000006    Time 0.085195    
2022-12-29 10:20:35,131 - Epoch: [188][   20/  113]    Overall Loss 1.255761    Objective Loss 1.255761                                        LR 0.000006    Time 0.074331    
2022-12-29 10:20:35,770 - Epoch: [188][   30/  113]    Overall Loss 1.277680    Objective Loss 1.277680                                        LR 0.000006    Time 0.070838    
2022-12-29 10:20:36,408 - Epoch: [188][   40/  113]    Overall Loss 1.292351    Objective Loss 1.292351                                        LR 0.000006    Time 0.069068    
2022-12-29 10:20:37,043 - Epoch: [188][   50/  113]    Overall Loss 1.286219    Objective Loss 1.286219                                        LR 0.000006    Time 0.067936    
2022-12-29 10:20:37,678 - Epoch: [188][   60/  113]    Overall Loss 1.284008    Objective Loss 1.284008                                        LR 0.000006    Time 0.067190    
2022-12-29 10:20:38,319 - Epoch: [188][   70/  113]    Overall Loss 1.276157    Objective Loss 1.276157                                        LR 0.000006    Time 0.066736    
2022-12-29 10:20:38,951 - Epoch: [188][   80/  113]    Overall Loss 1.258204    Objective Loss 1.258204                                        LR 0.000006    Time 0.066294    
2022-12-29 10:20:39,589 - Epoch: [188][   90/  113]    Overall Loss 1.250584    Objective Loss 1.250584                                        LR 0.000006    Time 0.066017    
2022-12-29 10:20:40,221 - Epoch: [188][  100/  113]    Overall Loss 1.259824    Objective Loss 1.259824                                        LR 0.000006    Time 0.065730    
2022-12-29 10:20:40,852 - Epoch: [188][  110/  113]    Overall Loss 1.262123    Objective Loss 1.262123                                        LR 0.000006    Time 0.065486    
2022-12-29 10:20:41,024 - Epoch: [188][  113/  113]    Overall Loss 1.264552    Objective Loss 1.264552    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065264    
2022-12-29 10:20:41,079 - --- validate (epoch=188)-----------
2022-12-29 10:20:41,079 - 200 samples (16 per mini-batch)
2022-12-29 10:20:41,641 - Epoch: [188][   10/   13]    Loss 1.273574    Top1 49.375000    Top5 96.875000    
2022-12-29 10:20:41,728 - Epoch: [188][   13/   13]    Loss 1.244769    Top1 52.000000    Top5 97.000000    
2022-12-29 10:20:41,777 - ==> Top1: 52.000    Top5: 97.000    Loss: 1.245

2022-12-29 10:20:41,777 - ==> Confusion:
[[20  2  1  2  3  0]
 [ 2 19  4  5  6  1]
 [ 4  6 15  2  4  0]
 [ 8  2  3 32  5  1]
 [ 5 12  5  5 17  1]
 [ 2  1  1  0  3  1]]

2022-12-29 10:20:41,779 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:41,779 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:41,798 - 

2022-12-29 10:20:41,798 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:42,637 - Epoch: [189][   10/  113]    Overall Loss 1.352150    Objective Loss 1.352150                                        LR 0.000006    Time 0.083840    
2022-12-29 10:20:43,276 - Epoch: [189][   20/  113]    Overall Loss 1.325774    Objective Loss 1.325774                                        LR 0.000006    Time 0.073819    
2022-12-29 10:20:43,915 - Epoch: [189][   30/  113]    Overall Loss 1.318169    Objective Loss 1.318169                                        LR 0.000006    Time 0.070492    
2022-12-29 10:20:44,552 - Epoch: [189][   40/  113]    Overall Loss 1.315359    Objective Loss 1.315359                                        LR 0.000006    Time 0.068779    
2022-12-29 10:20:45,184 - Epoch: [189][   50/  113]    Overall Loss 1.293938    Objective Loss 1.293938                                        LR 0.000006    Time 0.067654    
2022-12-29 10:20:45,815 - Epoch: [189][   60/  113]    Overall Loss 1.286060    Objective Loss 1.286060                                        LR 0.000006    Time 0.066892    
2022-12-29 10:20:46,447 - Epoch: [189][   70/  113]    Overall Loss 1.269129    Objective Loss 1.269129                                        LR 0.000006    Time 0.066355    
2022-12-29 10:20:47,080 - Epoch: [189][   80/  113]    Overall Loss 1.267181    Objective Loss 1.267181                                        LR 0.000006    Time 0.065967    
2022-12-29 10:20:47,711 - Epoch: [189][   90/  113]    Overall Loss 1.269249    Objective Loss 1.269249                                        LR 0.000006    Time 0.065636    
2022-12-29 10:20:48,345 - Epoch: [189][  100/  113]    Overall Loss 1.260624    Objective Loss 1.260624                                        LR 0.000006    Time 0.065410    
2022-12-29 10:20:48,968 - Epoch: [189][  110/  113]    Overall Loss 1.263764    Objective Loss 1.263764                                        LR 0.000006    Time 0.065127    
2022-12-29 10:20:49,141 - Epoch: [189][  113/  113]    Overall Loss 1.267207    Objective Loss 1.267207    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064920    
2022-12-29 10:20:49,200 - --- validate (epoch=189)-----------
2022-12-29 10:20:49,201 - 200 samples (16 per mini-batch)
2022-12-29 10:20:49,754 - Epoch: [189][   10/   13]    Loss 1.263272    Top1 51.250000    Top5 97.500000    
2022-12-29 10:20:49,838 - Epoch: [189][   13/   13]    Loss 1.254958    Top1 51.500000    Top5 96.500000    
2022-12-29 10:20:49,897 - ==> Top1: 51.500    Top5: 96.500    Loss: 1.255

2022-12-29 10:20:49,897 - ==> Confusion:
[[30  2  0  3  3  1]
 [ 5 13  2  5  6  0]
 [ 2 11 14  7  6  0]
 [ 2  1  3 29  7  1]
 [ 4  3  4  9 13  0]
 [ 2  2  1  1  4  4]]

2022-12-29 10:20:49,899 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:49,899 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:49,911 - 

2022-12-29 10:20:49,911 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:50,761 - Epoch: [190][   10/  113]    Overall Loss 1.292805    Objective Loss 1.292805                                        LR 0.000006    Time 0.084890    
2022-12-29 10:20:51,402 - Epoch: [190][   20/  113]    Overall Loss 1.268213    Objective Loss 1.268213                                        LR 0.000006    Time 0.074452    
2022-12-29 10:20:52,045 - Epoch: [190][   30/  113]    Overall Loss 1.260646    Objective Loss 1.260646                                        LR 0.000006    Time 0.071050    
2022-12-29 10:20:52,685 - Epoch: [190][   40/  113]    Overall Loss 1.248167    Objective Loss 1.248167                                        LR 0.000006    Time 0.069268    
2022-12-29 10:20:53,320 - Epoch: [190][   50/  113]    Overall Loss 1.242986    Objective Loss 1.242986                                        LR 0.000006    Time 0.068107    
2022-12-29 10:20:53,951 - Epoch: [190][   60/  113]    Overall Loss 1.258821    Objective Loss 1.258821                                        LR 0.000006    Time 0.067279    
2022-12-29 10:20:54,583 - Epoch: [190][   70/  113]    Overall Loss 1.265949    Objective Loss 1.265949                                        LR 0.000006    Time 0.066684    
2022-12-29 10:20:55,217 - Epoch: [190][   80/  113]    Overall Loss 1.250133    Objective Loss 1.250133                                        LR 0.000006    Time 0.066270    
2022-12-29 10:20:55,857 - Epoch: [190][   90/  113]    Overall Loss 1.253802    Objective Loss 1.253802                                        LR 0.000006    Time 0.066008    
2022-12-29 10:20:56,495 - Epoch: [190][  100/  113]    Overall Loss 1.263727    Objective Loss 1.263727                                        LR 0.000006    Time 0.065788    
2022-12-29 10:20:57,120 - Epoch: [190][  110/  113]    Overall Loss 1.263914    Objective Loss 1.263914                                        LR 0.000006    Time 0.065477    
2022-12-29 10:20:57,292 - Epoch: [190][  113/  113]    Overall Loss 1.267810    Objective Loss 1.267810    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.065263    
2022-12-29 10:20:57,353 - --- validate (epoch=190)-----------
2022-12-29 10:20:57,354 - 200 samples (16 per mini-batch)
2022-12-29 10:20:57,905 - Epoch: [190][   10/   13]    Loss 1.293894    Top1 49.375000    Top5 97.500000    
2022-12-29 10:20:57,990 - Epoch: [190][   13/   13]    Loss 1.264630    Top1 49.500000    Top5 97.500000    
2022-12-29 10:20:58,045 - ==> Top1: 49.500    Top5: 97.500    Loss: 1.265

2022-12-29 10:20:58,045 - ==> Confusion:
[[25  1  0  7  3  0]
 [ 1 17  1  7 11  0]
 [ 2  9 13  3  6  0]
 [ 4  3  1 35  1  0]
 [ 0 15  2  9  8  1]
 [ 2  4  1  3  4  1]]

2022-12-29 10:20:58,050 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:20:58,050 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:20:58,065 - 

2022-12-29 10:20:58,065 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:20:58,910 - Epoch: [191][   10/  113]    Overall Loss 1.301612    Objective Loss 1.301612                                        LR 0.000006    Time 0.084386    
2022-12-29 10:20:59,548 - Epoch: [191][   20/  113]    Overall Loss 1.288662    Objective Loss 1.288662                                        LR 0.000006    Time 0.074070    
2022-12-29 10:21:00,188 - Epoch: [191][   30/  113]    Overall Loss 1.287780    Objective Loss 1.287780                                        LR 0.000006    Time 0.070696    
2022-12-29 10:21:00,824 - Epoch: [191][   40/  113]    Overall Loss 1.297365    Objective Loss 1.297365                                        LR 0.000006    Time 0.068912    
2022-12-29 10:21:01,463 - Epoch: [191][   50/  113]    Overall Loss 1.278952    Objective Loss 1.278952                                        LR 0.000006    Time 0.067888    
2022-12-29 10:21:02,102 - Epoch: [191][   60/  113]    Overall Loss 1.300390    Objective Loss 1.300390                                        LR 0.000006    Time 0.067228    
2022-12-29 10:21:02,741 - Epoch: [191][   70/  113]    Overall Loss 1.295245    Objective Loss 1.295245                                        LR 0.000006    Time 0.066734    
2022-12-29 10:21:03,377 - Epoch: [191][   80/  113]    Overall Loss 1.288600    Objective Loss 1.288600                                        LR 0.000006    Time 0.066346    
2022-12-29 10:21:04,018 - Epoch: [191][   90/  113]    Overall Loss 1.289370    Objective Loss 1.289370                                        LR 0.000006    Time 0.066084    
2022-12-29 10:21:04,654 - Epoch: [191][  100/  113]    Overall Loss 1.282866    Objective Loss 1.282866                                        LR 0.000006    Time 0.065834    
2022-12-29 10:21:05,289 - Epoch: [191][  110/  113]    Overall Loss 1.277765    Objective Loss 1.277765                                        LR 0.000006    Time 0.065613    
2022-12-29 10:21:05,464 - Epoch: [191][  113/  113]    Overall Loss 1.278059    Objective Loss 1.278059    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.065422    
2022-12-29 10:21:05,517 - --- validate (epoch=191)-----------
2022-12-29 10:21:05,517 - 200 samples (16 per mini-batch)
2022-12-29 10:21:06,058 - Epoch: [191][   10/   13]    Loss 1.247897    Top1 50.000000    Top5 95.000000    
2022-12-29 10:21:06,141 - Epoch: [191][   13/   13]    Loss 1.252734    Top1 48.500000    Top5 95.000000    
2022-12-29 10:21:06,189 - ==> Top1: 48.500    Top5: 95.000    Loss: 1.253

2022-12-29 10:21:06,190 - ==> Confusion:
[[24  4  2  5  3  0]
 [ 4 17  2  6  8  0]
 [ 1 15  9  3 10  0]
 [ 0  3  3 34  4  1]
 [ 3  7  4  8 13  0]
 [ 1  3  0  1  2  0]]

2022-12-29 10:21:06,191 - ==> Best [Top1: 55.500   Top5: 98.500   Sparsity:0.00   Params: 289215 on epoch: 168]
2022-12-29 10:21:06,191 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:06,209 - 

2022-12-29 10:21:06,209 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:07,054 - Epoch: [192][   10/  113]    Overall Loss 1.383505    Objective Loss 1.383505                                        LR 0.000006    Time 0.084385    
2022-12-29 10:21:07,693 - Epoch: [192][   20/  113]    Overall Loss 1.365389    Objective Loss 1.365389                                        LR 0.000006    Time 0.074113    
2022-12-29 10:21:08,335 - Epoch: [192][   30/  113]    Overall Loss 1.348230    Objective Loss 1.348230                                        LR 0.000006    Time 0.070784    
2022-12-29 10:21:08,976 - Epoch: [192][   40/  113]    Overall Loss 1.329196    Objective Loss 1.329196                                        LR 0.000006    Time 0.069111    
2022-12-29 10:21:09,616 - Epoch: [192][   50/  113]    Overall Loss 1.317638    Objective Loss 1.317638                                        LR 0.000006    Time 0.068067    
2022-12-29 10:21:10,249 - Epoch: [192][   60/  113]    Overall Loss 1.329713    Objective Loss 1.329713                                        LR 0.000006    Time 0.067268    
2022-12-29 10:21:10,885 - Epoch: [192][   70/  113]    Overall Loss 1.316947    Objective Loss 1.316947                                        LR 0.000006    Time 0.066737    
2022-12-29 10:21:11,521 - Epoch: [192][   80/  113]    Overall Loss 1.310292    Objective Loss 1.310292                                        LR 0.000006    Time 0.066345    
2022-12-29 10:21:12,161 - Epoch: [192][   90/  113]    Overall Loss 1.316267    Objective Loss 1.316267                                        LR 0.000006    Time 0.066072    
2022-12-29 10:21:12,790 - Epoch: [192][  100/  113]    Overall Loss 1.304719    Objective Loss 1.304719                                        LR 0.000006    Time 0.065758    
2022-12-29 10:21:13,420 - Epoch: [192][  110/  113]    Overall Loss 1.295105    Objective Loss 1.295105                                        LR 0.000006    Time 0.065498    
2022-12-29 10:21:13,591 - Epoch: [192][  113/  113]    Overall Loss 1.295033    Objective Loss 1.295033    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065267    
2022-12-29 10:21:13,651 - --- validate (epoch=192)-----------
2022-12-29 10:21:13,651 - 200 samples (16 per mini-batch)
2022-12-29 10:21:14,202 - Epoch: [192][   10/   13]    Loss 1.216424    Top1 56.875000    Top5 100.000000    
2022-12-29 10:21:14,288 - Epoch: [192][   13/   13]    Loss 1.229276    Top1 55.500000    Top5 100.000000    
2022-12-29 10:21:14,348 - ==> Top1: 55.500    Top5: 100.000    Loss: 1.229

2022-12-29 10:21:14,348 - ==> Confusion:
[[13  1  1  7  3  0]
 [ 1 25  5  6  7  1]
 [ 0  8  9  5  8  0]
 [ 3  6  0 39  5  0]
 [ 3  5  0  7 24  0]
 [ 1  0  2  2  2  1]]

2022-12-29 10:21:14,351 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:21:14,351 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:14,386 - 

2022-12-29 10:21:14,386 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:15,235 - Epoch: [193][   10/  113]    Overall Loss 1.253819    Objective Loss 1.253819                                        LR 0.000006    Time 0.084610    
2022-12-29 10:21:15,872 - Epoch: [193][   20/  113]    Overall Loss 1.187857    Objective Loss 1.187857                                        LR 0.000006    Time 0.074163    
2022-12-29 10:21:16,509 - Epoch: [193][   30/  113]    Overall Loss 1.258365    Objective Loss 1.258365                                        LR 0.000006    Time 0.070650    
2022-12-29 10:21:17,141 - Epoch: [193][   40/  113]    Overall Loss 1.243105    Objective Loss 1.243105                                        LR 0.000006    Time 0.068788    
2022-12-29 10:21:17,777 - Epoch: [193][   50/  113]    Overall Loss 1.261180    Objective Loss 1.261180                                        LR 0.000006    Time 0.067734    
2022-12-29 10:21:18,416 - Epoch: [193][   60/  113]    Overall Loss 1.270023    Objective Loss 1.270023                                        LR 0.000006    Time 0.067086    
2022-12-29 10:21:19,061 - Epoch: [193][   70/  113]    Overall Loss 1.270859    Objective Loss 1.270859                                        LR 0.000006    Time 0.066641    
2022-12-29 10:21:19,698 - Epoch: [193][   80/  113]    Overall Loss 1.270551    Objective Loss 1.270551                                        LR 0.000006    Time 0.066269    
2022-12-29 10:21:20,336 - Epoch: [193][   90/  113]    Overall Loss 1.270808    Objective Loss 1.270808                                        LR 0.000006    Time 0.065994    
2022-12-29 10:21:20,976 - Epoch: [193][  100/  113]    Overall Loss 1.273809    Objective Loss 1.273809                                        LR 0.000006    Time 0.065784    
2022-12-29 10:21:21,605 - Epoch: [193][  110/  113]    Overall Loss 1.266534    Objective Loss 1.266534                                        LR 0.000006    Time 0.065526    
2022-12-29 10:21:21,779 - Epoch: [193][  113/  113]    Overall Loss 1.267034    Objective Loss 1.267034    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.065316    
2022-12-29 10:21:21,835 - --- validate (epoch=193)-----------
2022-12-29 10:21:21,836 - 200 samples (16 per mini-batch)
2022-12-29 10:21:22,393 - Epoch: [193][   10/   13]    Loss 1.179064    Top1 54.375000    Top5 97.500000    
2022-12-29 10:21:22,477 - Epoch: [193][   13/   13]    Loss 1.173499    Top1 55.500000    Top5 97.500000    
2022-12-29 10:21:22,528 - ==> Top1: 55.500    Top5: 97.500    Loss: 1.173

2022-12-29 10:21:22,528 - ==> Confusion:
[[21  2  1  2  4  0]
 [ 2 17  3  8  8  0]
 [ 0  8 12  2 10  0]
 [ 7  4  0 38  5  1]
 [ 2  4  1  9 22  0]
 [ 1  2  1  1  1  1]]

2022-12-29 10:21:22,531 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:21:22,531 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:22,548 - 

2022-12-29 10:21:22,549 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:23,375 - Epoch: [194][   10/  113]    Overall Loss 1.298752    Objective Loss 1.298752                                        LR 0.000006    Time 0.082528    
2022-12-29 10:21:24,016 - Epoch: [194][   20/  113]    Overall Loss 1.282357    Objective Loss 1.282357                                        LR 0.000006    Time 0.073282    
2022-12-29 10:21:24,658 - Epoch: [194][   30/  113]    Overall Loss 1.313678    Objective Loss 1.313678                                        LR 0.000006    Time 0.070239    
2022-12-29 10:21:25,297 - Epoch: [194][   40/  113]    Overall Loss 1.291418    Objective Loss 1.291418                                        LR 0.000006    Time 0.068648    
2022-12-29 10:21:25,938 - Epoch: [194][   50/  113]    Overall Loss 1.291385    Objective Loss 1.291385                                        LR 0.000006    Time 0.067724    
2022-12-29 10:21:26,580 - Epoch: [194][   60/  113]    Overall Loss 1.284097    Objective Loss 1.284097                                        LR 0.000006    Time 0.067143    
2022-12-29 10:21:27,221 - Epoch: [194][   70/  113]    Overall Loss 1.288613    Objective Loss 1.288613                                        LR 0.000006    Time 0.066693    
2022-12-29 10:21:27,857 - Epoch: [194][   80/  113]    Overall Loss 1.274864    Objective Loss 1.274864                                        LR 0.000006    Time 0.066306    
2022-12-29 10:21:28,503 - Epoch: [194][   90/  113]    Overall Loss 1.265513    Objective Loss 1.265513                                        LR 0.000006    Time 0.066107    
2022-12-29 10:21:29,145 - Epoch: [194][  100/  113]    Overall Loss 1.270264    Objective Loss 1.270264                                        LR 0.000006    Time 0.065914    
2022-12-29 10:21:29,774 - Epoch: [194][  110/  113]    Overall Loss 1.274489    Objective Loss 1.274489                                        LR 0.000006    Time 0.065631    
2022-12-29 10:21:29,947 - Epoch: [194][  113/  113]    Overall Loss 1.272165    Objective Loss 1.272165    Top1 62.500000    Top5 95.833333    LR 0.000006    Time 0.065418    
2022-12-29 10:21:30,007 - --- validate (epoch=194)-----------
2022-12-29 10:21:30,007 - 200 samples (16 per mini-batch)
2022-12-29 10:21:30,569 - Epoch: [194][   10/   13]    Loss 1.284504    Top1 49.375000    Top5 94.375000    
2022-12-29 10:21:30,654 - Epoch: [194][   13/   13]    Loss 1.303690    Top1 50.000000    Top5 94.500000    
2022-12-29 10:21:30,722 - ==> Top1: 50.000    Top5: 94.500    Loss: 1.304

2022-12-29 10:21:30,723 - ==> Confusion:
[[18  1  1  5  5  0]
 [ 1 24  3  5  4  1]
 [ 0 13  7  6  9  0]
 [ 1  5  2 35  2  0]
 [ 1 11  3  9 13  0]
 [ 1  4  1  4  2  3]]

2022-12-29 10:21:30,725 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:21:30,725 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:30,744 - 

2022-12-29 10:21:30,744 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:31,577 - Epoch: [195][   10/  113]    Overall Loss 1.313948    Objective Loss 1.313948                                        LR 0.000006    Time 0.083144    
2022-12-29 10:21:32,221 - Epoch: [195][   20/  113]    Overall Loss 1.397093    Objective Loss 1.397093                                        LR 0.000006    Time 0.073757    
2022-12-29 10:21:32,863 - Epoch: [195][   30/  113]    Overall Loss 1.363967    Objective Loss 1.363967                                        LR 0.000006    Time 0.070553    
2022-12-29 10:21:33,498 - Epoch: [195][   40/  113]    Overall Loss 1.336241    Objective Loss 1.336241                                        LR 0.000006    Time 0.068794    
2022-12-29 10:21:34,131 - Epoch: [195][   50/  113]    Overall Loss 1.321820    Objective Loss 1.321820                                        LR 0.000006    Time 0.067683    
2022-12-29 10:21:34,762 - Epoch: [195][   60/  113]    Overall Loss 1.293736    Objective Loss 1.293736                                        LR 0.000006    Time 0.066905    
2022-12-29 10:21:35,395 - Epoch: [195][   70/  113]    Overall Loss 1.296123    Objective Loss 1.296123                                        LR 0.000006    Time 0.066385    
2022-12-29 10:21:36,026 - Epoch: [195][   80/  113]    Overall Loss 1.286630    Objective Loss 1.286630                                        LR 0.000006    Time 0.065964    
2022-12-29 10:21:36,661 - Epoch: [195][   90/  113]    Overall Loss 1.292485    Objective Loss 1.292485                                        LR 0.000006    Time 0.065695    
2022-12-29 10:21:37,296 - Epoch: [195][  100/  113]    Overall Loss 1.285921    Objective Loss 1.285921                                        LR 0.000006    Time 0.065471    
2022-12-29 10:21:37,923 - Epoch: [195][  110/  113]    Overall Loss 1.300115    Objective Loss 1.300115                                        LR 0.000006    Time 0.065210    
2022-12-29 10:21:38,095 - Epoch: [195][  113/  113]    Overall Loss 1.298196    Objective Loss 1.298196    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.064997    
2022-12-29 10:21:38,154 - --- validate (epoch=195)-----------
2022-12-29 10:21:38,154 - 200 samples (16 per mini-batch)
2022-12-29 10:21:38,713 - Epoch: [195][   10/   13]    Loss 1.167314    Top1 50.000000    Top5 98.750000    
2022-12-29 10:21:38,800 - Epoch: [195][   13/   13]    Loss 1.144554    Top1 50.500000    Top5 99.000000    
2022-12-29 10:21:38,855 - ==> Top1: 50.500    Top5: 99.000    Loss: 1.145

2022-12-29 10:21:38,856 - ==> Confusion:
[[23  3  1  3  1  0]
 [ 1 20  1 11  8  0]
 [ 0 13 14  7  6  0]
 [ 2  5  1 34  6  0]
 [ 4  7  0 11 10  0]
 [ 3  1  0  1  3  0]]

2022-12-29 10:21:38,858 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:21:38,858 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:38,877 - 

2022-12-29 10:21:38,878 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:39,703 - Epoch: [196][   10/  113]    Overall Loss 1.268594    Objective Loss 1.268594                                        LR 0.000006    Time 0.082297    
2022-12-29 10:21:40,344 - Epoch: [196][   20/  113]    Overall Loss 1.327711    Objective Loss 1.327711                                        LR 0.000006    Time 0.073176    
2022-12-29 10:21:40,991 - Epoch: [196][   30/  113]    Overall Loss 1.272447    Objective Loss 1.272447                                        LR 0.000006    Time 0.070337    
2022-12-29 10:21:41,629 - Epoch: [196][   40/  113]    Overall Loss 1.265656    Objective Loss 1.265656                                        LR 0.000006    Time 0.068688    
2022-12-29 10:21:42,271 - Epoch: [196][   50/  113]    Overall Loss 1.254105    Objective Loss 1.254105                                        LR 0.000006    Time 0.067779    
2022-12-29 10:21:42,909 - Epoch: [196][   60/  113]    Overall Loss 1.250140    Objective Loss 1.250140                                        LR 0.000006    Time 0.067113    
2022-12-29 10:21:43,545 - Epoch: [196][   70/  113]    Overall Loss 1.253775    Objective Loss 1.253775                                        LR 0.000006    Time 0.066608    
2022-12-29 10:21:44,183 - Epoch: [196][   80/  113]    Overall Loss 1.261967    Objective Loss 1.261967                                        LR 0.000006    Time 0.066245    
2022-12-29 10:21:44,819 - Epoch: [196][   90/  113]    Overall Loss 1.253319    Objective Loss 1.253319                                        LR 0.000006    Time 0.065951    
2022-12-29 10:21:45,456 - Epoch: [196][  100/  113]    Overall Loss 1.263399    Objective Loss 1.263399                                        LR 0.000006    Time 0.065717    
2022-12-29 10:21:46,086 - Epoch: [196][  110/  113]    Overall Loss 1.263392    Objective Loss 1.263392                                        LR 0.000006    Time 0.065465    
2022-12-29 10:21:46,259 - Epoch: [196][  113/  113]    Overall Loss 1.270509    Objective Loss 1.270509    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065260    
2022-12-29 10:21:46,315 - --- validate (epoch=196)-----------
2022-12-29 10:21:46,315 - 200 samples (16 per mini-batch)
2022-12-29 10:21:46,869 - Epoch: [196][   10/   13]    Loss 1.405472    Top1 43.125000    Top5 96.250000    
2022-12-29 10:21:46,956 - Epoch: [196][   13/   13]    Loss 1.379910    Top1 45.000000    Top5 96.000000    
2022-12-29 10:21:47,013 - ==> Top1: 45.000    Top5: 96.000    Loss: 1.380

2022-12-29 10:21:47,013 - ==> Confusion:
[[20  4  2  5  3  0]
 [ 4 15  2  2  5  0]
 [ 1 16 16  4  5  0]
 [ 8  9  0 25  3  0]
 [ 2 10  5  8 14  0]
 [ 1  7  1  1  2  0]]

2022-12-29 10:21:47,017 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:21:47,017 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:47,040 - 

2022-12-29 10:21:47,040 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:47,884 - Epoch: [197][   10/  113]    Overall Loss 1.244228    Objective Loss 1.244228                                        LR 0.000006    Time 0.084274    
2022-12-29 10:21:48,529 - Epoch: [197][   20/  113]    Overall Loss 1.192336    Objective Loss 1.192336                                        LR 0.000006    Time 0.074369    
2022-12-29 10:21:49,174 - Epoch: [197][   30/  113]    Overall Loss 1.195951    Objective Loss 1.195951                                        LR 0.000006    Time 0.071075    
2022-12-29 10:21:49,820 - Epoch: [197][   40/  113]    Overall Loss 1.192563    Objective Loss 1.192563                                        LR 0.000006    Time 0.069431    
2022-12-29 10:21:50,459 - Epoch: [197][   50/  113]    Overall Loss 1.199647    Objective Loss 1.199647                                        LR 0.000006    Time 0.068322    
2022-12-29 10:21:51,101 - Epoch: [197][   60/  113]    Overall Loss 1.198775    Objective Loss 1.198775                                        LR 0.000006    Time 0.067621    
2022-12-29 10:21:51,743 - Epoch: [197][   70/  113]    Overall Loss 1.213920    Objective Loss 1.213920                                        LR 0.000006    Time 0.067128    
2022-12-29 10:21:52,380 - Epoch: [197][   80/  113]    Overall Loss 1.226751    Objective Loss 1.226751                                        LR 0.000006    Time 0.066689    
2022-12-29 10:21:53,015 - Epoch: [197][   90/  113]    Overall Loss 1.228958    Objective Loss 1.228958                                        LR 0.000006    Time 0.066332    
2022-12-29 10:21:53,646 - Epoch: [197][  100/  113]    Overall Loss 1.226750    Objective Loss 1.226750                                        LR 0.000006    Time 0.066000    
2022-12-29 10:21:54,275 - Epoch: [197][  110/  113]    Overall Loss 1.228812    Objective Loss 1.228812                                        LR 0.000006    Time 0.065715    
2022-12-29 10:21:54,451 - Epoch: [197][  113/  113]    Overall Loss 1.230519    Objective Loss 1.230519    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065526    
2022-12-29 10:21:54,507 - --- validate (epoch=197)-----------
2022-12-29 10:21:54,507 - 200 samples (16 per mini-batch)
2022-12-29 10:21:55,049 - Epoch: [197][   10/   13]    Loss 1.436728    Top1 40.625000    Top5 97.500000    
2022-12-29 10:21:55,134 - Epoch: [197][   13/   13]    Loss 1.392829    Top1 44.000000    Top5 97.000000    
2022-12-29 10:21:55,180 - ==> Top1: 44.000    Top5: 97.000    Loss: 1.393

2022-12-29 10:21:55,180 - ==> Confusion:
[[21  4  2  7  3  1]
 [ 4 16  8  3  8  0]
 [ 0  6  6  5 12  0]
 [ 1  3  1 22  7  1]
 [ 2 10  3 11 21  0]
 [ 3  3  0  2  2  2]]

2022-12-29 10:21:55,183 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:21:55,183 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:21:55,206 - 

2022-12-29 10:21:55,206 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:21:56,045 - Epoch: [198][   10/  113]    Overall Loss 1.339012    Objective Loss 1.339012                                        LR 0.000006    Time 0.083690    
2022-12-29 10:21:56,675 - Epoch: [198][   20/  113]    Overall Loss 1.321147    Objective Loss 1.321147                                        LR 0.000006    Time 0.073308    
2022-12-29 10:21:57,311 - Epoch: [198][   30/  113]    Overall Loss 1.312456    Objective Loss 1.312456                                        LR 0.000006    Time 0.070076    
2022-12-29 10:21:57,948 - Epoch: [198][   40/  113]    Overall Loss 1.297247    Objective Loss 1.297247                                        LR 0.000006    Time 0.068463    
2022-12-29 10:21:58,584 - Epoch: [198][   50/  113]    Overall Loss 1.291338    Objective Loss 1.291338                                        LR 0.000006    Time 0.067477    
2022-12-29 10:21:59,224 - Epoch: [198][   60/  113]    Overall Loss 1.298878    Objective Loss 1.298878                                        LR 0.000006    Time 0.066896    
2022-12-29 10:21:59,856 - Epoch: [198][   70/  113]    Overall Loss 1.280928    Objective Loss 1.280928                                        LR 0.000006    Time 0.066368    
2022-12-29 10:22:00,490 - Epoch: [198][   80/  113]    Overall Loss 1.275815    Objective Loss 1.275815                                        LR 0.000006    Time 0.065986    
2022-12-29 10:22:01,129 - Epoch: [198][   90/  113]    Overall Loss 1.280978    Objective Loss 1.280978                                        LR 0.000006    Time 0.065753    
2022-12-29 10:22:01,764 - Epoch: [198][  100/  113]    Overall Loss 1.278585    Objective Loss 1.278585                                        LR 0.000006    Time 0.065520    
2022-12-29 10:22:02,389 - Epoch: [198][  110/  113]    Overall Loss 1.277915    Objective Loss 1.277915                                        LR 0.000006    Time 0.065240    
2022-12-29 10:22:02,561 - Epoch: [198][  113/  113]    Overall Loss 1.278510    Objective Loss 1.278510    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.065026    
2022-12-29 10:22:02,624 - --- validate (epoch=198)-----------
2022-12-29 10:22:02,625 - 200 samples (16 per mini-batch)
2022-12-29 10:22:03,181 - Epoch: [198][   10/   13]    Loss 1.406353    Top1 41.250000    Top5 96.250000    
2022-12-29 10:22:03,266 - Epoch: [198][   13/   13]    Loss 1.359701    Top1 43.500000    Top5 96.000000    
2022-12-29 10:22:03,312 - ==> Top1: 43.500    Top5: 96.000    Loss: 1.360

2022-12-29 10:22:03,312 - ==> Confusion:
[[14  2  1 11  3  1]
 [ 4 20  6 14  8  0]
 [ 1 10 11  8  1  0]
 [ 3  4  1 31  4  0]
 [ 4  8  4  5 10  0]
 [ 2  4  1  0  3  1]]

2022-12-29 10:22:03,315 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:03,315 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:03,337 - 

2022-12-29 10:22:03,337 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:04,167 - Epoch: [199][   10/  113]    Overall Loss 1.432049    Objective Loss 1.432049                                        LR 0.000006    Time 0.082897    
2022-12-29 10:22:04,800 - Epoch: [199][   20/  113]    Overall Loss 1.347265    Objective Loss 1.347265                                        LR 0.000006    Time 0.073035    
2022-12-29 10:22:05,432 - Epoch: [199][   30/  113]    Overall Loss 1.347440    Objective Loss 1.347440                                        LR 0.000006    Time 0.069769    
2022-12-29 10:22:06,065 - Epoch: [199][   40/  113]    Overall Loss 1.300615    Objective Loss 1.300615                                        LR 0.000006    Time 0.068131    
2022-12-29 10:22:06,697 - Epoch: [199][   50/  113]    Overall Loss 1.284417    Objective Loss 1.284417                                        LR 0.000006    Time 0.067138    
2022-12-29 10:22:07,331 - Epoch: [199][   60/  113]    Overall Loss 1.276357    Objective Loss 1.276357                                        LR 0.000006    Time 0.066507    
2022-12-29 10:22:07,965 - Epoch: [199][   70/  113]    Overall Loss 1.293453    Objective Loss 1.293453                                        LR 0.000006    Time 0.066056    
2022-12-29 10:22:08,600 - Epoch: [199][   80/  113]    Overall Loss 1.271031    Objective Loss 1.271031                                        LR 0.000006    Time 0.065735    
2022-12-29 10:22:09,232 - Epoch: [199][   90/  113]    Overall Loss 1.269853    Objective Loss 1.269853                                        LR 0.000006    Time 0.065445    
2022-12-29 10:22:09,864 - Epoch: [199][  100/  113]    Overall Loss 1.269357    Objective Loss 1.269357                                        LR 0.000006    Time 0.065216    
2022-12-29 10:22:10,482 - Epoch: [199][  110/  113]    Overall Loss 1.278621    Objective Loss 1.278621                                        LR 0.000006    Time 0.064902    
2022-12-29 10:22:10,658 - Epoch: [199][  113/  113]    Overall Loss 1.281681    Objective Loss 1.281681    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064732    
2022-12-29 10:22:10,708 - --- validate (epoch=199)-----------
2022-12-29 10:22:10,708 - 200 samples (16 per mini-batch)
2022-12-29 10:22:11,267 - Epoch: [199][   10/   13]    Loss 1.293286    Top1 46.875000    Top5 97.500000    
2022-12-29 10:22:11,353 - Epoch: [199][   13/   13]    Loss 1.330906    Top1 46.000000    Top5 97.000000    
2022-12-29 10:22:11,400 - ==> Top1: 46.000    Top5: 97.000    Loss: 1.331

2022-12-29 10:22:11,401 - ==> Confusion:
[[16  6  1  4  1  1]
 [ 2 15  5 13  5  0]
 [ 1  9  4 12  6  0]
 [ 3  1  0 41  7  0]
 [ 2  5  1 12 16  0]
 [ 4  4  0  2  1  0]]

2022-12-29 10:22:11,402 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:11,402 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:11,418 - 

2022-12-29 10:22:11,418 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:12,268 - Epoch: [200][   10/  113]    Overall Loss 1.294562    Objective Loss 1.294562                                        LR 0.000006    Time 0.084868    
2022-12-29 10:22:12,908 - Epoch: [200][   20/  113]    Overall Loss 1.252133    Objective Loss 1.252133                                        LR 0.000006    Time 0.074438    
2022-12-29 10:22:13,548 - Epoch: [200][   30/  113]    Overall Loss 1.271864    Objective Loss 1.271864                                        LR 0.000006    Time 0.070944    
2022-12-29 10:22:14,185 - Epoch: [200][   40/  113]    Overall Loss 1.276728    Objective Loss 1.276728                                        LR 0.000006    Time 0.069109    
2022-12-29 10:22:14,819 - Epoch: [200][   50/  113]    Overall Loss 1.277066    Objective Loss 1.277066                                        LR 0.000006    Time 0.067960    
2022-12-29 10:22:15,455 - Epoch: [200][   60/  113]    Overall Loss 1.266641    Objective Loss 1.266641                                        LR 0.000006    Time 0.067233    
2022-12-29 10:22:16,089 - Epoch: [200][   70/  113]    Overall Loss 1.255596    Objective Loss 1.255596                                        LR 0.000006    Time 0.066681    
2022-12-29 10:22:16,726 - Epoch: [200][   80/  113]    Overall Loss 1.244121    Objective Loss 1.244121                                        LR 0.000006    Time 0.066294    
2022-12-29 10:22:17,362 - Epoch: [200][   90/  113]    Overall Loss 1.248536    Objective Loss 1.248536                                        LR 0.000006    Time 0.065992    
2022-12-29 10:22:17,994 - Epoch: [200][  100/  113]    Overall Loss 1.241850    Objective Loss 1.241850                                        LR 0.000006    Time 0.065710    
2022-12-29 10:22:18,623 - Epoch: [200][  110/  113]    Overall Loss 1.249772    Objective Loss 1.249772                                        LR 0.000006    Time 0.065449    
2022-12-29 10:22:18,795 - Epoch: [200][  113/  113]    Overall Loss 1.249263    Objective Loss 1.249263    Top1 50.000000    Top5 91.666667    LR 0.000006    Time 0.065228    
2022-12-29 10:22:18,853 - --- validate (epoch=200)-----------
2022-12-29 10:22:18,854 - 200 samples (16 per mini-batch)
2022-12-29 10:22:19,408 - Epoch: [200][   10/   13]    Loss 1.255828    Top1 45.625000    Top5 96.875000    
2022-12-29 10:22:19,492 - Epoch: [200][   13/   13]    Loss 1.256910    Top1 44.500000    Top5 97.500000    
2022-12-29 10:22:19,545 - ==> Top1: 44.500    Top5: 97.500    Loss: 1.257

2022-12-29 10:22:19,545 - ==> Confusion:
[[18  6  1  9  2  0]
 [ 4 23  3  3  1  1]
 [ 0 16 14  9  6  0]
 [ 4  9  2 24  3  0]
 [ 3 14  1  8  8  0]
 [ 1  2  0  2  1  2]]

2022-12-29 10:22:19,548 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:19,548 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:19,576 - 

2022-12-29 10:22:19,577 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:20,413 - Epoch: [201][   10/  113]    Overall Loss 1.175327    Objective Loss 1.175327                                        LR 0.000006    Time 0.083479    
2022-12-29 10:22:21,058 - Epoch: [201][   20/  113]    Overall Loss 1.108461    Objective Loss 1.108461                                        LR 0.000006    Time 0.073954    
2022-12-29 10:22:21,695 - Epoch: [201][   30/  113]    Overall Loss 1.151655    Objective Loss 1.151655                                        LR 0.000006    Time 0.070532    
2022-12-29 10:22:22,334 - Epoch: [201][   40/  113]    Overall Loss 1.166778    Objective Loss 1.166778                                        LR 0.000006    Time 0.068847    
2022-12-29 10:22:22,973 - Epoch: [201][   50/  113]    Overall Loss 1.181084    Objective Loss 1.181084                                        LR 0.000006    Time 0.067856    
2022-12-29 10:22:23,608 - Epoch: [201][   60/  113]    Overall Loss 1.185194    Objective Loss 1.185194                                        LR 0.000006    Time 0.067124    
2022-12-29 10:22:24,240 - Epoch: [201][   70/  113]    Overall Loss 1.193335    Objective Loss 1.193335                                        LR 0.000006    Time 0.066554    
2022-12-29 10:22:24,871 - Epoch: [201][   80/  113]    Overall Loss 1.189011    Objective Loss 1.189011                                        LR 0.000006    Time 0.066119    
2022-12-29 10:22:25,509 - Epoch: [201][   90/  113]    Overall Loss 1.193908    Objective Loss 1.193908                                        LR 0.000006    Time 0.065855    
2022-12-29 10:22:26,143 - Epoch: [201][  100/  113]    Overall Loss 1.192399    Objective Loss 1.192399                                        LR 0.000006    Time 0.065601    
2022-12-29 10:22:26,771 - Epoch: [201][  110/  113]    Overall Loss 1.205582    Objective Loss 1.205582                                        LR 0.000006    Time 0.065345    
2022-12-29 10:22:26,942 - Epoch: [201][  113/  113]    Overall Loss 1.208504    Objective Loss 1.208504    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065122    
2022-12-29 10:22:26,998 - --- validate (epoch=201)-----------
2022-12-29 10:22:26,998 - 200 samples (16 per mini-batch)
2022-12-29 10:22:27,546 - Epoch: [201][   10/   13]    Loss 1.328843    Top1 45.000000    Top5 99.375000    
2022-12-29 10:22:27,632 - Epoch: [201][   13/   13]    Loss 1.324396    Top1 44.500000    Top5 99.500000    
2022-12-29 10:22:27,691 - ==> Top1: 44.500    Top5: 99.500    Loss: 1.324

2022-12-29 10:22:27,692 - ==> Confusion:
[[14  3  0  5  0  0]
 [ 4 28  1  9  7  0]
 [ 2 10  9  8  3  0]
 [ 5  6  5 31  7  0]
 [ 3 15  2  5  7  1]
 [ 3  5  1  0  1  0]]

2022-12-29 10:22:27,695 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:27,696 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:27,721 - 

2022-12-29 10:22:27,721 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:28,553 - Epoch: [202][   10/  113]    Overall Loss 1.272292    Objective Loss 1.272292                                        LR 0.000006    Time 0.082955    
2022-12-29 10:22:29,193 - Epoch: [202][   20/  113]    Overall Loss 1.250635    Objective Loss 1.250635                                        LR 0.000006    Time 0.073446    
2022-12-29 10:22:29,832 - Epoch: [202][   30/  113]    Overall Loss 1.282302    Objective Loss 1.282302                                        LR 0.000006    Time 0.070255    
2022-12-29 10:22:30,471 - Epoch: [202][   40/  113]    Overall Loss 1.285752    Objective Loss 1.285752                                        LR 0.000006    Time 0.068653    
2022-12-29 10:22:31,108 - Epoch: [202][   50/  113]    Overall Loss 1.277708    Objective Loss 1.277708                                        LR 0.000006    Time 0.067649    
2022-12-29 10:22:31,742 - Epoch: [202][   60/  113]    Overall Loss 1.279333    Objective Loss 1.279333                                        LR 0.000006    Time 0.066947    
2022-12-29 10:22:32,378 - Epoch: [202][   70/  113]    Overall Loss 1.285167    Objective Loss 1.285167                                        LR 0.000006    Time 0.066453    
2022-12-29 10:22:33,009 - Epoch: [202][   80/  113]    Overall Loss 1.268386    Objective Loss 1.268386                                        LR 0.000006    Time 0.066035    
2022-12-29 10:22:33,649 - Epoch: [202][   90/  113]    Overall Loss 1.266350    Objective Loss 1.266350                                        LR 0.000006    Time 0.065796    
2022-12-29 10:22:34,282 - Epoch: [202][  100/  113]    Overall Loss 1.266438    Objective Loss 1.266438                                        LR 0.000006    Time 0.065541    
2022-12-29 10:22:34,903 - Epoch: [202][  110/  113]    Overall Loss 1.269967    Objective Loss 1.269967                                        LR 0.000006    Time 0.065226    
2022-12-29 10:22:35,079 - Epoch: [202][  113/  113]    Overall Loss 1.267149    Objective Loss 1.267149    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065051    
2022-12-29 10:22:35,135 - --- validate (epoch=202)-----------
2022-12-29 10:22:35,135 - 200 samples (16 per mini-batch)
2022-12-29 10:22:35,684 - Epoch: [202][   10/   13]    Loss 1.176547    Top1 50.000000    Top5 96.875000    
2022-12-29 10:22:35,769 - Epoch: [202][   13/   13]    Loss 1.159817    Top1 51.000000    Top5 97.500000    
2022-12-29 10:22:35,834 - ==> Top1: 51.000    Top5: 97.500    Loss: 1.160

2022-12-29 10:22:35,835 - ==> Confusion:
[[26  2  0  9  2  0]
 [ 4 26  5 11  8  0]
 [ 2  7  9  2  5  0]
 [ 4  2  2 31  2  0]
 [ 0  7  4 11  9  1]
 [ 3  1  0  0  4  1]]

2022-12-29 10:22:35,837 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:35,837 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:35,851 - 

2022-12-29 10:22:35,851 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:36,690 - Epoch: [203][   10/  113]    Overall Loss 1.303257    Objective Loss 1.303257                                        LR 0.000006    Time 0.083765    
2022-12-29 10:22:37,330 - Epoch: [203][   20/  113]    Overall Loss 1.311300    Objective Loss 1.311300                                        LR 0.000006    Time 0.073873    
2022-12-29 10:22:37,971 - Epoch: [203][   30/  113]    Overall Loss 1.302957    Objective Loss 1.302957                                        LR 0.000006    Time 0.070599    
2022-12-29 10:22:38,605 - Epoch: [203][   40/  113]    Overall Loss 1.286002    Objective Loss 1.286002                                        LR 0.000006    Time 0.068795    
2022-12-29 10:22:39,238 - Epoch: [203][   50/  113]    Overall Loss 1.275029    Objective Loss 1.275029                                        LR 0.000006    Time 0.067694    
2022-12-29 10:22:39,868 - Epoch: [203][   60/  113]    Overall Loss 1.263732    Objective Loss 1.263732                                        LR 0.000006    Time 0.066901    
2022-12-29 10:22:40,501 - Epoch: [203][   70/  113]    Overall Loss 1.250173    Objective Loss 1.250173                                        LR 0.000006    Time 0.066379    
2022-12-29 10:22:41,134 - Epoch: [203][   80/  113]    Overall Loss 1.235818    Objective Loss 1.235818                                        LR 0.000006    Time 0.065986    
2022-12-29 10:22:41,774 - Epoch: [203][   90/  113]    Overall Loss 1.250880    Objective Loss 1.250880                                        LR 0.000006    Time 0.065754    
2022-12-29 10:22:42,412 - Epoch: [203][  100/  113]    Overall Loss 1.248854    Objective Loss 1.248854                                        LR 0.000006    Time 0.065555    
2022-12-29 10:22:43,043 - Epoch: [203][  110/  113]    Overall Loss 1.248040    Objective Loss 1.248040                                        LR 0.000006    Time 0.065333    
2022-12-29 10:22:43,221 - Epoch: [203][  113/  113]    Overall Loss 1.248562    Objective Loss 1.248562    Top1 62.500000    Top5 91.666667    LR 0.000006    Time 0.065171    
2022-12-29 10:22:43,271 - --- validate (epoch=203)-----------
2022-12-29 10:22:43,272 - 200 samples (16 per mini-batch)
2022-12-29 10:22:43,825 - Epoch: [203][   10/   13]    Loss 1.376117    Top1 49.375000    Top5 95.000000    
2022-12-29 10:22:43,909 - Epoch: [203][   13/   13]    Loss 1.325876    Top1 51.000000    Top5 96.000000    
2022-12-29 10:22:43,971 - ==> Top1: 51.000    Top5: 96.000    Loss: 1.326

2022-12-29 10:22:43,972 - ==> Confusion:
[[19  2  1  5  1  1]
 [ 6 23  4  8  2  0]
 [ 0  8 12 11 10  1]
 [ 2  7  2 22  1  0]
 [ 1 11  2  6 26  0]
 [ 0  2  0  1  3  0]]

2022-12-29 10:22:43,974 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:43,974 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:43,995 - 

2022-12-29 10:22:43,996 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:44,828 - Epoch: [204][   10/  113]    Overall Loss 1.366758    Objective Loss 1.366758                                        LR 0.000006    Time 0.083148    
2022-12-29 10:22:45,469 - Epoch: [204][   20/  113]    Overall Loss 1.322663    Objective Loss 1.322663                                        LR 0.000006    Time 0.073576    
2022-12-29 10:22:46,107 - Epoch: [204][   30/  113]    Overall Loss 1.302525    Objective Loss 1.302525                                        LR 0.000006    Time 0.070306    
2022-12-29 10:22:46,745 - Epoch: [204][   40/  113]    Overall Loss 1.290884    Objective Loss 1.290884                                        LR 0.000006    Time 0.068685    
2022-12-29 10:22:47,386 - Epoch: [204][   50/  113]    Overall Loss 1.276377    Objective Loss 1.276377                                        LR 0.000006    Time 0.067752    
2022-12-29 10:22:48,022 - Epoch: [204][   60/  113]    Overall Loss 1.301429    Objective Loss 1.301429                                        LR 0.000006    Time 0.067049    
2022-12-29 10:22:48,660 - Epoch: [204][   70/  113]    Overall Loss 1.307179    Objective Loss 1.307179                                        LR 0.000006    Time 0.066581    
2022-12-29 10:22:49,296 - Epoch: [204][   80/  113]    Overall Loss 1.311063    Objective Loss 1.311063                                        LR 0.000006    Time 0.066198    
2022-12-29 10:22:49,933 - Epoch: [204][   90/  113]    Overall Loss 1.313503    Objective Loss 1.313503                                        LR 0.000006    Time 0.065921    
2022-12-29 10:22:50,568 - Epoch: [204][  100/  113]    Overall Loss 1.300689    Objective Loss 1.300689                                        LR 0.000006    Time 0.065669    
2022-12-29 10:22:51,196 - Epoch: [204][  110/  113]    Overall Loss 1.307525    Objective Loss 1.307525                                        LR 0.000006    Time 0.065399    
2022-12-29 10:22:51,371 - Epoch: [204][  113/  113]    Overall Loss 1.312686    Objective Loss 1.312686    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.065209    
2022-12-29 10:22:51,423 - --- validate (epoch=204)-----------
2022-12-29 10:22:51,423 - 200 samples (16 per mini-batch)
2022-12-29 10:22:51,970 - Epoch: [204][   10/   13]    Loss 1.320714    Top1 48.125000    Top5 97.500000    
2022-12-29 10:22:52,061 - Epoch: [204][   13/   13]    Loss 1.370680    Top1 46.000000    Top5 97.000000    
2022-12-29 10:22:52,106 - ==> Top1: 46.000    Top5: 97.000    Loss: 1.371

2022-12-29 10:22:52,107 - ==> Confusion:
[[18  4  2  8  2  1]
 [ 3 22  2  8  4  1]
 [ 0  7  9 10  7  0]
 [ 2  7  1 22  8  0]
 [ 3  8  1  8 19  0]
 [ 1  2  1  3  4  2]]

2022-12-29 10:22:52,110 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:22:52,110 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:22:52,123 - 

2022-12-29 10:22:52,123 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:22:52,973 - Epoch: [205][   10/  113]    Overall Loss 1.312367    Objective Loss 1.312367                                        LR 0.000006    Time 0.084914    
2022-12-29 10:22:53,613 - Epoch: [205][   20/  113]    Overall Loss 1.249872    Objective Loss 1.249872                                        LR 0.000006    Time 0.074431    
2022-12-29 10:22:54,251 - Epoch: [205][   30/  113]    Overall Loss 1.270297    Objective Loss 1.270297                                        LR 0.000006    Time 0.070873    
2022-12-29 10:22:54,894 - Epoch: [205][   40/  113]    Overall Loss 1.267895    Objective Loss 1.267895                                        LR 0.000006    Time 0.069209    
2022-12-29 10:22:55,528 - Epoch: [205][   50/  113]    Overall Loss 1.260210    Objective Loss 1.260210                                        LR 0.000006    Time 0.068052    
2022-12-29 10:22:56,165 - Epoch: [205][   60/  113]    Overall Loss 1.255684    Objective Loss 1.255684                                        LR 0.000006    Time 0.067318    
2022-12-29 10:22:56,805 - Epoch: [205][   70/  113]    Overall Loss 1.249009    Objective Loss 1.249009                                        LR 0.000006    Time 0.066841    
2022-12-29 10:22:57,446 - Epoch: [205][   80/  113]    Overall Loss 1.241614    Objective Loss 1.241614                                        LR 0.000006    Time 0.066489    
2022-12-29 10:22:58,085 - Epoch: [205][   90/  113]    Overall Loss 1.242519    Objective Loss 1.242519                                        LR 0.000006    Time 0.066192    
2022-12-29 10:22:58,720 - Epoch: [205][  100/  113]    Overall Loss 1.249647    Objective Loss 1.249647                                        LR 0.000006    Time 0.065918    
2022-12-29 10:22:59,345 - Epoch: [205][  110/  113]    Overall Loss 1.248853    Objective Loss 1.248853                                        LR 0.000006    Time 0.065599    
2022-12-29 10:22:59,517 - Epoch: [205][  113/  113]    Overall Loss 1.246984    Objective Loss 1.246984    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065382    
2022-12-29 10:22:59,562 - --- validate (epoch=205)-----------
2022-12-29 10:22:59,562 - 200 samples (16 per mini-batch)
2022-12-29 10:23:00,107 - Epoch: [205][   10/   13]    Loss 1.446704    Top1 41.250000    Top5 94.375000    
2022-12-29 10:23:00,193 - Epoch: [205][   13/   13]    Loss 1.386076    Top1 44.000000    Top5 95.000000    
2022-12-29 10:23:00,243 - ==> Top1: 44.000    Top5: 95.000    Loss: 1.386

2022-12-29 10:23:00,244 - ==> Confusion:
[[15  1  3 12  3  1]
 [ 1 19  5 12  3  1]
 [ 1  9  8  9  7  0]
 [ 8  6  1 30  3  0]
 [ 1  7  1  9 14  0]
 [ 1  2  0  3  2  2]]

2022-12-29 10:23:00,247 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:00,247 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:00,267 - 

2022-12-29 10:23:00,267 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:01,109 - Epoch: [206][   10/  113]    Overall Loss 1.272102    Objective Loss 1.272102                                        LR 0.000006    Time 0.084107    
2022-12-29 10:23:01,753 - Epoch: [206][   20/  113]    Overall Loss 1.304617    Objective Loss 1.304617                                        LR 0.000006    Time 0.074213    
2022-12-29 10:23:02,395 - Epoch: [206][   30/  113]    Overall Loss 1.303176    Objective Loss 1.303176                                        LR 0.000006    Time 0.070866    
2022-12-29 10:23:03,036 - Epoch: [206][   40/  113]    Overall Loss 1.261657    Objective Loss 1.261657                                        LR 0.000006    Time 0.069160    
2022-12-29 10:23:03,674 - Epoch: [206][   50/  113]    Overall Loss 1.247417    Objective Loss 1.247417                                        LR 0.000006    Time 0.068083    
2022-12-29 10:23:04,311 - Epoch: [206][   60/  113]    Overall Loss 1.258569    Objective Loss 1.258569                                        LR 0.000006    Time 0.067343    
2022-12-29 10:23:04,951 - Epoch: [206][   70/  113]    Overall Loss 1.265914    Objective Loss 1.265914                                        LR 0.000006    Time 0.066843    
2022-12-29 10:23:05,587 - Epoch: [206][   80/  113]    Overall Loss 1.264687    Objective Loss 1.264687                                        LR 0.000006    Time 0.066444    
2022-12-29 10:23:06,229 - Epoch: [206][   90/  113]    Overall Loss 1.264443    Objective Loss 1.264443                                        LR 0.000006    Time 0.066178    
2022-12-29 10:23:06,858 - Epoch: [206][  100/  113]    Overall Loss 1.260793    Objective Loss 1.260793                                        LR 0.000006    Time 0.065847    
2022-12-29 10:23:07,492 - Epoch: [206][  110/  113]    Overall Loss 1.266500    Objective Loss 1.266500                                        LR 0.000006    Time 0.065621    
2022-12-29 10:23:07,665 - Epoch: [206][  113/  113]    Overall Loss 1.269154    Objective Loss 1.269154    Top1 50.000000    Top5 91.666667    LR 0.000006    Time 0.065413    
2022-12-29 10:23:07,730 - --- validate (epoch=206)-----------
2022-12-29 10:23:07,730 - 200 samples (16 per mini-batch)
2022-12-29 10:23:08,277 - Epoch: [206][   10/   13]    Loss 1.293336    Top1 46.250000    Top5 96.875000    
2022-12-29 10:23:08,363 - Epoch: [206][   13/   13]    Loss 1.278359    Top1 46.500000    Top5 97.500000    
2022-12-29 10:23:08,428 - ==> Top1: 46.500    Top5: 97.500    Loss: 1.278

2022-12-29 10:23:08,429 - ==> Confusion:
[[18  5  0  8  2  0]
 [ 2 25  4  7 10  1]
 [ 1  8  7  2 12  0]
 [ 1  7  3 29  7  2]
 [ 2  8  4  3 14  1]
 [ 3  2  0  1  1  0]]

2022-12-29 10:23:08,431 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:08,431 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:08,440 - 

2022-12-29 10:23:08,440 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:09,265 - Epoch: [207][   10/  113]    Overall Loss 1.220792    Objective Loss 1.220792                                        LR 0.000006    Time 0.082387    
2022-12-29 10:23:09,904 - Epoch: [207][   20/  113]    Overall Loss 1.217156    Objective Loss 1.217156                                        LR 0.000006    Time 0.073076    
2022-12-29 10:23:10,539 - Epoch: [207][   30/  113]    Overall Loss 1.268587    Objective Loss 1.268587                                        LR 0.000006    Time 0.069896    
2022-12-29 10:23:11,179 - Epoch: [207][   40/  113]    Overall Loss 1.242455    Objective Loss 1.242455                                        LR 0.000006    Time 0.068396    
2022-12-29 10:23:11,810 - Epoch: [207][   50/  113]    Overall Loss 1.239194    Objective Loss 1.239194                                        LR 0.000006    Time 0.067325    
2022-12-29 10:23:12,446 - Epoch: [207][   60/  113]    Overall Loss 1.221431    Objective Loss 1.221431                                        LR 0.000006    Time 0.066695    
2022-12-29 10:23:13,075 - Epoch: [207][   70/  113]    Overall Loss 1.225227    Objective Loss 1.225227                                        LR 0.000006    Time 0.066146    
2022-12-29 10:23:13,709 - Epoch: [207][   80/  113]    Overall Loss 1.223245    Objective Loss 1.223245                                        LR 0.000006    Time 0.065801    
2022-12-29 10:23:14,343 - Epoch: [207][   90/  113]    Overall Loss 1.233989    Objective Loss 1.233989                                        LR 0.000006    Time 0.065529    
2022-12-29 10:23:14,974 - Epoch: [207][  100/  113]    Overall Loss 1.239677    Objective Loss 1.239677                                        LR 0.000006    Time 0.065280    
2022-12-29 10:23:15,609 - Epoch: [207][  110/  113]    Overall Loss 1.238569    Objective Loss 1.238569                                        LR 0.000006    Time 0.065115    
2022-12-29 10:23:15,780 - Epoch: [207][  113/  113]    Overall Loss 1.239894    Objective Loss 1.239894    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064900    
2022-12-29 10:23:15,830 - --- validate (epoch=207)-----------
2022-12-29 10:23:15,830 - 200 samples (16 per mini-batch)
2022-12-29 10:23:16,380 - Epoch: [207][   10/   13]    Loss 1.328672    Top1 45.625000    Top5 98.125000    
2022-12-29 10:23:16,464 - Epoch: [207][   13/   13]    Loss 1.264534    Top1 49.000000    Top5 98.000000    
2022-12-29 10:23:16,515 - ==> Top1: 49.000    Top5: 98.000    Loss: 1.265

2022-12-29 10:23:16,516 - ==> Confusion:
[[17  2  0  4  5  0]
 [ 4 26  2  8  7  0]
 [ 1  8  9 10  7  1]
 [ 5  4  3 28  3  1]
 [ 3  5  3  7 16  0]
 [ 2  1  2  1  3  2]]

2022-12-29 10:23:16,518 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:16,518 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:16,534 - 

2022-12-29 10:23:16,534 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:17,356 - Epoch: [208][   10/  113]    Overall Loss 1.240725    Objective Loss 1.240725                                        LR 0.000006    Time 0.082140    
2022-12-29 10:23:17,991 - Epoch: [208][   20/  113]    Overall Loss 1.266888    Objective Loss 1.266888                                        LR 0.000006    Time 0.072777    
2022-12-29 10:23:18,625 - Epoch: [208][   30/  113]    Overall Loss 1.282280    Objective Loss 1.282280                                        LR 0.000006    Time 0.069640    
2022-12-29 10:23:19,257 - Epoch: [208][   40/  113]    Overall Loss 1.266412    Objective Loss 1.266412                                        LR 0.000006    Time 0.068016    
2022-12-29 10:23:19,894 - Epoch: [208][   50/  113]    Overall Loss 1.263573    Objective Loss 1.263573                                        LR 0.000006    Time 0.067143    
2022-12-29 10:23:20,532 - Epoch: [208][   60/  113]    Overall Loss 1.251266    Objective Loss 1.251266                                        LR 0.000006    Time 0.066582    
2022-12-29 10:23:21,175 - Epoch: [208][   70/  113]    Overall Loss 1.257027    Objective Loss 1.257027                                        LR 0.000006    Time 0.066242    
2022-12-29 10:23:21,817 - Epoch: [208][   80/  113]    Overall Loss 1.253546    Objective Loss 1.253546                                        LR 0.000006    Time 0.065990    
2022-12-29 10:23:22,458 - Epoch: [208][   90/  113]    Overall Loss 1.258230    Objective Loss 1.258230                                        LR 0.000006    Time 0.065775    
2022-12-29 10:23:23,088 - Epoch: [208][  100/  113]    Overall Loss 1.246612    Objective Loss 1.246612                                        LR 0.000006    Time 0.065487    
2022-12-29 10:23:23,714 - Epoch: [208][  110/  113]    Overall Loss 1.240600    Objective Loss 1.240600                                        LR 0.000006    Time 0.065221    
2022-12-29 10:23:23,889 - Epoch: [208][  113/  113]    Overall Loss 1.247637    Objective Loss 1.247637    Top1 29.166667    Top5 100.000000    LR 0.000006    Time 0.065033    
2022-12-29 10:23:23,939 - --- validate (epoch=208)-----------
2022-12-29 10:23:23,939 - 200 samples (16 per mini-batch)
2022-12-29 10:23:24,486 - Epoch: [208][   10/   13]    Loss 1.360496    Top1 47.500000    Top5 98.125000    
2022-12-29 10:23:24,571 - Epoch: [208][   13/   13]    Loss 1.337404    Top1 45.500000    Top5 98.500000    
2022-12-29 10:23:24,623 - ==> Top1: 45.500    Top5: 98.500    Loss: 1.337

2022-12-29 10:23:24,624 - ==> Confusion:
[[14  3  0  6  1  0]
 [ 3 18 10  8 10  2]
 [ 1  9  8  3  8  0]
 [ 3  7  2 37  3  0]
 [ 2  7  2 12 14  0]
 [ 1  1  1  1  3  0]]

2022-12-29 10:23:24,627 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:24,627 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:24,654 - 

2022-12-29 10:23:24,654 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:25,477 - Epoch: [209][   10/  113]    Overall Loss 1.295720    Objective Loss 1.295720                                        LR 0.000006    Time 0.082169    
2022-12-29 10:23:26,109 - Epoch: [209][   20/  113]    Overall Loss 1.234095    Objective Loss 1.234095                                        LR 0.000006    Time 0.072656    
2022-12-29 10:23:26,743 - Epoch: [209][   30/  113]    Overall Loss 1.258679    Objective Loss 1.258679                                        LR 0.000006    Time 0.069538    
2022-12-29 10:23:27,381 - Epoch: [209][   40/  113]    Overall Loss 1.244517    Objective Loss 1.244517                                        LR 0.000006    Time 0.068106    
2022-12-29 10:23:28,019 - Epoch: [209][   50/  113]    Overall Loss 1.228568    Objective Loss 1.228568                                        LR 0.000006    Time 0.067240    
2022-12-29 10:23:28,651 - Epoch: [209][   60/  113]    Overall Loss 1.229637    Objective Loss 1.229637                                        LR 0.000006    Time 0.066551    
2022-12-29 10:23:29,284 - Epoch: [209][   70/  113]    Overall Loss 1.235911    Objective Loss 1.235911                                        LR 0.000006    Time 0.066075    
2022-12-29 10:23:29,915 - Epoch: [209][   80/  113]    Overall Loss 1.236863    Objective Loss 1.236863                                        LR 0.000006    Time 0.065699    
2022-12-29 10:23:30,550 - Epoch: [209][   90/  113]    Overall Loss 1.236846    Objective Loss 1.236846                                        LR 0.000006    Time 0.065457    
2022-12-29 10:23:31,180 - Epoch: [209][  100/  113]    Overall Loss 1.242068    Objective Loss 1.242068                                        LR 0.000006    Time 0.065204    
2022-12-29 10:23:31,812 - Epoch: [209][  110/  113]    Overall Loss 1.245844    Objective Loss 1.245844                                        LR 0.000006    Time 0.065016    
2022-12-29 10:23:31,985 - Epoch: [209][  113/  113]    Overall Loss 1.245164    Objective Loss 1.245164    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.064815    
2022-12-29 10:23:32,044 - --- validate (epoch=209)-----------
2022-12-29 10:23:32,044 - 200 samples (16 per mini-batch)
2022-12-29 10:23:32,599 - Epoch: [209][   10/   13]    Loss 1.277426    Top1 51.250000    Top5 98.125000    
2022-12-29 10:23:32,683 - Epoch: [209][   13/   13]    Loss 1.286663    Top1 51.000000    Top5 98.000000    
2022-12-29 10:23:32,747 - ==> Top1: 51.000    Top5: 98.000    Loss: 1.287

2022-12-29 10:23:32,747 - ==> Confusion:
[[19  2  0  8  0  1]
 [ 3 21  4  5  5  0]
 [ 1  8 14  6  5  0]
 [ 2  5  5 32  3  0]
 [ 2  5  4 16 15  1]
 [ 0  2  0  2  3  1]]

2022-12-29 10:23:32,749 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:32,750 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:32,763 - 

2022-12-29 10:23:32,763 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:33,607 - Epoch: [210][   10/  113]    Overall Loss 1.357252    Objective Loss 1.357252                                        LR 0.000006    Time 0.084324    
2022-12-29 10:23:34,248 - Epoch: [210][   20/  113]    Overall Loss 1.304437    Objective Loss 1.304437                                        LR 0.000006    Time 0.074151    
2022-12-29 10:23:34,887 - Epoch: [210][   30/  113]    Overall Loss 1.305988    Objective Loss 1.305988                                        LR 0.000006    Time 0.070714    
2022-12-29 10:23:35,530 - Epoch: [210][   40/  113]    Overall Loss 1.282411    Objective Loss 1.282411                                        LR 0.000006    Time 0.069104    
2022-12-29 10:23:36,169 - Epoch: [210][   50/  113]    Overall Loss 1.272720    Objective Loss 1.272720                                        LR 0.000006    Time 0.068061    
2022-12-29 10:23:36,803 - Epoch: [210][   60/  113]    Overall Loss 1.283986    Objective Loss 1.283986                                        LR 0.000006    Time 0.067273    
2022-12-29 10:23:37,443 - Epoch: [210][   70/  113]    Overall Loss 1.280059    Objective Loss 1.280059                                        LR 0.000006    Time 0.066793    
2022-12-29 10:23:38,081 - Epoch: [210][   80/  113]    Overall Loss 1.281856    Objective Loss 1.281856                                        LR 0.000006    Time 0.066413    
2022-12-29 10:23:38,718 - Epoch: [210][   90/  113]    Overall Loss 1.275813    Objective Loss 1.275813                                        LR 0.000006    Time 0.066116    
2022-12-29 10:23:39,358 - Epoch: [210][  100/  113]    Overall Loss 1.278937    Objective Loss 1.278937                                        LR 0.000006    Time 0.065892    
2022-12-29 10:23:39,983 - Epoch: [210][  110/  113]    Overall Loss 1.281454    Objective Loss 1.281454                                        LR 0.000006    Time 0.065582    
2022-12-29 10:23:40,156 - Epoch: [210][  113/  113]    Overall Loss 1.280789    Objective Loss 1.280789    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065369    
2022-12-29 10:23:40,215 - --- validate (epoch=210)-----------
2022-12-29 10:23:40,216 - 200 samples (16 per mini-batch)
2022-12-29 10:23:40,780 - Epoch: [210][   10/   13]    Loss 1.167910    Top1 54.375000    Top5 98.125000    
2022-12-29 10:23:40,867 - Epoch: [210][   13/   13]    Loss 1.230141    Top1 51.500000    Top5 97.000000    
2022-12-29 10:23:40,923 - ==> Top1: 51.500    Top5: 97.000    Loss: 1.230

2022-12-29 10:23:40,924 - ==> Confusion:
[[18  3  2  2  1  0]
 [ 0 23  1  2 10  0]
 [ 1 10 12  2 12  0]
 [ 6  3  1 32 10  0]
 [ 2 11  1 12 17  0]
 [ 0  1  0  2  2  1]]

2022-12-29 10:23:40,926 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:40,927 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:40,946 - 

2022-12-29 10:23:40,947 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:41,801 - Epoch: [211][   10/  113]    Overall Loss 1.294909    Objective Loss 1.294909                                        LR 0.000006    Time 0.085298    
2022-12-29 10:23:42,444 - Epoch: [211][   20/  113]    Overall Loss 1.288688    Objective Loss 1.288688                                        LR 0.000006    Time 0.074769    
2022-12-29 10:23:43,082 - Epoch: [211][   30/  113]    Overall Loss 1.246868    Objective Loss 1.246868                                        LR 0.000006    Time 0.071079    
2022-12-29 10:23:43,719 - Epoch: [211][   40/  113]    Overall Loss 1.266300    Objective Loss 1.266300                                        LR 0.000006    Time 0.069217    
2022-12-29 10:23:44,353 - Epoch: [211][   50/  113]    Overall Loss 1.265872    Objective Loss 1.265872                                        LR 0.000006    Time 0.068047    
2022-12-29 10:23:44,986 - Epoch: [211][   60/  113]    Overall Loss 1.269006    Objective Loss 1.269006                                        LR 0.000006    Time 0.067252    
2022-12-29 10:23:45,622 - Epoch: [211][   70/  113]    Overall Loss 1.264609    Objective Loss 1.264609                                        LR 0.000006    Time 0.066722    
2022-12-29 10:23:46,254 - Epoch: [211][   80/  113]    Overall Loss 1.270739    Objective Loss 1.270739                                        LR 0.000006    Time 0.066283    
2022-12-29 10:23:46,887 - Epoch: [211][   90/  113]    Overall Loss 1.264452    Objective Loss 1.264452                                        LR 0.000006    Time 0.065936    
2022-12-29 10:23:47,522 - Epoch: [211][  100/  113]    Overall Loss 1.271556    Objective Loss 1.271556                                        LR 0.000006    Time 0.065688    
2022-12-29 10:23:48,151 - Epoch: [211][  110/  113]    Overall Loss 1.279175    Objective Loss 1.279175                                        LR 0.000006    Time 0.065431    
2022-12-29 10:23:48,330 - Epoch: [211][  113/  113]    Overall Loss 1.279891    Objective Loss 1.279891    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065279    
2022-12-29 10:23:48,388 - --- validate (epoch=211)-----------
2022-12-29 10:23:48,388 - 200 samples (16 per mini-batch)
2022-12-29 10:23:48,949 - Epoch: [211][   10/   13]    Loss 1.271991    Top1 51.875000    Top5 98.125000    
2022-12-29 10:23:49,034 - Epoch: [211][   13/   13]    Loss 1.295494    Top1 52.500000    Top5 97.500000    
2022-12-29 10:23:49,102 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.295

2022-12-29 10:23:49,102 - ==> Confusion:
[[16  1  0  8  2  0]
 [ 8 24  1 10  3  0]
 [ 1  9  9 10  5  0]
 [ 3  0  1 36  5  0]
 [ 4  4  2  9 18  0]
 [ 2  2  2  3  0  2]]

2022-12-29 10:23:49,105 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:49,105 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:49,128 - 

2022-12-29 10:23:49,129 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:49,964 - Epoch: [212][   10/  113]    Overall Loss 1.332665    Objective Loss 1.332665                                        LR 0.000006    Time 0.083394    
2022-12-29 10:23:50,603 - Epoch: [212][   20/  113]    Overall Loss 1.275085    Objective Loss 1.275085                                        LR 0.000006    Time 0.073630    
2022-12-29 10:23:51,241 - Epoch: [212][   30/  113]    Overall Loss 1.287992    Objective Loss 1.287992                                        LR 0.000006    Time 0.070347    
2022-12-29 10:23:51,882 - Epoch: [212][   40/  113]    Overall Loss 1.299847    Objective Loss 1.299847                                        LR 0.000006    Time 0.068775    
2022-12-29 10:23:52,529 - Epoch: [212][   50/  113]    Overall Loss 1.275245    Objective Loss 1.275245                                        LR 0.000006    Time 0.067945    
2022-12-29 10:23:53,166 - Epoch: [212][   60/  113]    Overall Loss 1.264333    Objective Loss 1.264333                                        LR 0.000006    Time 0.067234    
2022-12-29 10:23:53,804 - Epoch: [212][   70/  113]    Overall Loss 1.252644    Objective Loss 1.252644                                        LR 0.000006    Time 0.066692    
2022-12-29 10:23:54,437 - Epoch: [212][   80/  113]    Overall Loss 1.243906    Objective Loss 1.243906                                        LR 0.000006    Time 0.066269    
2022-12-29 10:23:55,073 - Epoch: [212][   90/  113]    Overall Loss 1.245979    Objective Loss 1.245979                                        LR 0.000006    Time 0.065971    
2022-12-29 10:23:55,707 - Epoch: [212][  100/  113]    Overall Loss 1.246648    Objective Loss 1.246648                                        LR 0.000006    Time 0.065708    
2022-12-29 10:23:56,339 - Epoch: [212][  110/  113]    Overall Loss 1.252508    Objective Loss 1.252508                                        LR 0.000006    Time 0.065478    
2022-12-29 10:23:56,515 - Epoch: [212][  113/  113]    Overall Loss 1.253564    Objective Loss 1.253564    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065286    
2022-12-29 10:23:56,579 - --- validate (epoch=212)-----------
2022-12-29 10:23:56,580 - 200 samples (16 per mini-batch)
2022-12-29 10:23:57,131 - Epoch: [212][   10/   13]    Loss 1.196591    Top1 54.375000    Top5 96.875000    
2022-12-29 10:23:57,218 - Epoch: [212][   13/   13]    Loss 1.245438    Top1 51.000000    Top5 97.000000    
2022-12-29 10:23:57,265 - ==> Top1: 51.000    Top5: 97.000    Loss: 1.245

2022-12-29 10:23:57,266 - ==> Confusion:
[[21  2  2  6  1  0]
 [ 1 18  3  5 16  1]
 [ 1  8 12  3  7  0]
 [ 4  2  1 31  6  1]
 [ 4  6  2  6 19  0]
 [ 1  1  2  2  4  1]]

2022-12-29 10:23:57,268 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:23:57,269 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:23:57,291 - 

2022-12-29 10:23:57,292 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:23:58,123 - Epoch: [213][   10/  113]    Overall Loss 1.454463    Objective Loss 1.454463                                        LR 0.000006    Time 0.083036    
2022-12-29 10:23:58,765 - Epoch: [213][   20/  113]    Overall Loss 1.394451    Objective Loss 1.394451                                        LR 0.000006    Time 0.073597    
2022-12-29 10:23:59,405 - Epoch: [213][   30/  113]    Overall Loss 1.326403    Objective Loss 1.326403                                        LR 0.000006    Time 0.070391    
2022-12-29 10:24:00,051 - Epoch: [213][   40/  113]    Overall Loss 1.306277    Objective Loss 1.306277                                        LR 0.000006    Time 0.068912    
2022-12-29 10:24:00,687 - Epoch: [213][   50/  113]    Overall Loss 1.284022    Objective Loss 1.284022                                        LR 0.000006    Time 0.067844    
2022-12-29 10:24:01,325 - Epoch: [213][   60/  113]    Overall Loss 1.274369    Objective Loss 1.274369                                        LR 0.000006    Time 0.067165    
2022-12-29 10:24:01,964 - Epoch: [213][   70/  113]    Overall Loss 1.278031    Objective Loss 1.278031                                        LR 0.000006    Time 0.066701    
2022-12-29 10:24:02,608 - Epoch: [213][   80/  113]    Overall Loss 1.264712    Objective Loss 1.264712                                        LR 0.000006    Time 0.066406    
2022-12-29 10:24:03,242 - Epoch: [213][   90/  113]    Overall Loss 1.261079    Objective Loss 1.261079                                        LR 0.000006    Time 0.066065    
2022-12-29 10:24:03,885 - Epoch: [213][  100/  113]    Overall Loss 1.243579    Objective Loss 1.243579                                        LR 0.000006    Time 0.065877    
2022-12-29 10:24:04,515 - Epoch: [213][  110/  113]    Overall Loss 1.240296    Objective Loss 1.240296                                        LR 0.000006    Time 0.065614    
2022-12-29 10:24:04,687 - Epoch: [213][  113/  113]    Overall Loss 1.241856    Objective Loss 1.241856    Top1 33.333333    Top5 95.833333    LR 0.000006    Time 0.065389    
2022-12-29 10:24:04,754 - --- validate (epoch=213)-----------
2022-12-29 10:24:04,754 - 200 samples (16 per mini-batch)
2022-12-29 10:24:05,325 - Epoch: [213][   10/   13]    Loss 1.167979    Top1 55.625000    Top5 97.500000    
2022-12-29 10:24:05,409 - Epoch: [213][   13/   13]    Loss 1.240755    Top1 51.500000    Top5 98.000000    
2022-12-29 10:24:05,480 - ==> Top1: 51.500    Top5: 98.000    Loss: 1.241

2022-12-29 10:24:05,480 - ==> Confusion:
[[24  6  0  7  0  0]
 [ 3 18  3  7  3  1]
 [ 1  4 10  6  3  0]
 [ 5  6  0 42  1  1]
 [ 1 11  1 16  7  0]
 [ 3  3  1  2  2  2]]

2022-12-29 10:24:05,483 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:05,484 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:05,496 - 

2022-12-29 10:24:05,496 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:06,345 - Epoch: [214][   10/  113]    Overall Loss 1.273220    Objective Loss 1.273220                                        LR 0.000006    Time 0.084804    
2022-12-29 10:24:06,988 - Epoch: [214][   20/  113]    Overall Loss 1.270123    Objective Loss 1.270123                                        LR 0.000006    Time 0.074499    
2022-12-29 10:24:07,628 - Epoch: [214][   30/  113]    Overall Loss 1.228359    Objective Loss 1.228359                                        LR 0.000006    Time 0.070989    
2022-12-29 10:24:08,277 - Epoch: [214][   40/  113]    Overall Loss 1.267756    Objective Loss 1.267756                                        LR 0.000006    Time 0.069453    
2022-12-29 10:24:08,915 - Epoch: [214][   50/  113]    Overall Loss 1.303359    Objective Loss 1.303359                                        LR 0.000006    Time 0.068312    
2022-12-29 10:24:09,547 - Epoch: [214][   60/  113]    Overall Loss 1.291431    Objective Loss 1.291431                                        LR 0.000006    Time 0.067448    
2022-12-29 10:24:10,184 - Epoch: [214][   70/  113]    Overall Loss 1.277016    Objective Loss 1.277016                                        LR 0.000006    Time 0.066910    
2022-12-29 10:24:10,817 - Epoch: [214][   80/  113]    Overall Loss 1.277987    Objective Loss 1.277987                                        LR 0.000006    Time 0.066449    
2022-12-29 10:24:11,451 - Epoch: [214][   90/  113]    Overall Loss 1.287507    Objective Loss 1.287507                                        LR 0.000006    Time 0.066108    
2022-12-29 10:24:12,087 - Epoch: [214][  100/  113]    Overall Loss 1.281584    Objective Loss 1.281584                                        LR 0.000006    Time 0.065853    
2022-12-29 10:24:12,719 - Epoch: [214][  110/  113]    Overall Loss 1.292090    Objective Loss 1.292090                                        LR 0.000006    Time 0.065605    
2022-12-29 10:24:12,892 - Epoch: [214][  113/  113]    Overall Loss 1.292379    Objective Loss 1.292379    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065393    
2022-12-29 10:24:12,948 - --- validate (epoch=214)-----------
2022-12-29 10:24:12,949 - 200 samples (16 per mini-batch)
2022-12-29 10:24:13,495 - Epoch: [214][   10/   13]    Loss 1.273240    Top1 50.625000    Top5 96.875000    
2022-12-29 10:24:13,579 - Epoch: [214][   13/   13]    Loss 1.198351    Top1 54.000000    Top5 97.500000    
2022-12-29 10:24:13,629 - ==> Top1: 54.000    Top5: 97.500    Loss: 1.198

2022-12-29 10:24:13,630 - ==> Confusion:
[[23  4  0  2  2  0]
 [ 3 22  1  2  5  0]
 [ 1  7 11  6 11  0]
 [ 2  8  0 33  5  0]
 [ 3 12  2 11 17  0]
 [ 0  2  0  2  1  2]]

2022-12-29 10:24:13,633 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:13,633 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:13,655 - 

2022-12-29 10:24:13,655 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:14,487 - Epoch: [215][   10/  113]    Overall Loss 1.414747    Objective Loss 1.414747                                        LR 0.000006    Time 0.083062    
2022-12-29 10:24:15,121 - Epoch: [215][   20/  113]    Overall Loss 1.339771    Objective Loss 1.339771                                        LR 0.000006    Time 0.073223    
2022-12-29 10:24:15,757 - Epoch: [215][   30/  113]    Overall Loss 1.311279    Objective Loss 1.311279                                        LR 0.000006    Time 0.069981    
2022-12-29 10:24:16,389 - Epoch: [215][   40/  113]    Overall Loss 1.319041    Objective Loss 1.319041                                        LR 0.000006    Time 0.068296    
2022-12-29 10:24:17,024 - Epoch: [215][   50/  113]    Overall Loss 1.311084    Objective Loss 1.311084                                        LR 0.000006    Time 0.067311    
2022-12-29 10:24:17,657 - Epoch: [215][   60/  113]    Overall Loss 1.306691    Objective Loss 1.306691                                        LR 0.000006    Time 0.066641    
2022-12-29 10:24:18,293 - Epoch: [215][   70/  113]    Overall Loss 1.302597    Objective Loss 1.302597                                        LR 0.000006    Time 0.066208    
2022-12-29 10:24:18,928 - Epoch: [215][   80/  113]    Overall Loss 1.305609    Objective Loss 1.305609                                        LR 0.000006    Time 0.065857    
2022-12-29 10:24:19,566 - Epoch: [215][   90/  113]    Overall Loss 1.303801    Objective Loss 1.303801                                        LR 0.000006    Time 0.065619    
2022-12-29 10:24:20,202 - Epoch: [215][  100/  113]    Overall Loss 1.305764    Objective Loss 1.305764                                        LR 0.000006    Time 0.065414    
2022-12-29 10:24:20,828 - Epoch: [215][  110/  113]    Overall Loss 1.310953    Objective Loss 1.310953                                        LR 0.000006    Time 0.065151    
2022-12-29 10:24:21,004 - Epoch: [215][  113/  113]    Overall Loss 1.314963    Objective Loss 1.314963    Top1 41.666667    Top5 91.666667    LR 0.000006    Time 0.064978    
2022-12-29 10:24:21,060 - --- validate (epoch=215)-----------
2022-12-29 10:24:21,060 - 200 samples (16 per mini-batch)
2022-12-29 10:24:21,612 - Epoch: [215][   10/   13]    Loss 1.158221    Top1 52.500000    Top5 98.750000    
2022-12-29 10:24:21,696 - Epoch: [215][   13/   13]    Loss 1.208545    Top1 50.000000    Top5 99.000000    
2022-12-29 10:24:21,756 - ==> Top1: 50.000    Top5: 99.000    Loss: 1.209

2022-12-29 10:24:21,757 - ==> Confusion:
[[16  3  0  5  4  0]
 [ 4 18  8  4 11  0]
 [ 1 10 11  5  4  0]
 [ 4  3  1 40  5  0]
 [ 1  8  1  9 12  0]
 [ 1  2  2  2  2  3]]

2022-12-29 10:24:21,759 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:21,759 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:21,777 - 

2022-12-29 10:24:21,778 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:22,629 - Epoch: [216][   10/  113]    Overall Loss 1.323479    Objective Loss 1.323479                                        LR 0.000006    Time 0.085067    
2022-12-29 10:24:23,264 - Epoch: [216][   20/  113]    Overall Loss 1.238654    Objective Loss 1.238654                                        LR 0.000006    Time 0.074241    
2022-12-29 10:24:23,902 - Epoch: [216][   30/  113]    Overall Loss 1.247651    Objective Loss 1.247651                                        LR 0.000006    Time 0.070759    
2022-12-29 10:24:24,545 - Epoch: [216][   40/  113]    Overall Loss 1.265347    Objective Loss 1.265347                                        LR 0.000006    Time 0.069119    
2022-12-29 10:24:25,181 - Epoch: [216][   50/  113]    Overall Loss 1.241760    Objective Loss 1.241760                                        LR 0.000006    Time 0.068007    
2022-12-29 10:24:25,817 - Epoch: [216][   60/  113]    Overall Loss 1.247877    Objective Loss 1.247877                                        LR 0.000006    Time 0.067269    
2022-12-29 10:24:26,454 - Epoch: [216][   70/  113]    Overall Loss 1.253394    Objective Loss 1.253394                                        LR 0.000006    Time 0.066757    
2022-12-29 10:24:27,089 - Epoch: [216][   80/  113]    Overall Loss 1.263127    Objective Loss 1.263127                                        LR 0.000006    Time 0.066337    
2022-12-29 10:24:27,722 - Epoch: [216][   90/  113]    Overall Loss 1.268795    Objective Loss 1.268795                                        LR 0.000006    Time 0.065996    
2022-12-29 10:24:28,358 - Epoch: [216][  100/  113]    Overall Loss 1.264407    Objective Loss 1.264407                                        LR 0.000006    Time 0.065753    
2022-12-29 10:24:28,991 - Epoch: [216][  110/  113]    Overall Loss 1.267644    Objective Loss 1.267644                                        LR 0.000006    Time 0.065517    
2022-12-29 10:24:29,164 - Epoch: [216][  113/  113]    Overall Loss 1.269370    Objective Loss 1.269370    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065310    
2022-12-29 10:24:29,213 - --- validate (epoch=216)-----------
2022-12-29 10:24:29,213 - 200 samples (16 per mini-batch)
2022-12-29 10:24:29,761 - Epoch: [216][   10/   13]    Loss 1.273180    Top1 52.500000    Top5 96.875000    
2022-12-29 10:24:29,845 - Epoch: [216][   13/   13]    Loss 1.276684    Top1 53.000000    Top5 96.500000    
2022-12-29 10:24:29,905 - ==> Top1: 53.000    Top5: 96.500    Loss: 1.277

2022-12-29 10:24:29,905 - ==> Confusion:
[[23  3  0  8  1  1]
 [ 5 23  6 10  4  0]
 [ 1  6 11  1  7  0]
 [ 5  4  1 30  2  1]
 [ 2  7  2  6 18  0]
 [ 2  3  2  3  1  1]]

2022-12-29 10:24:29,907 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:29,907 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:29,917 - 

2022-12-29 10:24:29,917 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:30,756 - Epoch: [217][   10/  113]    Overall Loss 1.341351    Objective Loss 1.341351                                        LR 0.000006    Time 0.083737    
2022-12-29 10:24:31,395 - Epoch: [217][   20/  113]    Overall Loss 1.264528    Objective Loss 1.264528                                        LR 0.000006    Time 0.073826    
2022-12-29 10:24:32,039 - Epoch: [217][   30/  113]    Overall Loss 1.279008    Objective Loss 1.279008                                        LR 0.000006    Time 0.070648    
2022-12-29 10:24:32,682 - Epoch: [217][   40/  113]    Overall Loss 1.234782    Objective Loss 1.234782                                        LR 0.000006    Time 0.069050    
2022-12-29 10:24:33,320 - Epoch: [217][   50/  113]    Overall Loss 1.238763    Objective Loss 1.238763                                        LR 0.000006    Time 0.067998    
2022-12-29 10:24:33,956 - Epoch: [217][   60/  113]    Overall Loss 1.240142    Objective Loss 1.240142                                        LR 0.000006    Time 0.067260    
2022-12-29 10:24:34,591 - Epoch: [217][   70/  113]    Overall Loss 1.241985    Objective Loss 1.241985                                        LR 0.000006    Time 0.066714    
2022-12-29 10:24:35,227 - Epoch: [217][   80/  113]    Overall Loss 1.247615    Objective Loss 1.247615                                        LR 0.000006    Time 0.066324    
2022-12-29 10:24:35,864 - Epoch: [217][   90/  113]    Overall Loss 1.246719    Objective Loss 1.246719                                        LR 0.000006    Time 0.066029    
2022-12-29 10:24:36,505 - Epoch: [217][  100/  113]    Overall Loss 1.243454    Objective Loss 1.243454                                        LR 0.000006    Time 0.065827    
2022-12-29 10:24:37,135 - Epoch: [217][  110/  113]    Overall Loss 1.244208    Objective Loss 1.244208                                        LR 0.000006    Time 0.065559    
2022-12-29 10:24:37,311 - Epoch: [217][  113/  113]    Overall Loss 1.244877    Objective Loss 1.244877    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065374    
2022-12-29 10:24:37,360 - --- validate (epoch=217)-----------
2022-12-29 10:24:37,360 - 200 samples (16 per mini-batch)
2022-12-29 10:24:37,911 - Epoch: [217][   10/   13]    Loss 1.210669    Top1 48.750000    Top5 97.500000    
2022-12-29 10:24:37,999 - Epoch: [217][   13/   13]    Loss 1.236551    Top1 50.000000    Top5 97.000000    
2022-12-29 10:24:38,062 - ==> Top1: 50.000    Top5: 97.000    Loss: 1.237

2022-12-29 10:24:38,062 - ==> Confusion:
[[23  0  2  6  4  0]
 [ 1 16  4  9  8  0]
 [ 0  8 16  6  7  0]
 [ 2  6  2 26  7  1]
 [ 2  4  1 10 16  0]
 [ 0  3  1  2  4  3]]

2022-12-29 10:24:38,066 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:38,066 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:38,080 - 

2022-12-29 10:24:38,080 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:38,912 - Epoch: [218][   10/  113]    Overall Loss 1.212778    Objective Loss 1.212778                                        LR 0.000006    Time 0.083081    
2022-12-29 10:24:39,552 - Epoch: [218][   20/  113]    Overall Loss 1.227064    Objective Loss 1.227064                                        LR 0.000006    Time 0.073494    
2022-12-29 10:24:40,195 - Epoch: [218][   30/  113]    Overall Loss 1.221110    Objective Loss 1.221110                                        LR 0.000006    Time 0.070419    
2022-12-29 10:24:40,832 - Epoch: [218][   40/  113]    Overall Loss 1.221986    Objective Loss 1.221986                                        LR 0.000006    Time 0.068739    
2022-12-29 10:24:41,467 - Epoch: [218][   50/  113]    Overall Loss 1.234659    Objective Loss 1.234659                                        LR 0.000006    Time 0.067676    
2022-12-29 10:24:42,108 - Epoch: [218][   60/  113]    Overall Loss 1.246756    Objective Loss 1.246756                                        LR 0.000006    Time 0.067066    
2022-12-29 10:24:42,742 - Epoch: [218][   70/  113]    Overall Loss 1.269514    Objective Loss 1.269514                                        LR 0.000006    Time 0.066542    
2022-12-29 10:24:43,377 - Epoch: [218][   80/  113]    Overall Loss 1.252414    Objective Loss 1.252414                                        LR 0.000006    Time 0.066155    
2022-12-29 10:24:44,013 - Epoch: [218][   90/  113]    Overall Loss 1.257183    Objective Loss 1.257183                                        LR 0.000006    Time 0.065857    
2022-12-29 10:24:44,650 - Epoch: [218][  100/  113]    Overall Loss 1.255690    Objective Loss 1.255690                                        LR 0.000006    Time 0.065640    
2022-12-29 10:24:45,274 - Epoch: [218][  110/  113]    Overall Loss 1.259819    Objective Loss 1.259819                                        LR 0.000006    Time 0.065344    
2022-12-29 10:24:45,448 - Epoch: [218][  113/  113]    Overall Loss 1.261900    Objective Loss 1.261900    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065145    
2022-12-29 10:24:45,507 - --- validate (epoch=218)-----------
2022-12-29 10:24:45,507 - 200 samples (16 per mini-batch)
2022-12-29 10:24:46,055 - Epoch: [218][   10/   13]    Loss 1.380716    Top1 44.375000    Top5 98.125000    
2022-12-29 10:24:46,142 - Epoch: [218][   13/   13]    Loss 1.336175    Top1 46.000000    Top5 98.000000    
2022-12-29 10:24:46,188 - ==> Top1: 46.000    Top5: 98.000    Loss: 1.336

2022-12-29 10:24:46,188 - ==> Confusion:
[[24  4  1  7  2  0]
 [ 3 22  3 12  8  0]
 [ 1  4 12  5  7  0]
 [ 1  7  3 22  5  0]
 [ 2  9  1  9 11  0]
 [ 3  2  2  3  4  1]]

2022-12-29 10:24:46,190 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:46,190 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:46,202 - 

2022-12-29 10:24:46,203 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:47,035 - Epoch: [219][   10/  113]    Overall Loss 1.407689    Objective Loss 1.407689                                        LR 0.000006    Time 0.083197    
2022-12-29 10:24:47,674 - Epoch: [219][   20/  113]    Overall Loss 1.325958    Objective Loss 1.325958                                        LR 0.000006    Time 0.073491    
2022-12-29 10:24:48,313 - Epoch: [219][   30/  113]    Overall Loss 1.301615    Objective Loss 1.301615                                        LR 0.000006    Time 0.070287    
2022-12-29 10:24:48,948 - Epoch: [219][   40/  113]    Overall Loss 1.268960    Objective Loss 1.268960                                        LR 0.000006    Time 0.068572    
2022-12-29 10:24:49,584 - Epoch: [219][   50/  113]    Overall Loss 1.268835    Objective Loss 1.268835                                        LR 0.000006    Time 0.067562    
2022-12-29 10:24:50,221 - Epoch: [219][   60/  113]    Overall Loss 1.252749    Objective Loss 1.252749                                        LR 0.000006    Time 0.066926    
2022-12-29 10:24:50,857 - Epoch: [219][   70/  113]    Overall Loss 1.243403    Objective Loss 1.243403                                        LR 0.000006    Time 0.066434    
2022-12-29 10:24:51,494 - Epoch: [219][   80/  113]    Overall Loss 1.236247    Objective Loss 1.236247                                        LR 0.000006    Time 0.066085    
2022-12-29 10:24:52,136 - Epoch: [219][   90/  113]    Overall Loss 1.236906    Objective Loss 1.236906                                        LR 0.000006    Time 0.065871    
2022-12-29 10:24:52,774 - Epoch: [219][  100/  113]    Overall Loss 1.235969    Objective Loss 1.235969                                        LR 0.000006    Time 0.065661    
2022-12-29 10:24:53,403 - Epoch: [219][  110/  113]    Overall Loss 1.240906    Objective Loss 1.240906                                        LR 0.000006    Time 0.065408    
2022-12-29 10:24:53,574 - Epoch: [219][  113/  113]    Overall Loss 1.241413    Objective Loss 1.241413    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065184    
2022-12-29 10:24:53,635 - --- validate (epoch=219)-----------
2022-12-29 10:24:53,636 - 200 samples (16 per mini-batch)
2022-12-29 10:24:54,191 - Epoch: [219][   10/   13]    Loss 1.301694    Top1 52.500000    Top5 96.250000    
2022-12-29 10:24:54,275 - Epoch: [219][   13/   13]    Loss 1.278632    Top1 52.500000    Top5 96.500000    
2022-12-29 10:24:54,322 - ==> Top1: 52.500    Top5: 96.500    Loss: 1.279

2022-12-29 10:24:54,323 - ==> Confusion:
[[20  1  0  1  4  0]
 [ 2 21  3  8  8  0]
 [ 2 17 20  3 10  0]
 [ 4  2  1 29  8  1]
 [ 2  3  2  5 14  1]
 [ 0  2  1  1  3  1]]

2022-12-29 10:24:54,325 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:24:54,326 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:24:54,346 - 

2022-12-29 10:24:54,346 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:24:55,178 - Epoch: [220][   10/  113]    Overall Loss 1.293830    Objective Loss 1.293830                                        LR 0.000006    Time 0.083106    
2022-12-29 10:24:55,817 - Epoch: [220][   20/  113]    Overall Loss 1.254784    Objective Loss 1.254784                                        LR 0.000006    Time 0.073448    
2022-12-29 10:24:56,457 - Epoch: [220][   30/  113]    Overall Loss 1.265972    Objective Loss 1.265972                                        LR 0.000006    Time 0.070274    
2022-12-29 10:24:57,094 - Epoch: [220][   40/  113]    Overall Loss 1.232883    Objective Loss 1.232883                                        LR 0.000006    Time 0.068620    
2022-12-29 10:24:57,729 - Epoch: [220][   50/  113]    Overall Loss 1.234755    Objective Loss 1.234755                                        LR 0.000006    Time 0.067604    
2022-12-29 10:24:58,365 - Epoch: [220][   60/  113]    Overall Loss 1.246649    Objective Loss 1.246649                                        LR 0.000006    Time 0.066914    
2022-12-29 10:24:59,001 - Epoch: [220][   70/  113]    Overall Loss 1.245650    Objective Loss 1.245650                                        LR 0.000006    Time 0.066441    
2022-12-29 10:24:59,634 - Epoch: [220][   80/  113]    Overall Loss 1.232263    Objective Loss 1.232263                                        LR 0.000006    Time 0.066043    
2022-12-29 10:25:00,274 - Epoch: [220][   90/  113]    Overall Loss 1.243793    Objective Loss 1.243793                                        LR 0.000006    Time 0.065804    
2022-12-29 10:25:00,905 - Epoch: [220][  100/  113]    Overall Loss 1.238142    Objective Loss 1.238142                                        LR 0.000006    Time 0.065536    
2022-12-29 10:25:01,529 - Epoch: [220][  110/  113]    Overall Loss 1.234200    Objective Loss 1.234200                                        LR 0.000006    Time 0.065242    
2022-12-29 10:25:01,701 - Epoch: [220][  113/  113]    Overall Loss 1.241099    Objective Loss 1.241099    Top1 33.333333    Top5 87.500000    LR 0.000006    Time 0.065030    
2022-12-29 10:25:01,764 - --- validate (epoch=220)-----------
2022-12-29 10:25:01,764 - 200 samples (16 per mini-batch)
2022-12-29 10:25:02,321 - Epoch: [220][   10/   13]    Loss 1.243001    Top1 51.875000    Top5 98.750000    
2022-12-29 10:25:02,407 - Epoch: [220][   13/   13]    Loss 1.274428    Top1 50.500000    Top5 98.000000    
2022-12-29 10:25:02,473 - ==> Top1: 50.500    Top5: 98.000    Loss: 1.274

2022-12-29 10:25:02,474 - ==> Confusion:
[[13  3  1  4  1  1]
 [ 4 25  2  5  6  0]
 [ 0  7 13  7  8  0]
 [ 3  4  3 33  2  1]
 [ 6 10  1  8 16  0]
 [ 4  2  0  2  4  1]]

2022-12-29 10:25:02,476 - ==> Best [Top1: 55.500   Top5: 100.000   Sparsity:0.00   Params: 289216 on epoch: 192]
2022-12-29 10:25:02,477 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:02,498 - 

2022-12-29 10:25:02,498 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:03,329 - Epoch: [221][   10/  113]    Overall Loss 1.284016    Objective Loss 1.284016                                        LR 0.000006    Time 0.083006    
2022-12-29 10:25:03,965 - Epoch: [221][   20/  113]    Overall Loss 1.234759    Objective Loss 1.234759                                        LR 0.000006    Time 0.073248    
2022-12-29 10:25:04,611 - Epoch: [221][   30/  113]    Overall Loss 1.261569    Objective Loss 1.261569                                        LR 0.000006    Time 0.070355    
2022-12-29 10:25:05,251 - Epoch: [221][   40/  113]    Overall Loss 1.287607    Objective Loss 1.287607                                        LR 0.000006    Time 0.068769    
2022-12-29 10:25:05,888 - Epoch: [221][   50/  113]    Overall Loss 1.274940    Objective Loss 1.274940                                        LR 0.000006    Time 0.067739    
2022-12-29 10:25:06,525 - Epoch: [221][   60/  113]    Overall Loss 1.270667    Objective Loss 1.270667                                        LR 0.000006    Time 0.067064    
2022-12-29 10:25:07,164 - Epoch: [221][   70/  113]    Overall Loss 1.279270    Objective Loss 1.279270                                        LR 0.000006    Time 0.066597    
2022-12-29 10:25:07,803 - Epoch: [221][   80/  113]    Overall Loss 1.272315    Objective Loss 1.272315                                        LR 0.000006    Time 0.066260    
2022-12-29 10:25:08,446 - Epoch: [221][   90/  113]    Overall Loss 1.278240    Objective Loss 1.278240                                        LR 0.000006    Time 0.066032    
2022-12-29 10:25:09,084 - Epoch: [221][  100/  113]    Overall Loss 1.279448    Objective Loss 1.279448                                        LR 0.000006    Time 0.065800    
2022-12-29 10:25:09,719 - Epoch: [221][  110/  113]    Overall Loss 1.283319    Objective Loss 1.283319                                        LR 0.000006    Time 0.065590    
2022-12-29 10:25:09,897 - Epoch: [221][  113/  113]    Overall Loss 1.284593    Objective Loss 1.284593    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.065422    
2022-12-29 10:25:09,946 - --- validate (epoch=221)-----------
2022-12-29 10:25:09,946 - 200 samples (16 per mini-batch)
2022-12-29 10:25:10,505 - Epoch: [221][   10/   13]    Loss 1.184898    Top1 56.875000    Top5 97.500000    
2022-12-29 10:25:10,592 - Epoch: [221][   13/   13]    Loss 1.180750    Top1 56.000000    Top5 98.000000    
2022-12-29 10:25:10,643 - ==> Top1: 56.000    Top5: 98.000    Loss: 1.181

2022-12-29 10:25:10,643 - ==> Confusion:
[[21  3  0  2  1  0]
 [ 1 25  6  3  5  1]
 [ 0  8 12  7  6  0]
 [ 6  3  0 32  5  1]
 [ 6 10  3  5 21  1]
 [ 1  0  0  2  2  1]]

2022-12-29 10:25:10,645 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:10,645 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:10,667 - 

2022-12-29 10:25:10,668 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:11,513 - Epoch: [222][   10/  113]    Overall Loss 1.253764    Objective Loss 1.253764                                        LR 0.000006    Time 0.084351    
2022-12-29 10:25:12,150 - Epoch: [222][   20/  113]    Overall Loss 1.276368    Objective Loss 1.276368                                        LR 0.000006    Time 0.073973    
2022-12-29 10:25:12,792 - Epoch: [222][   30/  113]    Overall Loss 1.264707    Objective Loss 1.264707                                        LR 0.000006    Time 0.070704    
2022-12-29 10:25:13,429 - Epoch: [222][   40/  113]    Overall Loss 1.271381    Objective Loss 1.271381                                        LR 0.000006    Time 0.068936    
2022-12-29 10:25:14,060 - Epoch: [222][   50/  113]    Overall Loss 1.274333    Objective Loss 1.274333                                        LR 0.000006    Time 0.067777    
2022-12-29 10:25:14,695 - Epoch: [222][   60/  113]    Overall Loss 1.273288    Objective Loss 1.273288                                        LR 0.000006    Time 0.067051    
2022-12-29 10:25:15,334 - Epoch: [222][   70/  113]    Overall Loss 1.273413    Objective Loss 1.273413                                        LR 0.000006    Time 0.066588    
2022-12-29 10:25:15,969 - Epoch: [222][   80/  113]    Overall Loss 1.256834    Objective Loss 1.256834                                        LR 0.000006    Time 0.066195    
2022-12-29 10:25:16,605 - Epoch: [222][   90/  113]    Overall Loss 1.260308    Objective Loss 1.260308                                        LR 0.000006    Time 0.065911    
2022-12-29 10:25:17,235 - Epoch: [222][  100/  113]    Overall Loss 1.256414    Objective Loss 1.256414                                        LR 0.000006    Time 0.065614    
2022-12-29 10:25:17,859 - Epoch: [222][  110/  113]    Overall Loss 1.253993    Objective Loss 1.253993                                        LR 0.000006    Time 0.065314    
2022-12-29 10:25:18,031 - Epoch: [222][  113/  113]    Overall Loss 1.256771    Objective Loss 1.256771    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065101    
2022-12-29 10:25:18,092 - --- validate (epoch=222)-----------
2022-12-29 10:25:18,092 - 200 samples (16 per mini-batch)
2022-12-29 10:25:18,645 - Epoch: [222][   10/   13]    Loss 1.285433    Top1 48.125000    Top5 96.875000    
2022-12-29 10:25:18,729 - Epoch: [222][   13/   13]    Loss 1.322824    Top1 46.500000    Top5 97.000000    
2022-12-29 10:25:18,777 - ==> Top1: 46.500    Top5: 97.000    Loss: 1.323

2022-12-29 10:25:18,777 - ==> Confusion:
[[23  1  0  7  3  0]
 [ 4 17  2  5  7  1]
 [ 5  6 11  7  7  0]
 [ 7  4  2 23  3  0]
 [ 5 10  4  9 19  0]
 [ 0  2  3  1  2  0]]

2022-12-29 10:25:18,780 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:18,780 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:18,801 - 

2022-12-29 10:25:18,801 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:19,651 - Epoch: [223][   10/  113]    Overall Loss 1.294302    Objective Loss 1.294302                                        LR 0.000006    Time 0.084780    
2022-12-29 10:25:20,287 - Epoch: [223][   20/  113]    Overall Loss 1.320440    Objective Loss 1.320440                                        LR 0.000006    Time 0.074170    
2022-12-29 10:25:20,934 - Epoch: [223][   30/  113]    Overall Loss 1.290656    Objective Loss 1.290656                                        LR 0.000006    Time 0.070994    
2022-12-29 10:25:21,567 - Epoch: [223][   40/  113]    Overall Loss 1.275933    Objective Loss 1.275933                                        LR 0.000006    Time 0.069046    
2022-12-29 10:25:22,207 - Epoch: [223][   50/  113]    Overall Loss 1.281435    Objective Loss 1.281435                                        LR 0.000006    Time 0.068032    
2022-12-29 10:25:22,842 - Epoch: [223][   60/  113]    Overall Loss 1.266194    Objective Loss 1.266194                                        LR 0.000006    Time 0.067269    
2022-12-29 10:25:23,478 - Epoch: [223][   70/  113]    Overall Loss 1.276742    Objective Loss 1.276742                                        LR 0.000006    Time 0.066733    
2022-12-29 10:25:24,109 - Epoch: [223][   80/  113]    Overall Loss 1.270742    Objective Loss 1.270742                                        LR 0.000006    Time 0.066273    
2022-12-29 10:25:24,749 - Epoch: [223][   90/  113]    Overall Loss 1.272061    Objective Loss 1.272061                                        LR 0.000006    Time 0.066019    
2022-12-29 10:25:25,384 - Epoch: [223][  100/  113]    Overall Loss 1.272977    Objective Loss 1.272977                                        LR 0.000006    Time 0.065764    
2022-12-29 10:25:26,014 - Epoch: [223][  110/  113]    Overall Loss 1.272786    Objective Loss 1.272786                                        LR 0.000006    Time 0.065511    
2022-12-29 10:25:26,187 - Epoch: [223][  113/  113]    Overall Loss 1.276634    Objective Loss 1.276634    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065297    
2022-12-29 10:25:26,251 - --- validate (epoch=223)-----------
2022-12-29 10:25:26,252 - 200 samples (16 per mini-batch)
2022-12-29 10:25:26,808 - Epoch: [223][   10/   13]    Loss 1.236393    Top1 55.625000    Top5 98.750000    
2022-12-29 10:25:26,892 - Epoch: [223][   13/   13]    Loss 1.235289    Top1 55.000000    Top5 98.500000    
2022-12-29 10:25:26,943 - ==> Top1: 55.000    Top5: 98.500    Loss: 1.235

2022-12-29 10:25:26,944 - ==> Confusion:
[[ 9  5  0  6  1  0]
 [ 3 27  4  7  5  0]
 [ 1  8 16  4  2  0]
 [ 1  5  0 44  1  0]
 [ 3 10  3 12 10  1]
 [ 0  3  0  4  1  4]]

2022-12-29 10:25:26,945 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:26,946 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:26,955 - 

2022-12-29 10:25:26,956 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:27,785 - Epoch: [224][   10/  113]    Overall Loss 1.363882    Objective Loss 1.363882                                        LR 0.000006    Time 0.082886    
2022-12-29 10:25:28,424 - Epoch: [224][   20/  113]    Overall Loss 1.336151    Objective Loss 1.336151                                        LR 0.000006    Time 0.073364    
2022-12-29 10:25:29,066 - Epoch: [224][   30/  113]    Overall Loss 1.312213    Objective Loss 1.312213                                        LR 0.000006    Time 0.070264    
2022-12-29 10:25:29,700 - Epoch: [224][   40/  113]    Overall Loss 1.286479    Objective Loss 1.286479                                        LR 0.000006    Time 0.068541    
2022-12-29 10:25:30,334 - Epoch: [224][   50/  113]    Overall Loss 1.270975    Objective Loss 1.270975                                        LR 0.000006    Time 0.067503    
2022-12-29 10:25:30,966 - Epoch: [224][   60/  113]    Overall Loss 1.273226    Objective Loss 1.273226                                        LR 0.000006    Time 0.066776    
2022-12-29 10:25:31,600 - Epoch: [224][   70/  113]    Overall Loss 1.274566    Objective Loss 1.274566                                        LR 0.000006    Time 0.066288    
2022-12-29 10:25:32,238 - Epoch: [224][   80/  113]    Overall Loss 1.268683    Objective Loss 1.268683                                        LR 0.000006    Time 0.065973    
2022-12-29 10:25:32,870 - Epoch: [224][   90/  113]    Overall Loss 1.278284    Objective Loss 1.278284                                        LR 0.000006    Time 0.065661    
2022-12-29 10:25:33,501 - Epoch: [224][  100/  113]    Overall Loss 1.266957    Objective Loss 1.266957                                        LR 0.000006    Time 0.065395    
2022-12-29 10:25:34,122 - Epoch: [224][  110/  113]    Overall Loss 1.275700    Objective Loss 1.275700                                        LR 0.000006    Time 0.065093    
2022-12-29 10:25:34,295 - Epoch: [224][  113/  113]    Overall Loss 1.274866    Objective Loss 1.274866    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064895    
2022-12-29 10:25:34,344 - --- validate (epoch=224)-----------
2022-12-29 10:25:34,344 - 200 samples (16 per mini-batch)
2022-12-29 10:25:34,902 - Epoch: [224][   10/   13]    Loss 1.266562    Top1 50.625000    Top5 97.500000    
2022-12-29 10:25:34,989 - Epoch: [224][   13/   13]    Loss 1.256217    Top1 51.000000    Top5 97.000000    
2022-12-29 10:25:35,053 - ==> Top1: 51.000    Top5: 97.000    Loss: 1.256

2022-12-29 10:25:35,054 - ==> Confusion:
[[20  1  0  6  2  2]
 [ 2 23  4 10  9  1]
 [ 2  3  9  7  7  0]
 [ 5  4  0 34  1  2]
 [ 4  9  2  9 14  0]
 [ 1  3  0  1  1  2]]

2022-12-29 10:25:35,058 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:35,058 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:35,081 - 

2022-12-29 10:25:35,082 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:35,928 - Epoch: [225][   10/  113]    Overall Loss 1.352110    Objective Loss 1.352110                                        LR 0.000006    Time 0.084413    
2022-12-29 10:25:36,570 - Epoch: [225][   20/  113]    Overall Loss 1.315739    Objective Loss 1.315739                                        LR 0.000006    Time 0.074313    
2022-12-29 10:25:37,213 - Epoch: [225][   30/  113]    Overall Loss 1.341836    Objective Loss 1.341836                                        LR 0.000006    Time 0.070955    
2022-12-29 10:25:37,847 - Epoch: [225][   40/  113]    Overall Loss 1.341733    Objective Loss 1.341733                                        LR 0.000006    Time 0.069051    
2022-12-29 10:25:38,475 - Epoch: [225][   50/  113]    Overall Loss 1.313707    Objective Loss 1.313707                                        LR 0.000006    Time 0.067782    
2022-12-29 10:25:39,103 - Epoch: [225][   60/  113]    Overall Loss 1.303188    Objective Loss 1.303188                                        LR 0.000006    Time 0.066957    
2022-12-29 10:25:39,734 - Epoch: [225][   70/  113]    Overall Loss 1.296253    Objective Loss 1.296253                                        LR 0.000006    Time 0.066387    
2022-12-29 10:25:40,360 - Epoch: [225][   80/  113]    Overall Loss 1.277715    Objective Loss 1.277715                                        LR 0.000006    Time 0.065915    
2022-12-29 10:25:40,993 - Epoch: [225][   90/  113]    Overall Loss 1.286690    Objective Loss 1.286690                                        LR 0.000006    Time 0.065620    
2022-12-29 10:25:41,625 - Epoch: [225][  100/  113]    Overall Loss 1.285688    Objective Loss 1.285688                                        LR 0.000006    Time 0.065371    
2022-12-29 10:25:42,262 - Epoch: [225][  110/  113]    Overall Loss 1.285985    Objective Loss 1.285985                                        LR 0.000006    Time 0.065219    
2022-12-29 10:25:42,435 - Epoch: [225][  113/  113]    Overall Loss 1.293757    Objective Loss 1.293757    Top1 37.500000    Top5 91.666667    LR 0.000006    Time 0.065007    
2022-12-29 10:25:42,484 - --- validate (epoch=225)-----------
2022-12-29 10:25:42,484 - 200 samples (16 per mini-batch)
2022-12-29 10:25:43,032 - Epoch: [225][   10/   13]    Loss 1.276245    Top1 41.875000    Top5 98.125000    
2022-12-29 10:25:43,117 - Epoch: [225][   13/   13]    Loss 1.301384    Top1 42.000000    Top5 97.500000    
2022-12-29 10:25:43,164 - ==> Top1: 42.000    Top5: 97.500    Loss: 1.301

2022-12-29 10:25:43,164 - ==> Confusion:
[[16  4  0  4  4  1]
 [ 2 14  3 11 11  2]
 [ 1 10 12  6  5  0]
 [ 5  4  3 28  1  0]
 [ 2 15  3 10 13  1]
 [ 0  2  0  2  4  1]]

2022-12-29 10:25:43,166 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:43,166 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:43,181 - 

2022-12-29 10:25:43,181 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:44,021 - Epoch: [226][   10/  113]    Overall Loss 1.365001    Objective Loss 1.365001                                        LR 0.000006    Time 0.083951    
2022-12-29 10:25:44,669 - Epoch: [226][   20/  113]    Overall Loss 1.321239    Objective Loss 1.321239                                        LR 0.000006    Time 0.074304    
2022-12-29 10:25:45,303 - Epoch: [226][   30/  113]    Overall Loss 1.291885    Objective Loss 1.291885                                        LR 0.000006    Time 0.070651    
2022-12-29 10:25:45,940 - Epoch: [226][   40/  113]    Overall Loss 1.273422    Objective Loss 1.273422                                        LR 0.000006    Time 0.068906    
2022-12-29 10:25:46,570 - Epoch: [226][   50/  113]    Overall Loss 1.276144    Objective Loss 1.276144                                        LR 0.000006    Time 0.067721    
2022-12-29 10:25:47,209 - Epoch: [226][   60/  113]    Overall Loss 1.285349    Objective Loss 1.285349                                        LR 0.000006    Time 0.067079    
2022-12-29 10:25:47,842 - Epoch: [226][   70/  113]    Overall Loss 1.276506    Objective Loss 1.276506                                        LR 0.000006    Time 0.066520    
2022-12-29 10:25:48,471 - Epoch: [226][   80/  113]    Overall Loss 1.271034    Objective Loss 1.271034                                        LR 0.000006    Time 0.066067    
2022-12-29 10:25:49,105 - Epoch: [226][   90/  113]    Overall Loss 1.268263    Objective Loss 1.268263                                        LR 0.000006    Time 0.065772    
2022-12-29 10:25:49,740 - Epoch: [226][  100/  113]    Overall Loss 1.273365    Objective Loss 1.273365                                        LR 0.000006    Time 0.065535    
2022-12-29 10:25:50,371 - Epoch: [226][  110/  113]    Overall Loss 1.260271    Objective Loss 1.260271                                        LR 0.000006    Time 0.065307    
2022-12-29 10:25:50,544 - Epoch: [226][  113/  113]    Overall Loss 1.268985    Objective Loss 1.268985    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.065101    
2022-12-29 10:25:50,605 - --- validate (epoch=226)-----------
2022-12-29 10:25:50,605 - 200 samples (16 per mini-batch)
2022-12-29 10:25:51,168 - Epoch: [226][   10/   13]    Loss 1.235214    Top1 52.500000    Top5 97.500000    
2022-12-29 10:25:51,256 - Epoch: [226][   13/   13]    Loss 1.218676    Top1 54.500000    Top5 98.000000    
2022-12-29 10:25:51,301 - ==> Top1: 54.500    Top5: 98.000    Loss: 1.219

2022-12-29 10:25:51,301 - ==> Confusion:
[[23  1  1  3  2  0]
 [ 2 16  3  5  5  0]
 [ 3  5 14  7  6  2]
 [ 0  4  2 34  8  3]
 [ 2 13  0  9 21  0]
 [ 1  1  0  2  1  1]]

2022-12-29 10:25:51,303 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:51,304 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:51,315 - 

2022-12-29 10:25:51,315 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:25:52,142 - Epoch: [227][   10/  113]    Overall Loss 1.247011    Objective Loss 1.247011                                        LR 0.000006    Time 0.082592    
2022-12-29 10:25:52,781 - Epoch: [227][   20/  113]    Overall Loss 1.281864    Objective Loss 1.281864                                        LR 0.000006    Time 0.073204    
2022-12-29 10:25:53,413 - Epoch: [227][   30/  113]    Overall Loss 1.241233    Objective Loss 1.241233                                        LR 0.000006    Time 0.069868    
2022-12-29 10:25:54,047 - Epoch: [227][   40/  113]    Overall Loss 1.237059    Objective Loss 1.237059                                        LR 0.000006    Time 0.068229    
2022-12-29 10:25:54,679 - Epoch: [227][   50/  113]    Overall Loss 1.224591    Objective Loss 1.224591                                        LR 0.000006    Time 0.067211    
2022-12-29 10:25:55,305 - Epoch: [227][   60/  113]    Overall Loss 1.238189    Objective Loss 1.238189                                        LR 0.000006    Time 0.066437    
2022-12-29 10:25:55,935 - Epoch: [227][   70/  113]    Overall Loss 1.231560    Objective Loss 1.231560                                        LR 0.000006    Time 0.065939    
2022-12-29 10:25:56,562 - Epoch: [227][   80/  113]    Overall Loss 1.225049    Objective Loss 1.225049                                        LR 0.000006    Time 0.065524    
2022-12-29 10:25:57,199 - Epoch: [227][   90/  113]    Overall Loss 1.227744    Objective Loss 1.227744                                        LR 0.000006    Time 0.065323    
2022-12-29 10:25:57,828 - Epoch: [227][  100/  113]    Overall Loss 1.232134    Objective Loss 1.232134                                        LR 0.000006    Time 0.065078    
2022-12-29 10:25:58,453 - Epoch: [227][  110/  113]    Overall Loss 1.237119    Objective Loss 1.237119                                        LR 0.000006    Time 0.064834    
2022-12-29 10:25:58,629 - Epoch: [227][  113/  113]    Overall Loss 1.241129    Objective Loss 1.241129    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064670    
2022-12-29 10:25:58,669 - --- validate (epoch=227)-----------
2022-12-29 10:25:58,670 - 200 samples (16 per mini-batch)
2022-12-29 10:25:59,214 - Epoch: [227][   10/   13]    Loss 1.277844    Top1 48.750000    Top5 96.250000    
2022-12-29 10:25:59,300 - Epoch: [227][   13/   13]    Loss 1.261327    Top1 48.500000    Top5 96.500000    
2022-12-29 10:25:59,351 - ==> Top1: 48.500    Top5: 96.500    Loss: 1.261

2022-12-29 10:25:59,352 - ==> Confusion:
[[14  3  2  4  0  0]
 [ 2 30  4 12  3  0]
 [ 1  9 13  7  7  0]
 [ 3  9  1 26  6  0]
 [ 3  7  1 12 14  0]
 [ 1  4  0  1  1  0]]

2022-12-29 10:25:59,354 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:25:59,354 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:25:59,374 - 

2022-12-29 10:25:59,374 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:00,216 - Epoch: [228][   10/  113]    Overall Loss 1.257113    Objective Loss 1.257113                                        LR 0.000006    Time 0.083949    
2022-12-29 10:26:00,854 - Epoch: [228][   20/  113]    Overall Loss 1.244285    Objective Loss 1.244285                                        LR 0.000006    Time 0.073869    
2022-12-29 10:26:01,489 - Epoch: [228][   30/  113]    Overall Loss 1.276756    Objective Loss 1.276756                                        LR 0.000006    Time 0.070402    
2022-12-29 10:26:02,123 - Epoch: [228][   40/  113]    Overall Loss 1.261635    Objective Loss 1.261635                                        LR 0.000006    Time 0.068628    
2022-12-29 10:26:02,767 - Epoch: [228][   50/  113]    Overall Loss 1.273901    Objective Loss 1.273901                                        LR 0.000006    Time 0.067781    
2022-12-29 10:26:03,405 - Epoch: [228][   60/  113]    Overall Loss 1.269432    Objective Loss 1.269432                                        LR 0.000006    Time 0.067113    
2022-12-29 10:26:04,041 - Epoch: [228][   70/  113]    Overall Loss 1.271893    Objective Loss 1.271893                                        LR 0.000006    Time 0.066598    
2022-12-29 10:26:04,676 - Epoch: [228][   80/  113]    Overall Loss 1.274465    Objective Loss 1.274465                                        LR 0.000006    Time 0.066204    
2022-12-29 10:26:05,312 - Epoch: [228][   90/  113]    Overall Loss 1.259935    Objective Loss 1.259935                                        LR 0.000006    Time 0.065913    
2022-12-29 10:26:05,948 - Epoch: [228][  100/  113]    Overall Loss 1.256376    Objective Loss 1.256376                                        LR 0.000006    Time 0.065673    
2022-12-29 10:26:06,575 - Epoch: [228][  110/  113]    Overall Loss 1.265861    Objective Loss 1.265861                                        LR 0.000006    Time 0.065397    
2022-12-29 10:26:06,747 - Epoch: [228][  113/  113]    Overall Loss 1.261779    Objective Loss 1.261779    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.065183    
2022-12-29 10:26:06,814 - --- validate (epoch=228)-----------
2022-12-29 10:26:06,815 - 200 samples (16 per mini-batch)
2022-12-29 10:26:07,368 - Epoch: [228][   10/   13]    Loss 1.324359    Top1 46.875000    Top5 98.750000    
2022-12-29 10:26:07,456 - Epoch: [228][   13/   13]    Loss 1.326492    Top1 46.000000    Top5 98.000000    
2022-12-29 10:26:07,509 - ==> Top1: 46.000    Top5: 98.000    Loss: 1.326

2022-12-29 10:26:07,510 - ==> Confusion:
[[25  5  0  8  5  0]
 [ 3 25  4 11  7  1]
 [ 2  6 10  2 10  0]
 [ 3  6  1 15  2  0]
 [ 3  4  5  9 14  0]
 [ 4  2  3  1  1  3]]

2022-12-29 10:26:07,512 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:07,512 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:07,531 - 

2022-12-29 10:26:07,532 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:08,368 - Epoch: [229][   10/  113]    Overall Loss 1.269441    Objective Loss 1.269441                                        LR 0.000006    Time 0.083569    
2022-12-29 10:26:09,009 - Epoch: [229][   20/  113]    Overall Loss 1.236604    Objective Loss 1.236604                                        LR 0.000006    Time 0.073812    
2022-12-29 10:26:09,652 - Epoch: [229][   30/  113]    Overall Loss 1.261890    Objective Loss 1.261890                                        LR 0.000006    Time 0.070598    
2022-12-29 10:26:10,288 - Epoch: [229][   40/  113]    Overall Loss 1.252961    Objective Loss 1.252961                                        LR 0.000006    Time 0.068858    
2022-12-29 10:26:10,920 - Epoch: [229][   50/  113]    Overall Loss 1.246559    Objective Loss 1.246559                                        LR 0.000006    Time 0.067715    
2022-12-29 10:26:11,553 - Epoch: [229][   60/  113]    Overall Loss 1.247796    Objective Loss 1.247796                                        LR 0.000006    Time 0.066966    
2022-12-29 10:26:12,193 - Epoch: [229][   70/  113]    Overall Loss 1.255918    Objective Loss 1.255918                                        LR 0.000006    Time 0.066530    
2022-12-29 10:26:12,824 - Epoch: [229][   80/  113]    Overall Loss 1.259783    Objective Loss 1.259783                                        LR 0.000006    Time 0.066099    
2022-12-29 10:26:13,458 - Epoch: [229][   90/  113]    Overall Loss 1.257766    Objective Loss 1.257766                                        LR 0.000006    Time 0.065795    
2022-12-29 10:26:14,091 - Epoch: [229][  100/  113]    Overall Loss 1.262677    Objective Loss 1.262677                                        LR 0.000006    Time 0.065533    
2022-12-29 10:26:14,719 - Epoch: [229][  110/  113]    Overall Loss 1.275499    Objective Loss 1.275499                                        LR 0.000006    Time 0.065279    
2022-12-29 10:26:14,891 - Epoch: [229][  113/  113]    Overall Loss 1.276130    Objective Loss 1.276130    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065069    
2022-12-29 10:26:14,938 - --- validate (epoch=229)-----------
2022-12-29 10:26:14,938 - 200 samples (16 per mini-batch)
2022-12-29 10:26:15,485 - Epoch: [229][   10/   13]    Loss 1.261717    Top1 48.125000    Top5 97.500000    
2022-12-29 10:26:15,571 - Epoch: [229][   13/   13]    Loss 1.253746    Top1 50.500000    Top5 98.000000    
2022-12-29 10:26:15,628 - ==> Top1: 50.500    Top5: 98.000    Loss: 1.254

2022-12-29 10:26:15,629 - ==> Confusion:
[[16  1  1  2  1  1]
 [ 3 21  3  7  7  2]
 [ 2  7 11  4  8  1]
 [ 5  5  1 35  0  1]
 [ 3 12  4  7 16  2]
 [ 4  1  0  1  3  2]]

2022-12-29 10:26:15,631 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:15,631 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:15,650 - 

2022-12-29 10:26:15,650 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:16,487 - Epoch: [230][   10/  113]    Overall Loss 1.322567    Objective Loss 1.322567                                        LR 0.000006    Time 0.083603    
2022-12-29 10:26:17,129 - Epoch: [230][   20/  113]    Overall Loss 1.226060    Objective Loss 1.226060                                        LR 0.000006    Time 0.073862    
2022-12-29 10:26:17,766 - Epoch: [230][   30/  113]    Overall Loss 1.249548    Objective Loss 1.249548                                        LR 0.000006    Time 0.070477    
2022-12-29 10:26:18,405 - Epoch: [230][   40/  113]    Overall Loss 1.265246    Objective Loss 1.265246                                        LR 0.000006    Time 0.068818    
2022-12-29 10:26:19,041 - Epoch: [230][   50/  113]    Overall Loss 1.262903    Objective Loss 1.262903                                        LR 0.000006    Time 0.067763    
2022-12-29 10:26:19,680 - Epoch: [230][   60/  113]    Overall Loss 1.237735    Objective Loss 1.237735                                        LR 0.000006    Time 0.067106    
2022-12-29 10:26:20,312 - Epoch: [230][   70/  113]    Overall Loss 1.236171    Objective Loss 1.236171                                        LR 0.000006    Time 0.066545    
2022-12-29 10:26:20,952 - Epoch: [230][   80/  113]    Overall Loss 1.234965    Objective Loss 1.234965                                        LR 0.000006    Time 0.066230    
2022-12-29 10:26:21,587 - Epoch: [230][   90/  113]    Overall Loss 1.236401    Objective Loss 1.236401                                        LR 0.000006    Time 0.065921    
2022-12-29 10:26:22,225 - Epoch: [230][  100/  113]    Overall Loss 1.238086    Objective Loss 1.238086                                        LR 0.000006    Time 0.065698    
2022-12-29 10:26:22,848 - Epoch: [230][  110/  113]    Overall Loss 1.241391    Objective Loss 1.241391                                        LR 0.000006    Time 0.065386    
2022-12-29 10:26:23,026 - Epoch: [230][  113/  113]    Overall Loss 1.238862    Objective Loss 1.238862    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065226    
2022-12-29 10:26:23,076 - --- validate (epoch=230)-----------
2022-12-29 10:26:23,077 - 200 samples (16 per mini-batch)
2022-12-29 10:26:23,625 - Epoch: [230][   10/   13]    Loss 1.323212    Top1 46.250000    Top5 96.875000    
2022-12-29 10:26:23,710 - Epoch: [230][   13/   13]    Loss 1.325440    Top1 47.000000    Top5 96.500000    
2022-12-29 10:26:23,763 - ==> Top1: 47.000    Top5: 96.500    Loss: 1.325

2022-12-29 10:26:23,763 - ==> Confusion:
[[12  3  0  8  0  0]
 [ 2 27  5 11  7  0]
 [ 2  7 11  5 10  0]
 [ 2  6  1 28  4  2]
 [ 1  8  5  6 16  0]
 [ 1  3  1  1  5  0]]

2022-12-29 10:26:23,766 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:23,766 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:23,788 - 

2022-12-29 10:26:23,788 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:24,619 - Epoch: [231][   10/  113]    Overall Loss 1.295509    Objective Loss 1.295509                                        LR 0.000006    Time 0.082954    
2022-12-29 10:26:25,261 - Epoch: [231][   20/  113]    Overall Loss 1.308898    Objective Loss 1.308898                                        LR 0.000006    Time 0.073591    
2022-12-29 10:26:25,901 - Epoch: [231][   30/  113]    Overall Loss 1.306695    Objective Loss 1.306695                                        LR 0.000006    Time 0.070353    
2022-12-29 10:26:26,537 - Epoch: [231][   40/  113]    Overall Loss 1.272435    Objective Loss 1.272435                                        LR 0.000006    Time 0.068658    
2022-12-29 10:26:27,173 - Epoch: [231][   50/  113]    Overall Loss 1.275683    Objective Loss 1.275683                                        LR 0.000006    Time 0.067631    
2022-12-29 10:26:27,809 - Epoch: [231][   60/  113]    Overall Loss 1.273414    Objective Loss 1.273414                                        LR 0.000006    Time 0.066954    
2022-12-29 10:26:28,443 - Epoch: [231][   70/  113]    Overall Loss 1.268292    Objective Loss 1.268292                                        LR 0.000006    Time 0.066445    
2022-12-29 10:26:29,078 - Epoch: [231][   80/  113]    Overall Loss 1.269202    Objective Loss 1.269202                                        LR 0.000006    Time 0.066072    
2022-12-29 10:26:29,717 - Epoch: [231][   90/  113]    Overall Loss 1.274915    Objective Loss 1.274915                                        LR 0.000006    Time 0.065822    
2022-12-29 10:26:30,349 - Epoch: [231][  100/  113]    Overall Loss 1.281555    Objective Loss 1.281555                                        LR 0.000006    Time 0.065560    
2022-12-29 10:26:30,976 - Epoch: [231][  110/  113]    Overall Loss 1.284747    Objective Loss 1.284747                                        LR 0.000006    Time 0.065290    
2022-12-29 10:26:31,149 - Epoch: [231][  113/  113]    Overall Loss 1.291159    Objective Loss 1.291159    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065084    
2022-12-29 10:26:31,206 - --- validate (epoch=231)-----------
2022-12-29 10:26:31,206 - 200 samples (16 per mini-batch)
2022-12-29 10:26:31,759 - Epoch: [231][   10/   13]    Loss 1.269373    Top1 52.500000    Top5 96.875000    
2022-12-29 10:26:31,844 - Epoch: [231][   13/   13]    Loss 1.255254    Top1 52.500000    Top5 97.500000    
2022-12-29 10:26:31,895 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.255

2022-12-29 10:26:31,895 - ==> Confusion:
[[28  3  0  5  3  0]
 [ 1 12  2  8 11  0]
 [ 2  5 10  4  7  1]
 [ 8  2  1 36  6  0]
 [ 1  6  2  5 18  1]
 [ 2  3  1  2  3  1]]

2022-12-29 10:26:31,897 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:31,898 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:31,913 - 

2022-12-29 10:26:31,913 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:32,737 - Epoch: [232][   10/  113]    Overall Loss 1.316196    Objective Loss 1.316196                                        LR 0.000006    Time 0.082257    
2022-12-29 10:26:33,376 - Epoch: [232][   20/  113]    Overall Loss 1.259979    Objective Loss 1.259979                                        LR 0.000006    Time 0.073058    
2022-12-29 10:26:34,012 - Epoch: [232][   30/  113]    Overall Loss 1.268682    Objective Loss 1.268682                                        LR 0.000006    Time 0.069908    
2022-12-29 10:26:34,648 - Epoch: [232][   40/  113]    Overall Loss 1.275062    Objective Loss 1.275062                                        LR 0.000006    Time 0.068311    
2022-12-29 10:26:35,278 - Epoch: [232][   50/  113]    Overall Loss 1.277454    Objective Loss 1.277454                                        LR 0.000006    Time 0.067233    
2022-12-29 10:26:35,913 - Epoch: [232][   60/  113]    Overall Loss 1.257777    Objective Loss 1.257777                                        LR 0.000006    Time 0.066604    
2022-12-29 10:26:36,547 - Epoch: [232][   70/  113]    Overall Loss 1.274537    Objective Loss 1.274537                                        LR 0.000006    Time 0.066137    
2022-12-29 10:26:37,176 - Epoch: [232][   80/  113]    Overall Loss 1.279224    Objective Loss 1.279224                                        LR 0.000006    Time 0.065730    
2022-12-29 10:26:37,812 - Epoch: [232][   90/  113]    Overall Loss 1.274516    Objective Loss 1.274516                                        LR 0.000006    Time 0.065483    
2022-12-29 10:26:38,441 - Epoch: [232][  100/  113]    Overall Loss 1.257476    Objective Loss 1.257476                                        LR 0.000006    Time 0.065224    
2022-12-29 10:26:39,072 - Epoch: [232][  110/  113]    Overall Loss 1.249320    Objective Loss 1.249320                                        LR 0.000006    Time 0.065026    
2022-12-29 10:26:39,246 - Epoch: [232][  113/  113]    Overall Loss 1.250390    Objective Loss 1.250390    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064835    
2022-12-29 10:26:39,298 - --- validate (epoch=232)-----------
2022-12-29 10:26:39,298 - 200 samples (16 per mini-batch)
2022-12-29 10:26:39,841 - Epoch: [232][   10/   13]    Loss 1.300673    Top1 48.125000    Top5 98.750000    
2022-12-29 10:26:39,925 - Epoch: [232][   13/   13]    Loss 1.272168    Top1 50.000000    Top5 99.000000    
2022-12-29 10:26:39,979 - ==> Top1: 50.000    Top5: 99.000    Loss: 1.272

2022-12-29 10:26:39,980 - ==> Confusion:
[[17  2  2  6  0  0]
 [ 3 15  6  5  4  0]
 [ 2 10 12  4 11  0]
 [ 4  1  1 38  2  1]
 [ 3 12  0  9 16  0]
 [ 3  3  4  1  1  2]]

2022-12-29 10:26:39,981 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:39,982 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:40,003 - 

2022-12-29 10:26:40,003 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:40,855 - Epoch: [233][   10/  113]    Overall Loss 1.279710    Objective Loss 1.279710                                        LR 0.000006    Time 0.085056    
2022-12-29 10:26:41,498 - Epoch: [233][   20/  113]    Overall Loss 1.325335    Objective Loss 1.325335                                        LR 0.000006    Time 0.074665    
2022-12-29 10:26:42,136 - Epoch: [233][   30/  113]    Overall Loss 1.254857    Objective Loss 1.254857                                        LR 0.000006    Time 0.071030    
2022-12-29 10:26:42,781 - Epoch: [233][   40/  113]    Overall Loss 1.243808    Objective Loss 1.243808                                        LR 0.000006    Time 0.069386    
2022-12-29 10:26:43,418 - Epoch: [233][   50/  113]    Overall Loss 1.245281    Objective Loss 1.245281                                        LR 0.000006    Time 0.068241    
2022-12-29 10:26:44,048 - Epoch: [233][   60/  113]    Overall Loss 1.234377    Objective Loss 1.234377                                        LR 0.000006    Time 0.067347    
2022-12-29 10:26:44,679 - Epoch: [233][   70/  113]    Overall Loss 1.244512    Objective Loss 1.244512                                        LR 0.000006    Time 0.066742    
2022-12-29 10:26:45,310 - Epoch: [233][   80/  113]    Overall Loss 1.252045    Objective Loss 1.252045                                        LR 0.000006    Time 0.066281    
2022-12-29 10:26:45,944 - Epoch: [233][   90/  113]    Overall Loss 1.251408    Objective Loss 1.251408                                        LR 0.000006    Time 0.065949    
2022-12-29 10:26:46,575 - Epoch: [233][  100/  113]    Overall Loss 1.253629    Objective Loss 1.253629                                        LR 0.000006    Time 0.065661    
2022-12-29 10:26:47,202 - Epoch: [233][  110/  113]    Overall Loss 1.243687    Objective Loss 1.243687                                        LR 0.000006    Time 0.065389    
2022-12-29 10:26:47,375 - Epoch: [233][  113/  113]    Overall Loss 1.239901    Objective Loss 1.239901    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.065181    
2022-12-29 10:26:47,424 - --- validate (epoch=233)-----------
2022-12-29 10:26:47,425 - 200 samples (16 per mini-batch)
2022-12-29 10:26:47,976 - Epoch: [233][   10/   13]    Loss 1.307970    Top1 53.125000    Top5 96.250000    
2022-12-29 10:26:48,060 - Epoch: [233][   13/   13]    Loss 1.300655    Top1 52.500000    Top5 96.500000    
2022-12-29 10:26:48,123 - ==> Top1: 52.500    Top5: 96.500    Loss: 1.301

2022-12-29 10:26:48,123 - ==> Confusion:
[[21  3  2  2  3  0]
 [ 3 26  2  8  3  0]
 [ 3  6  6  5  9  0]
 [ 4  2  0 34  6  0]
 [ 5 12  1 10 16  0]
 [ 1  1  0  4  0  2]]

2022-12-29 10:26:48,126 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:48,126 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:48,145 - 

2022-12-29 10:26:48,145 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:48,986 - Epoch: [234][   10/  113]    Overall Loss 1.206747    Objective Loss 1.206747                                        LR 0.000006    Time 0.083967    
2022-12-29 10:26:49,625 - Epoch: [234][   20/  113]    Overall Loss 1.217962    Objective Loss 1.217962                                        LR 0.000006    Time 0.073893    
2022-12-29 10:26:50,263 - Epoch: [234][   30/  113]    Overall Loss 1.271718    Objective Loss 1.271718                                        LR 0.000006    Time 0.070513    
2022-12-29 10:26:50,900 - Epoch: [234][   40/  113]    Overall Loss 1.270313    Objective Loss 1.270313                                        LR 0.000006    Time 0.068807    
2022-12-29 10:26:51,534 - Epoch: [234][   50/  113]    Overall Loss 1.268461    Objective Loss 1.268461                                        LR 0.000006    Time 0.067712    
2022-12-29 10:26:52,168 - Epoch: [234][   60/  113]    Overall Loss 1.258542    Objective Loss 1.258542                                        LR 0.000006    Time 0.066989    
2022-12-29 10:26:52,807 - Epoch: [234][   70/  113]    Overall Loss 1.268741    Objective Loss 1.268741                                        LR 0.000006    Time 0.066537    
2022-12-29 10:26:53,439 - Epoch: [234][   80/  113]    Overall Loss 1.276892    Objective Loss 1.276892                                        LR 0.000006    Time 0.066116    
2022-12-29 10:26:54,075 - Epoch: [234][   90/  113]    Overall Loss 1.283554    Objective Loss 1.283554                                        LR 0.000006    Time 0.065825    
2022-12-29 10:26:54,710 - Epoch: [234][  100/  113]    Overall Loss 1.277191    Objective Loss 1.277191                                        LR 0.000006    Time 0.065595    
2022-12-29 10:26:55,343 - Epoch: [234][  110/  113]    Overall Loss 1.278502    Objective Loss 1.278502                                        LR 0.000006    Time 0.065376    
2022-12-29 10:26:55,514 - Epoch: [234][  113/  113]    Overall Loss 1.281826    Objective Loss 1.281826    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065152    
2022-12-29 10:26:55,561 - --- validate (epoch=234)-----------
2022-12-29 10:26:55,561 - 200 samples (16 per mini-batch)
2022-12-29 10:26:56,118 - Epoch: [234][   10/   13]    Loss 1.184076    Top1 53.125000    Top5 98.750000    
2022-12-29 10:26:56,204 - Epoch: [234][   13/   13]    Loss 1.170895    Top1 52.500000    Top5 99.000000    
2022-12-29 10:26:56,255 - ==> Top1: 52.500    Top5: 99.000    Loss: 1.171

2022-12-29 10:26:56,255 - ==> Confusion:
[[15  1  0  6  2  0]
 [ 2 26  1  4 12  0]
 [ 1  7 10  8  8  0]
 [ 2  2  5 33  7  1]
 [ 2  9  2  5 21  1]
 [ 1  2  2  1  1  0]]

2022-12-29 10:26:56,259 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:26:56,259 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:26:56,273 - 

2022-12-29 10:26:56,273 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:26:57,105 - Epoch: [235][   10/  113]    Overall Loss 1.276284    Objective Loss 1.276284                                        LR 0.000006    Time 0.083091    
2022-12-29 10:26:57,745 - Epoch: [235][   20/  113]    Overall Loss 1.295139    Objective Loss 1.295139                                        LR 0.000006    Time 0.073504    
2022-12-29 10:26:58,382 - Epoch: [235][   30/  113]    Overall Loss 1.326048    Objective Loss 1.326048                                        LR 0.000006    Time 0.070201    
2022-12-29 10:26:59,018 - Epoch: [235][   40/  113]    Overall Loss 1.315338    Objective Loss 1.315338                                        LR 0.000006    Time 0.068563    
2022-12-29 10:26:59,655 - Epoch: [235][   50/  113]    Overall Loss 1.288532    Objective Loss 1.288532                                        LR 0.000006    Time 0.067578    
2022-12-29 10:27:00,288 - Epoch: [235][   60/  113]    Overall Loss 1.266268    Objective Loss 1.266268                                        LR 0.000006    Time 0.066852    
2022-12-29 10:27:00,928 - Epoch: [235][   70/  113]    Overall Loss 1.269171    Objective Loss 1.269171                                        LR 0.000006    Time 0.066431    
2022-12-29 10:27:01,565 - Epoch: [235][   80/  113]    Overall Loss 1.278337    Objective Loss 1.278337                                        LR 0.000006    Time 0.066094    
2022-12-29 10:27:02,207 - Epoch: [235][   90/  113]    Overall Loss 1.267989    Objective Loss 1.267989                                        LR 0.000006    Time 0.065874    
2022-12-29 10:27:02,843 - Epoch: [235][  100/  113]    Overall Loss 1.268111    Objective Loss 1.268111                                        LR 0.000006    Time 0.065641    
2022-12-29 10:27:03,473 - Epoch: [235][  110/  113]    Overall Loss 1.258080    Objective Loss 1.258080                                        LR 0.000006    Time 0.065396    
2022-12-29 10:27:03,651 - Epoch: [235][  113/  113]    Overall Loss 1.258149    Objective Loss 1.258149    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.065231    
2022-12-29 10:27:03,708 - --- validate (epoch=235)-----------
2022-12-29 10:27:03,708 - 200 samples (16 per mini-batch)
2022-12-29 10:27:04,258 - Epoch: [235][   10/   13]    Loss 1.346534    Top1 46.875000    Top5 95.000000    
2022-12-29 10:27:04,341 - Epoch: [235][   13/   13]    Loss 1.327218    Top1 48.500000    Top5 95.500000    
2022-12-29 10:27:04,393 - ==> Top1: 48.500    Top5: 95.500    Loss: 1.327

2022-12-29 10:27:04,394 - ==> Confusion:
[[16  2  0  7  0  0]
 [ 5 25  4  8  6  0]
 [ 1  6 11  8  8  0]
 [ 7  6  1 33  4  2]
 [ 8  2  1  7 11  1]
 [ 0  2  1  3  3  1]]

2022-12-29 10:27:04,397 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:27:04,397 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:04,411 - 

2022-12-29 10:27:04,411 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:05,240 - Epoch: [236][   10/  113]    Overall Loss 1.250011    Objective Loss 1.250011                                        LR 0.000006    Time 0.082813    
2022-12-29 10:27:05,873 - Epoch: [236][   20/  113]    Overall Loss 1.231258    Objective Loss 1.231258                                        LR 0.000006    Time 0.073045    
2022-12-29 10:27:06,512 - Epoch: [236][   30/  113]    Overall Loss 1.225119    Objective Loss 1.225119                                        LR 0.000006    Time 0.069984    
2022-12-29 10:27:07,148 - Epoch: [236][   40/  113]    Overall Loss 1.211855    Objective Loss 1.211855                                        LR 0.000006    Time 0.068368    
2022-12-29 10:27:07,778 - Epoch: [236][   50/  113]    Overall Loss 1.215483    Objective Loss 1.215483                                        LR 0.000006    Time 0.067285    
2022-12-29 10:27:08,410 - Epoch: [236][   60/  113]    Overall Loss 1.245793    Objective Loss 1.245793                                        LR 0.000006    Time 0.066594    
2022-12-29 10:27:09,046 - Epoch: [236][   70/  113]    Overall Loss 1.246484    Objective Loss 1.246484                                        LR 0.000006    Time 0.066155    
2022-12-29 10:27:09,681 - Epoch: [236][   80/  113]    Overall Loss 1.257543    Objective Loss 1.257543                                        LR 0.000006    Time 0.065818    
2022-12-29 10:27:10,317 - Epoch: [236][   90/  113]    Overall Loss 1.256466    Objective Loss 1.256466                                        LR 0.000006    Time 0.065566    
2022-12-29 10:27:10,948 - Epoch: [236][  100/  113]    Overall Loss 1.258775    Objective Loss 1.258775                                        LR 0.000006    Time 0.065319    
2022-12-29 10:27:11,576 - Epoch: [236][  110/  113]    Overall Loss 1.253702    Objective Loss 1.253702                                        LR 0.000006    Time 0.065086    
2022-12-29 10:27:11,748 - Epoch: [236][  113/  113]    Overall Loss 1.262254    Objective Loss 1.262254    Top1 29.166667    Top5 95.833333    LR 0.000006    Time 0.064873    
2022-12-29 10:27:11,807 - --- validate (epoch=236)-----------
2022-12-29 10:27:11,808 - 200 samples (16 per mini-batch)
2022-12-29 10:27:12,366 - Epoch: [236][   10/   13]    Loss 1.310787    Top1 51.250000    Top5 96.250000    
2022-12-29 10:27:12,451 - Epoch: [236][   13/   13]    Loss 1.362241    Top1 50.500000    Top5 97.000000    
2022-12-29 10:27:12,514 - ==> Top1: 50.500    Top5: 97.000    Loss: 1.362

2022-12-29 10:27:12,515 - ==> Confusion:
[[19  5  1  6  3  0]
 [ 1 28  2  7  5  2]
 [ 2  7 10  6  4  0]
 [ 2  7  3 27  0  0]
 [ 4 13  2  7 13  0]
 [ 1  2  3  1  3  4]]

2022-12-29 10:27:12,517 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:27:12,518 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:12,538 - 

2022-12-29 10:27:12,539 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:13,370 - Epoch: [237][   10/  113]    Overall Loss 1.104990    Objective Loss 1.104990                                        LR 0.000006    Time 0.083042    
2022-12-29 10:27:14,009 - Epoch: [237][   20/  113]    Overall Loss 1.123148    Objective Loss 1.123148                                        LR 0.000006    Time 0.073446    
2022-12-29 10:27:14,654 - Epoch: [237][   30/  113]    Overall Loss 1.182275    Objective Loss 1.182275                                        LR 0.000006    Time 0.070446    
2022-12-29 10:27:15,288 - Epoch: [237][   40/  113]    Overall Loss 1.211563    Objective Loss 1.211563                                        LR 0.000006    Time 0.068683    
2022-12-29 10:27:15,921 - Epoch: [237][   50/  113]    Overall Loss 1.200463    Objective Loss 1.200463                                        LR 0.000006    Time 0.067587    
2022-12-29 10:27:16,556 - Epoch: [237][   60/  113]    Overall Loss 1.207062    Objective Loss 1.207062                                        LR 0.000006    Time 0.066906    
2022-12-29 10:27:17,192 - Epoch: [237][   70/  113]    Overall Loss 1.210652    Objective Loss 1.210652                                        LR 0.000006    Time 0.066424    
2022-12-29 10:27:17,821 - Epoch: [237][   80/  113]    Overall Loss 1.215025    Objective Loss 1.215025                                        LR 0.000006    Time 0.065979    
2022-12-29 10:27:18,458 - Epoch: [237][   90/  113]    Overall Loss 1.218664    Objective Loss 1.218664                                        LR 0.000006    Time 0.065720    
2022-12-29 10:27:19,091 - Epoch: [237][  100/  113]    Overall Loss 1.220588    Objective Loss 1.220588                                        LR 0.000006    Time 0.065475    
2022-12-29 10:27:19,725 - Epoch: [237][  110/  113]    Overall Loss 1.223552    Objective Loss 1.223552                                        LR 0.000006    Time 0.065273    
2022-12-29 10:27:19,899 - Epoch: [237][  113/  113]    Overall Loss 1.222192    Objective Loss 1.222192    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.065076    
2022-12-29 10:27:19,960 - --- validate (epoch=237)-----------
2022-12-29 10:27:19,961 - 200 samples (16 per mini-batch)
2022-12-29 10:27:20,506 - Epoch: [237][   10/   13]    Loss 1.309616    Top1 47.500000    Top5 97.500000    
2022-12-29 10:27:20,592 - Epoch: [237][   13/   13]    Loss 1.301006    Top1 48.500000    Top5 97.500000    
2022-12-29 10:27:20,631 - ==> Top1: 48.500    Top5: 97.500    Loss: 1.301

2022-12-29 10:27:20,632 - ==> Confusion:
[[16  2  0  9  5  0]
 [ 0 26  3 10  6  0]
 [ 0  4  9  4 11  0]
 [ 5  8  1 36  6  0]
 [ 0  8  1  6 10  0]
 [ 4  6  0  0  4  0]]

2022-12-29 10:27:20,634 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:27:20,634 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:20,656 - 

2022-12-29 10:27:20,657 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:21,492 - Epoch: [238][   10/  113]    Overall Loss 1.239244    Objective Loss 1.239244                                        LR 0.000006    Time 0.083358    
2022-12-29 10:27:22,126 - Epoch: [238][   20/  113]    Overall Loss 1.238989    Objective Loss 1.238989                                        LR 0.000006    Time 0.073325    
2022-12-29 10:27:22,768 - Epoch: [238][   30/  113]    Overall Loss 1.245548    Objective Loss 1.245548                                        LR 0.000006    Time 0.070269    
2022-12-29 10:27:23,405 - Epoch: [238][   40/  113]    Overall Loss 1.204407    Objective Loss 1.204407                                        LR 0.000006    Time 0.068614    
2022-12-29 10:27:24,033 - Epoch: [238][   50/  113]    Overall Loss 1.220076    Objective Loss 1.220076                                        LR 0.000006    Time 0.067448    
2022-12-29 10:27:24,663 - Epoch: [238][   60/  113]    Overall Loss 1.221010    Objective Loss 1.221010                                        LR 0.000006    Time 0.066694    
2022-12-29 10:27:25,295 - Epoch: [238][   70/  113]    Overall Loss 1.237478    Objective Loss 1.237478                                        LR 0.000006    Time 0.066199    
2022-12-29 10:27:25,927 - Epoch: [238][   80/  113]    Overall Loss 1.242237    Objective Loss 1.242237                                        LR 0.000006    Time 0.065812    
2022-12-29 10:27:26,566 - Epoch: [238][   90/  113]    Overall Loss 1.256980    Objective Loss 1.256980                                        LR 0.000006    Time 0.065592    
2022-12-29 10:27:27,204 - Epoch: [238][  100/  113]    Overall Loss 1.263417    Objective Loss 1.263417                                        LR 0.000006    Time 0.065409    
2022-12-29 10:27:27,829 - Epoch: [238][  110/  113]    Overall Loss 1.266480    Objective Loss 1.266480                                        LR 0.000006    Time 0.065139    
2022-12-29 10:27:28,003 - Epoch: [238][  113/  113]    Overall Loss 1.266223    Objective Loss 1.266223    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064950    
2022-12-29 10:27:28,067 - --- validate (epoch=238)-----------
2022-12-29 10:27:28,067 - 200 samples (16 per mini-batch)
2022-12-29 10:27:28,622 - Epoch: [238][   10/   13]    Loss 1.341963    Top1 47.500000    Top5 96.250000    
2022-12-29 10:27:28,706 - Epoch: [238][   13/   13]    Loss 1.365082    Top1 48.500000    Top5 95.500000    
2022-12-29 10:27:28,770 - ==> Top1: 48.500    Top5: 95.500    Loss: 1.365

2022-12-29 10:27:28,771 - ==> Confusion:
[[25  1  4  3  3  0]
 [ 4 19  6  5 10  0]
 [ 2  6 11  3  2  0]
 [ 5  6  2 27  4  0]
 [ 1 11  1  6 15  0]
 [ 4  6  2  2  4  0]]

2022-12-29 10:27:28,774 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:27:28,775 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:28,797 - 

2022-12-29 10:27:28,797 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:29,635 - Epoch: [239][   10/  113]    Overall Loss 1.348047    Objective Loss 1.348047                                        LR 0.000006    Time 0.083681    
2022-12-29 10:27:30,270 - Epoch: [239][   20/  113]    Overall Loss 1.292245    Objective Loss 1.292245                                        LR 0.000006    Time 0.073574    
2022-12-29 10:27:30,908 - Epoch: [239][   30/  113]    Overall Loss 1.294427    Objective Loss 1.294427                                        LR 0.000006    Time 0.070313    
2022-12-29 10:27:31,546 - Epoch: [239][   40/  113]    Overall Loss 1.267418    Objective Loss 1.267418                                        LR 0.000006    Time 0.068655    
2022-12-29 10:27:32,175 - Epoch: [239][   50/  113]    Overall Loss 1.262901    Objective Loss 1.262901                                        LR 0.000006    Time 0.067498    
2022-12-29 10:27:32,808 - Epoch: [239][   60/  113]    Overall Loss 1.246070    Objective Loss 1.246070                                        LR 0.000006    Time 0.066783    
2022-12-29 10:27:33,439 - Epoch: [239][   70/  113]    Overall Loss 1.241043    Objective Loss 1.241043                                        LR 0.000006    Time 0.066257    
2022-12-29 10:27:34,071 - Epoch: [239][   80/  113]    Overall Loss 1.255982    Objective Loss 1.255982                                        LR 0.000006    Time 0.065875    
2022-12-29 10:27:34,709 - Epoch: [239][   90/  113]    Overall Loss 1.253271    Objective Loss 1.253271                                        LR 0.000006    Time 0.065630    
2022-12-29 10:27:35,345 - Epoch: [239][  100/  113]    Overall Loss 1.254862    Objective Loss 1.254862                                        LR 0.000006    Time 0.065428    
2022-12-29 10:27:35,974 - Epoch: [239][  110/  113]    Overall Loss 1.253853    Objective Loss 1.253853                                        LR 0.000006    Time 0.065193    
2022-12-29 10:27:36,146 - Epoch: [239][  113/  113]    Overall Loss 1.250043    Objective Loss 1.250043    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064978    
2022-12-29 10:27:36,198 - --- validate (epoch=239)-----------
2022-12-29 10:27:36,198 - 200 samples (16 per mini-batch)
2022-12-29 10:27:36,753 - Epoch: [239][   10/   13]    Loss 1.269491    Top1 55.625000    Top5 97.500000    
2022-12-29 10:27:36,837 - Epoch: [239][   13/   13]    Loss 1.243811    Top1 55.500000    Top5 97.500000    
2022-12-29 10:27:36,885 - ==> Top1: 55.500    Top5: 97.500    Loss: 1.244

2022-12-29 10:27:36,885 - ==> Confusion:
[[19  3  1  4  1  0]
 [ 1 26  4  7  4  0]
 [ 0  9 14  2  7  0]
 [ 4  6  0 30  4  0]
 [ 1 13  2  4 21  0]
 [ 2  4  0  3  3  1]]

2022-12-29 10:27:36,888 - ==> Best [Top1: 56.000   Top5: 98.000   Sparsity:0.00   Params: 289216 on epoch: 221]
2022-12-29 10:27:36,888 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:36,899 - 

2022-12-29 10:27:36,899 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:37,741 - Epoch: [240][   10/  113]    Overall Loss 1.323308    Objective Loss 1.323308                                        LR 0.000006    Time 0.084053    
2022-12-29 10:27:38,378 - Epoch: [240][   20/  113]    Overall Loss 1.277614    Objective Loss 1.277614                                        LR 0.000006    Time 0.073880    
2022-12-29 10:27:39,015 - Epoch: [240][   30/  113]    Overall Loss 1.253521    Objective Loss 1.253521                                        LR 0.000006    Time 0.070449    
2022-12-29 10:27:39,653 - Epoch: [240][   40/  113]    Overall Loss 1.290373    Objective Loss 1.290373                                        LR 0.000006    Time 0.068781    
2022-12-29 10:27:40,286 - Epoch: [240][   50/  113]    Overall Loss 1.278957    Objective Loss 1.278957                                        LR 0.000006    Time 0.067671    
2022-12-29 10:27:40,918 - Epoch: [240][   60/  113]    Overall Loss 1.293946    Objective Loss 1.293946                                        LR 0.000006    Time 0.066919    
2022-12-29 10:27:41,561 - Epoch: [240][   70/  113]    Overall Loss 1.299027    Objective Loss 1.299027                                        LR 0.000006    Time 0.066541    
2022-12-29 10:27:42,199 - Epoch: [240][   80/  113]    Overall Loss 1.294638    Objective Loss 1.294638                                        LR 0.000006    Time 0.066191    
2022-12-29 10:27:42,835 - Epoch: [240][   90/  113]    Overall Loss 1.287125    Objective Loss 1.287125                                        LR 0.000006    Time 0.065898    
2022-12-29 10:27:43,470 - Epoch: [240][  100/  113]    Overall Loss 1.285598    Objective Loss 1.285598                                        LR 0.000006    Time 0.065650    
2022-12-29 10:27:44,094 - Epoch: [240][  110/  113]    Overall Loss 1.287541    Objective Loss 1.287541                                        LR 0.000006    Time 0.065353    
2022-12-29 10:27:44,271 - Epoch: [240][  113/  113]    Overall Loss 1.288055    Objective Loss 1.288055    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.065178    
2022-12-29 10:27:44,325 - --- validate (epoch=240)-----------
2022-12-29 10:27:44,326 - 200 samples (16 per mini-batch)
2022-12-29 10:27:44,870 - Epoch: [240][   10/   13]    Loss 1.242278    Top1 55.000000    Top5 99.375000    
2022-12-29 10:27:44,956 - Epoch: [240][   13/   13]    Loss 1.172857    Top1 56.000000    Top5 99.500000    
2022-12-29 10:27:45,002 - ==> Top1: 56.000    Top5: 99.500    Loss: 1.173

2022-12-29 10:27:45,002 - ==> Confusion:
[[19  2  0  5  3  0]
 [ 3 23  8  4  9  0]
 [ 4  4 14  8  4  0]
 [ 3  3  1 30  4  1]
 [ 1  5  2  6 23  1]
 [ 1  1  1  1  3  3]]

2022-12-29 10:27:45,005 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:27:45,005 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:45,031 - 

2022-12-29 10:27:45,032 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:45,855 - Epoch: [241][   10/  113]    Overall Loss 1.216215    Objective Loss 1.216215                                        LR 0.000006    Time 0.082135    
2022-12-29 10:27:46,491 - Epoch: [241][   20/  113]    Overall Loss 1.233480    Objective Loss 1.233480                                        LR 0.000006    Time 0.072872    
2022-12-29 10:27:47,129 - Epoch: [241][   30/  113]    Overall Loss 1.219381    Objective Loss 1.219381                                        LR 0.000006    Time 0.069808    
2022-12-29 10:27:47,771 - Epoch: [241][   40/  113]    Overall Loss 1.211549    Objective Loss 1.211549                                        LR 0.000006    Time 0.068387    
2022-12-29 10:27:48,402 - Epoch: [241][   50/  113]    Overall Loss 1.209496    Objective Loss 1.209496                                        LR 0.000006    Time 0.067329    
2022-12-29 10:27:49,032 - Epoch: [241][   60/  113]    Overall Loss 1.227742    Objective Loss 1.227742                                        LR 0.000006    Time 0.066606    
2022-12-29 10:27:49,664 - Epoch: [241][   70/  113]    Overall Loss 1.240875    Objective Loss 1.240875                                        LR 0.000006    Time 0.066109    
2022-12-29 10:27:50,297 - Epoch: [241][   80/  113]    Overall Loss 1.234731    Objective Loss 1.234731                                        LR 0.000006    Time 0.065748    
2022-12-29 10:27:50,929 - Epoch: [241][   90/  113]    Overall Loss 1.240056    Objective Loss 1.240056                                        LR 0.000006    Time 0.065459    
2022-12-29 10:27:51,562 - Epoch: [241][  100/  113]    Overall Loss 1.254051    Objective Loss 1.254051                                        LR 0.000006    Time 0.065246    
2022-12-29 10:27:52,195 - Epoch: [241][  110/  113]    Overall Loss 1.257996    Objective Loss 1.257996                                        LR 0.000006    Time 0.065057    
2022-12-29 10:27:52,371 - Epoch: [241][  113/  113]    Overall Loss 1.261193    Objective Loss 1.261193    Top1 41.666667    Top5 91.666667    LR 0.000006    Time 0.064888    
2022-12-29 10:27:52,440 - --- validate (epoch=241)-----------
2022-12-29 10:27:52,441 - 200 samples (16 per mini-batch)
2022-12-29 10:27:52,991 - Epoch: [241][   10/   13]    Loss 1.323409    Top1 50.000000    Top5 96.875000    
2022-12-29 10:27:53,075 - Epoch: [241][   13/   13]    Loss 1.326888    Top1 51.000000    Top5 97.500000    
2022-12-29 10:27:53,139 - ==> Top1: 51.000    Top5: 97.500    Loss: 1.327

2022-12-29 10:27:53,139 - ==> Confusion:
[[21  1  0  7  2  0]
 [ 2 20  1  5  7  0]
 [ 0  4  7  4 10  0]
 [ 4  9  1 36  6  0]
 [ 3  6  3 12 15  2]
 [ 3  1  0  3  2  3]]

2022-12-29 10:27:53,141 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:27:53,142 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:27:53,163 - 

2022-12-29 10:27:53,164 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:27:54,009 - Epoch: [242][   10/  113]    Overall Loss 1.256752    Objective Loss 1.256752                                        LR 0.000006    Time 0.084416    
2022-12-29 10:27:54,647 - Epoch: [242][   20/  113]    Overall Loss 1.273342    Objective Loss 1.273342                                        LR 0.000006    Time 0.074097    
2022-12-29 10:27:55,283 - Epoch: [242][   30/  113]    Overall Loss 1.300965    Objective Loss 1.300965                                        LR 0.000006    Time 0.070585    
2022-12-29 10:27:55,918 - Epoch: [242][   40/  113]    Overall Loss 1.274975    Objective Loss 1.274975                                        LR 0.000006    Time 0.068792    
2022-12-29 10:27:56,551 - Epoch: [242][   50/  113]    Overall Loss 1.277946    Objective Loss 1.277946                                        LR 0.000006    Time 0.067689    
2022-12-29 10:27:57,183 - Epoch: [242][   60/  113]    Overall Loss 1.305095    Objective Loss 1.305095                                        LR 0.000006    Time 0.066939    
2022-12-29 10:27:57,824 - Epoch: [242][   70/  113]    Overall Loss 1.308292    Objective Loss 1.308292                                        LR 0.000006    Time 0.066521    
2022-12-29 10:27:58,462 - Epoch: [242][   80/  113]    Overall Loss 1.307209    Objective Loss 1.307209                                        LR 0.000006    Time 0.066175    
2022-12-29 10:27:59,098 - Epoch: [242][   90/  113]    Overall Loss 1.303581    Objective Loss 1.303581                                        LR 0.000006    Time 0.065886    
2022-12-29 10:27:59,735 - Epoch: [242][  100/  113]    Overall Loss 1.294471    Objective Loss 1.294471                                        LR 0.000006    Time 0.065661    
2022-12-29 10:28:00,363 - Epoch: [242][  110/  113]    Overall Loss 1.305557    Objective Loss 1.305557                                        LR 0.000006    Time 0.065396    
2022-12-29 10:28:00,537 - Epoch: [242][  113/  113]    Overall Loss 1.308433    Objective Loss 1.308433    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.065199    
2022-12-29 10:28:00,592 - --- validate (epoch=242)-----------
2022-12-29 10:28:00,593 - 200 samples (16 per mini-batch)
2022-12-29 10:28:01,150 - Epoch: [242][   10/   13]    Loss 1.317917    Top1 48.125000    Top5 97.500000    
2022-12-29 10:28:01,234 - Epoch: [242][   13/   13]    Loss 1.385485    Top1 47.500000    Top5 96.500000    
2022-12-29 10:28:01,282 - ==> Top1: 47.500    Top5: 96.500    Loss: 1.385

2022-12-29 10:28:01,283 - ==> Confusion:
[[13  4  0  8  3  1]
 [ 2 13  1  4  9  0]
 [ 1  7 12  5 13  1]
 [ 4  3  3 33  4  0]
 [10  5  1  6 22  0]
 [ 3  3  1  2  1  2]]

2022-12-29 10:28:01,286 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:01,286 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:01,308 - 

2022-12-29 10:28:01,309 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:02,140 - Epoch: [243][   10/  113]    Overall Loss 1.312787    Objective Loss 1.312787                                        LR 0.000006    Time 0.083023    
2022-12-29 10:28:02,779 - Epoch: [243][   20/  113]    Overall Loss 1.305844    Objective Loss 1.305844                                        LR 0.000006    Time 0.073405    
2022-12-29 10:28:03,414 - Epoch: [243][   30/  113]    Overall Loss 1.293140    Objective Loss 1.293140                                        LR 0.000006    Time 0.070113    
2022-12-29 10:28:04,049 - Epoch: [243][   40/  113]    Overall Loss 1.280202    Objective Loss 1.280202                                        LR 0.000006    Time 0.068446    
2022-12-29 10:28:04,681 - Epoch: [243][   50/  113]    Overall Loss 1.270905    Objective Loss 1.270905                                        LR 0.000006    Time 0.067378    
2022-12-29 10:28:05,308 - Epoch: [243][   60/  113]    Overall Loss 1.253346    Objective Loss 1.253346                                        LR 0.000006    Time 0.066598    
2022-12-29 10:28:05,944 - Epoch: [243][   70/  113]    Overall Loss 1.265717    Objective Loss 1.265717                                        LR 0.000006    Time 0.066157    
2022-12-29 10:28:06,576 - Epoch: [243][   80/  113]    Overall Loss 1.288500    Objective Loss 1.288500                                        LR 0.000006    Time 0.065775    
2022-12-29 10:28:07,208 - Epoch: [243][   90/  113]    Overall Loss 1.281595    Objective Loss 1.281595                                        LR 0.000006    Time 0.065491    
2022-12-29 10:28:07,836 - Epoch: [243][  100/  113]    Overall Loss 1.281304    Objective Loss 1.281304                                        LR 0.000006    Time 0.065215    
2022-12-29 10:28:08,467 - Epoch: [243][  110/  113]    Overall Loss 1.278544    Objective Loss 1.278544                                        LR 0.000006    Time 0.065022    
2022-12-29 10:28:08,642 - Epoch: [243][  113/  113]    Overall Loss 1.284492    Objective Loss 1.284492    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.064841    
2022-12-29 10:28:08,698 - --- validate (epoch=243)-----------
2022-12-29 10:28:08,698 - 200 samples (16 per mini-batch)
2022-12-29 10:28:09,256 - Epoch: [243][   10/   13]    Loss 1.227880    Top1 51.250000    Top5 98.125000    
2022-12-29 10:28:09,340 - Epoch: [243][   13/   13]    Loss 1.194889    Top1 53.000000    Top5 98.500000    
2022-12-29 10:28:09,400 - ==> Top1: 53.000    Top5: 98.500    Loss: 1.195

2022-12-29 10:28:09,400 - ==> Confusion:
[[33  4  1  4  1  0]
 [ 3 24  3  6 10  0]
 [ 1  9 11  4  9  0]
 [ 2  2  1 25  6  0]
 [ 2  8  3  5 11  0]
 [ 0  6  0  0  4  2]]

2022-12-29 10:28:09,403 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:09,403 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:09,422 - 

2022-12-29 10:28:09,423 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:10,277 - Epoch: [244][   10/  113]    Overall Loss 1.337682    Objective Loss 1.337682                                        LR 0.000006    Time 0.085267    
2022-12-29 10:28:10,914 - Epoch: [244][   20/  113]    Overall Loss 1.307675    Objective Loss 1.307675                                        LR 0.000006    Time 0.074450    
2022-12-29 10:28:11,552 - Epoch: [244][   30/  113]    Overall Loss 1.297886    Objective Loss 1.297886                                        LR 0.000006    Time 0.070894    
2022-12-29 10:28:12,191 - Epoch: [244][   40/  113]    Overall Loss 1.298572    Objective Loss 1.298572                                        LR 0.000006    Time 0.069113    
2022-12-29 10:28:12,832 - Epoch: [244][   50/  113]    Overall Loss 1.274039    Objective Loss 1.274039                                        LR 0.000006    Time 0.068107    
2022-12-29 10:28:13,463 - Epoch: [244][   60/  113]    Overall Loss 1.270297    Objective Loss 1.270297                                        LR 0.000006    Time 0.067268    
2022-12-29 10:28:14,100 - Epoch: [244][   70/  113]    Overall Loss 1.261249    Objective Loss 1.261249                                        LR 0.000006    Time 0.066743    
2022-12-29 10:28:14,729 - Epoch: [244][   80/  113]    Overall Loss 1.254320    Objective Loss 1.254320                                        LR 0.000006    Time 0.066261    
2022-12-29 10:28:15,360 - Epoch: [244][   90/  113]    Overall Loss 1.256479    Objective Loss 1.256479                                        LR 0.000006    Time 0.065908    
2022-12-29 10:28:15,999 - Epoch: [244][  100/  113]    Overall Loss 1.251662    Objective Loss 1.251662                                        LR 0.000006    Time 0.065699    
2022-12-29 10:28:16,626 - Epoch: [244][  110/  113]    Overall Loss 1.254007    Objective Loss 1.254007                                        LR 0.000006    Time 0.065419    
2022-12-29 10:28:16,798 - Epoch: [244][  113/  113]    Overall Loss 1.252097    Objective Loss 1.252097    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065204    
2022-12-29 10:28:16,862 - --- validate (epoch=244)-----------
2022-12-29 10:28:16,863 - 200 samples (16 per mini-batch)
2022-12-29 10:28:17,416 - Epoch: [244][   10/   13]    Loss 1.256336    Top1 48.750000    Top5 97.500000    
2022-12-29 10:28:17,502 - Epoch: [244][   13/   13]    Loss 1.292841    Top1 47.500000    Top5 96.500000    
2022-12-29 10:28:17,542 - ==> Top1: 47.500    Top5: 96.500    Loss: 1.293

2022-12-29 10:28:17,542 - ==> Confusion:
[[21  2  0  7  3  0]
 [ 4 15  7  2  9  0]
 [ 0  4 15  0 11  0]
 [ 6  4  2 28  7  1]
 [ 2  5  6  8 16  0]
 [ 2  7  0  1  5  0]]

2022-12-29 10:28:17,544 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:17,545 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:17,559 - 

2022-12-29 10:28:17,560 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:18,410 - Epoch: [245][   10/  113]    Overall Loss 1.334151    Objective Loss 1.334151                                        LR 0.000006    Time 0.084901    
2022-12-29 10:28:19,045 - Epoch: [245][   20/  113]    Overall Loss 1.267072    Objective Loss 1.267072                                        LR 0.000006    Time 0.074203    
2022-12-29 10:28:19,685 - Epoch: [245][   30/  113]    Overall Loss 1.304578    Objective Loss 1.304578                                        LR 0.000006    Time 0.070762    
2022-12-29 10:28:20,318 - Epoch: [245][   40/  113]    Overall Loss 1.280048    Objective Loss 1.280048                                        LR 0.000006    Time 0.068907    
2022-12-29 10:28:20,955 - Epoch: [245][   50/  113]    Overall Loss 1.288816    Objective Loss 1.288816                                        LR 0.000006    Time 0.067856    
2022-12-29 10:28:21,590 - Epoch: [245][   60/  113]    Overall Loss 1.296132    Objective Loss 1.296132                                        LR 0.000006    Time 0.067116    
2022-12-29 10:28:22,232 - Epoch: [245][   70/  113]    Overall Loss 1.282907    Objective Loss 1.282907                                        LR 0.000006    Time 0.066688    
2022-12-29 10:28:22,869 - Epoch: [245][   80/  113]    Overall Loss 1.266431    Objective Loss 1.266431                                        LR 0.000006    Time 0.066306    
2022-12-29 10:28:23,506 - Epoch: [245][   90/  113]    Overall Loss 1.282711    Objective Loss 1.282711                                        LR 0.000006    Time 0.066013    
2022-12-29 10:28:24,144 - Epoch: [245][  100/  113]    Overall Loss 1.277509    Objective Loss 1.277509                                        LR 0.000006    Time 0.065785    
2022-12-29 10:28:24,766 - Epoch: [245][  110/  113]    Overall Loss 1.276290    Objective Loss 1.276290                                        LR 0.000006    Time 0.065462    
2022-12-29 10:28:24,941 - Epoch: [245][  113/  113]    Overall Loss 1.278727    Objective Loss 1.278727    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.065265    
2022-12-29 10:28:25,008 - --- validate (epoch=245)-----------
2022-12-29 10:28:25,008 - 200 samples (16 per mini-batch)
2022-12-29 10:28:25,558 - Epoch: [245][   10/   13]    Loss 1.242486    Top1 50.625000    Top5 99.375000    
2022-12-29 10:28:25,645 - Epoch: [245][   13/   13]    Loss 1.204151    Top1 52.000000    Top5 99.500000    
2022-12-29 10:28:25,702 - ==> Top1: 52.000    Top5: 99.500    Loss: 1.204

2022-12-29 10:28:25,702 - ==> Confusion:
[[24  1  0  5  1  0]
 [ 0 20  4  4 12  0]
 [ 3  8 15 10  9  0]
 [ 4  3  0 26  5  0]
 [ 2  5  1 10 18  0]
 [ 1  2  2  3  1  1]]

2022-12-29 10:28:25,706 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:25,707 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:25,730 - 

2022-12-29 10:28:25,730 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:26,574 - Epoch: [246][   10/  113]    Overall Loss 1.232332    Objective Loss 1.232332                                        LR 0.000006    Time 0.084358    
2022-12-29 10:28:27,215 - Epoch: [246][   20/  113]    Overall Loss 1.187981    Objective Loss 1.187981                                        LR 0.000006    Time 0.074156    
2022-12-29 10:28:27,858 - Epoch: [246][   30/  113]    Overall Loss 1.222959    Objective Loss 1.222959                                        LR 0.000006    Time 0.070881    
2022-12-29 10:28:28,492 - Epoch: [246][   40/  113]    Overall Loss 1.212658    Objective Loss 1.212658                                        LR 0.000006    Time 0.068987    
2022-12-29 10:28:29,128 - Epoch: [246][   50/  113]    Overall Loss 1.202989    Objective Loss 1.202989                                        LR 0.000006    Time 0.067899    
2022-12-29 10:28:29,762 - Epoch: [246][   60/  113]    Overall Loss 1.206041    Objective Loss 1.206041                                        LR 0.000006    Time 0.067151    
2022-12-29 10:28:30,399 - Epoch: [246][   70/  113]    Overall Loss 1.210727    Objective Loss 1.210727                                        LR 0.000006    Time 0.066640    
2022-12-29 10:28:31,030 - Epoch: [246][   80/  113]    Overall Loss 1.200033    Objective Loss 1.200033                                        LR 0.000006    Time 0.066200    
2022-12-29 10:28:31,662 - Epoch: [246][   90/  113]    Overall Loss 1.219922    Objective Loss 1.219922                                        LR 0.000006    Time 0.065859    
2022-12-29 10:28:32,294 - Epoch: [246][  100/  113]    Overall Loss 1.217253    Objective Loss 1.217253                                        LR 0.000006    Time 0.065593    
2022-12-29 10:28:32,923 - Epoch: [246][  110/  113]    Overall Loss 1.216736    Objective Loss 1.216736                                        LR 0.000006    Time 0.065337    
2022-12-29 10:28:33,095 - Epoch: [246][  113/  113]    Overall Loss 1.218778    Objective Loss 1.218778    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065123    
2022-12-29 10:28:33,151 - --- validate (epoch=246)-----------
2022-12-29 10:28:33,151 - 200 samples (16 per mini-batch)
2022-12-29 10:28:33,709 - Epoch: [246][   10/   13]    Loss 1.283092    Top1 48.750000    Top5 96.250000    
2022-12-29 10:28:33,792 - Epoch: [246][   13/   13]    Loss 1.284678    Top1 50.000000    Top5 96.500000    
2022-12-29 10:28:33,838 - ==> Top1: 50.000    Top5: 96.500    Loss: 1.285

2022-12-29 10:28:33,839 - ==> Confusion:
[[25  2  0  5  0  1]
 [ 3 26  2  7  5  0]
 [ 3  8  7  9  8  0]
 [ 5  4  1 28  2  0]
 [ 3  9  1 15 13  0]
 [ 1  1  0  4  1  1]]

2022-12-29 10:28:33,842 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:33,842 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:33,867 - 

2022-12-29 10:28:33,867 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:34,706 - Epoch: [247][   10/  113]    Overall Loss 1.315642    Objective Loss 1.315642                                        LR 0.000006    Time 0.083762    
2022-12-29 10:28:35,344 - Epoch: [247][   20/  113]    Overall Loss 1.298350    Objective Loss 1.298350                                        LR 0.000006    Time 0.073777    
2022-12-29 10:28:35,982 - Epoch: [247][   30/  113]    Overall Loss 1.292483    Objective Loss 1.292483                                        LR 0.000006    Time 0.070432    
2022-12-29 10:28:36,621 - Epoch: [247][   40/  113]    Overall Loss 1.309123    Objective Loss 1.309123                                        LR 0.000006    Time 0.068797    
2022-12-29 10:28:37,258 - Epoch: [247][   50/  113]    Overall Loss 1.317371    Objective Loss 1.317371                                        LR 0.000006    Time 0.067751    
2022-12-29 10:28:37,895 - Epoch: [247][   60/  113]    Overall Loss 1.318592    Objective Loss 1.318592                                        LR 0.000006    Time 0.067069    
2022-12-29 10:28:38,537 - Epoch: [247][   70/  113]    Overall Loss 1.321977    Objective Loss 1.321977                                        LR 0.000006    Time 0.066659    
2022-12-29 10:28:39,172 - Epoch: [247][   80/  113]    Overall Loss 1.302620    Objective Loss 1.302620                                        LR 0.000006    Time 0.066251    
2022-12-29 10:28:39,809 - Epoch: [247][   90/  113]    Overall Loss 1.298304    Objective Loss 1.298304                                        LR 0.000006    Time 0.065963    
2022-12-29 10:28:40,448 - Epoch: [247][  100/  113]    Overall Loss 1.294690    Objective Loss 1.294690                                        LR 0.000006    Time 0.065756    
2022-12-29 10:28:41,071 - Epoch: [247][  110/  113]    Overall Loss 1.306714    Objective Loss 1.306714                                        LR 0.000006    Time 0.065441    
2022-12-29 10:28:41,247 - Epoch: [247][  113/  113]    Overall Loss 1.314285    Objective Loss 1.314285    Top1 33.333333    Top5 100.000000    LR 0.000006    Time 0.065256    
2022-12-29 10:28:41,304 - --- validate (epoch=247)-----------
2022-12-29 10:28:41,305 - 200 samples (16 per mini-batch)
2022-12-29 10:28:41,859 - Epoch: [247][   10/   13]    Loss 1.296907    Top1 45.000000    Top5 96.875000    
2022-12-29 10:28:41,944 - Epoch: [247][   13/   13]    Loss 1.279603    Top1 45.000000    Top5 96.000000    
2022-12-29 10:28:41,992 - ==> Top1: 45.000    Top5: 96.000    Loss: 1.280

2022-12-29 10:28:41,992 - ==> Confusion:
[[15  2  0  6  0  1]
 [ 2 22  4  6  5  0]
 [ 0 14  9  4  6  0]
 [ 2  7  1 30  8  1]
 [ 2  8  8 10 14  1]
 [ 1  6  0  3  2  0]]

2022-12-29 10:28:41,995 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:41,995 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:42,010 - 

2022-12-29 10:28:42,011 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:42,852 - Epoch: [248][   10/  113]    Overall Loss 1.122707    Objective Loss 1.122707                                        LR 0.000006    Time 0.084017    
2022-12-29 10:28:43,487 - Epoch: [248][   20/  113]    Overall Loss 1.152088    Objective Loss 1.152088                                        LR 0.000006    Time 0.073737    
2022-12-29 10:28:44,125 - Epoch: [248][   30/  113]    Overall Loss 1.194420    Objective Loss 1.194420                                        LR 0.000006    Time 0.070431    
2022-12-29 10:28:44,759 - Epoch: [248][   40/  113]    Overall Loss 1.198027    Objective Loss 1.198027                                        LR 0.000006    Time 0.068658    
2022-12-29 10:28:45,394 - Epoch: [248][   50/  113]    Overall Loss 1.206297    Objective Loss 1.206297                                        LR 0.000006    Time 0.067616    
2022-12-29 10:28:46,030 - Epoch: [248][   60/  113]    Overall Loss 1.210822    Objective Loss 1.210822                                        LR 0.000006    Time 0.066932    
2022-12-29 10:28:46,665 - Epoch: [248][   70/  113]    Overall Loss 1.220667    Objective Loss 1.220667                                        LR 0.000006    Time 0.066434    
2022-12-29 10:28:47,297 - Epoch: [248][   80/  113]    Overall Loss 1.204277    Objective Loss 1.204277                                        LR 0.000006    Time 0.066028    
2022-12-29 10:28:47,937 - Epoch: [248][   90/  113]    Overall Loss 1.208947    Objective Loss 1.208947                                        LR 0.000006    Time 0.065795    
2022-12-29 10:28:48,573 - Epoch: [248][  100/  113]    Overall Loss 1.209127    Objective Loss 1.209127                                        LR 0.000006    Time 0.065575    
2022-12-29 10:28:49,203 - Epoch: [248][  110/  113]    Overall Loss 1.211374    Objective Loss 1.211374                                        LR 0.000006    Time 0.065329    
2022-12-29 10:28:49,376 - Epoch: [248][  113/  113]    Overall Loss 1.220452    Objective Loss 1.220452    Top1 29.166667    Top5 95.833333    LR 0.000006    Time 0.065128    
2022-12-29 10:28:49,424 - --- validate (epoch=248)-----------
2022-12-29 10:28:49,424 - 200 samples (16 per mini-batch)
2022-12-29 10:28:49,970 - Epoch: [248][   10/   13]    Loss 1.208101    Top1 51.250000    Top5 96.875000    
2022-12-29 10:28:50,054 - Epoch: [248][   13/   13]    Loss 1.167424    Top1 52.500000    Top5 97.000000    
2022-12-29 10:28:50,101 - ==> Top1: 52.500    Top5: 97.000    Loss: 1.167

2022-12-29 10:28:50,102 - ==> Confusion:
[[21  4  0  2  3  0]
 [ 3 25  4  3  8  0]
 [ 2  9 13  5  8  0]
 [ 2  4  2 31  7  0]
 [ 4  9  2  8 14  0]
 [ 2  2  0  0  2  1]]

2022-12-29 10:28:50,104 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:50,104 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:50,125 - 

2022-12-29 10:28:50,126 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:50,977 - Epoch: [249][   10/  113]    Overall Loss 1.250315    Objective Loss 1.250315                                        LR 0.000006    Time 0.084986    
2022-12-29 10:28:51,617 - Epoch: [249][   20/  113]    Overall Loss 1.222101    Objective Loss 1.222101                                        LR 0.000006    Time 0.074484    
2022-12-29 10:28:52,252 - Epoch: [249][   30/  113]    Overall Loss 1.226396    Objective Loss 1.226396                                        LR 0.000006    Time 0.070805    
2022-12-29 10:28:52,892 - Epoch: [249][   40/  113]    Overall Loss 1.225936    Objective Loss 1.225936                                        LR 0.000006    Time 0.069098    
2022-12-29 10:28:53,524 - Epoch: [249][   50/  113]    Overall Loss 1.237161    Objective Loss 1.237161                                        LR 0.000006    Time 0.067906    
2022-12-29 10:28:54,157 - Epoch: [249][   60/  113]    Overall Loss 1.249688    Objective Loss 1.249688                                        LR 0.000006    Time 0.067133    
2022-12-29 10:28:54,789 - Epoch: [249][   70/  113]    Overall Loss 1.240692    Objective Loss 1.240692                                        LR 0.000006    Time 0.066559    
2022-12-29 10:28:55,418 - Epoch: [249][   80/  113]    Overall Loss 1.231048    Objective Loss 1.231048                                        LR 0.000006    Time 0.066097    
2022-12-29 10:28:56,049 - Epoch: [249][   90/  113]    Overall Loss 1.232640    Objective Loss 1.232640                                        LR 0.000006    Time 0.065756    
2022-12-29 10:28:56,683 - Epoch: [249][  100/  113]    Overall Loss 1.237512    Objective Loss 1.237512                                        LR 0.000006    Time 0.065521    
2022-12-29 10:28:57,309 - Epoch: [249][  110/  113]    Overall Loss 1.249225    Objective Loss 1.249225                                        LR 0.000006    Time 0.065246    
2022-12-29 10:28:57,480 - Epoch: [249][  113/  113]    Overall Loss 1.251266    Objective Loss 1.251266    Top1 50.000000    Top5 91.666667    LR 0.000006    Time 0.065025    
2022-12-29 10:28:57,536 - --- validate (epoch=249)-----------
2022-12-29 10:28:57,536 - 200 samples (16 per mini-batch)
2022-12-29 10:28:58,091 - Epoch: [249][   10/   13]    Loss 1.140632    Top1 51.250000    Top5 98.750000    
2022-12-29 10:28:58,180 - Epoch: [249][   13/   13]    Loss 1.202461    Top1 49.500000    Top5 98.500000    
2022-12-29 10:28:58,234 - ==> Top1: 49.500    Top5: 98.500    Loss: 1.202

2022-12-29 10:28:58,235 - ==> Confusion:
[[19  1  0  8  3  0]
 [ 1 15  4 18  8  0]
 [ 0  6 16  7  8  2]
 [ 3  5  0 37  1  0]
 [ 3  2  5 10 12  0]
 [ 0  1  1  3  1  0]]

2022-12-29 10:28:58,237 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:28:58,237 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:28:58,250 - 

2022-12-29 10:28:58,250 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:28:59,087 - Epoch: [250][   10/  113]    Overall Loss 1.284678    Objective Loss 1.284678                                        LR 0.000006    Time 0.083535    
2022-12-29 10:28:59,727 - Epoch: [250][   20/  113]    Overall Loss 1.295553    Objective Loss 1.295553                                        LR 0.000006    Time 0.073753    
2022-12-29 10:29:00,365 - Epoch: [250][   30/  113]    Overall Loss 1.270264    Objective Loss 1.270264                                        LR 0.000006    Time 0.070422    
2022-12-29 10:29:01,002 - Epoch: [250][   40/  113]    Overall Loss 1.243663    Objective Loss 1.243663                                        LR 0.000006    Time 0.068735    
2022-12-29 10:29:01,636 - Epoch: [250][   50/  113]    Overall Loss 1.265735    Objective Loss 1.265735                                        LR 0.000006    Time 0.067644    
2022-12-29 10:29:02,270 - Epoch: [250][   60/  113]    Overall Loss 1.258175    Objective Loss 1.258175                                        LR 0.000006    Time 0.066943    
2022-12-29 10:29:02,909 - Epoch: [250][   70/  113]    Overall Loss 1.258595    Objective Loss 1.258595                                        LR 0.000006    Time 0.066500    
2022-12-29 10:29:03,540 - Epoch: [250][   80/  113]    Overall Loss 1.253483    Objective Loss 1.253483                                        LR 0.000006    Time 0.066068    
2022-12-29 10:29:04,177 - Epoch: [250][   90/  113]    Overall Loss 1.253627    Objective Loss 1.253627                                        LR 0.000006    Time 0.065798    
2022-12-29 10:29:04,809 - Epoch: [250][  100/  113]    Overall Loss 1.257050    Objective Loss 1.257050                                        LR 0.000006    Time 0.065532    
2022-12-29 10:29:05,443 - Epoch: [250][  110/  113]    Overall Loss 1.258929    Objective Loss 1.258929                                        LR 0.000006    Time 0.065327    
2022-12-29 10:29:05,615 - Epoch: [250][  113/  113]    Overall Loss 1.260005    Objective Loss 1.260005    Top1 66.666667    Top5 95.833333    LR 0.000006    Time 0.065119    
2022-12-29 10:29:05,671 - --- validate (epoch=250)-----------
2022-12-29 10:29:05,671 - 200 samples (16 per mini-batch)
2022-12-29 10:29:06,218 - Epoch: [250][   10/   13]    Loss 1.292429    Top1 44.375000    Top5 98.125000    
2022-12-29 10:29:06,306 - Epoch: [250][   13/   13]    Loss 1.341069    Top1 45.500000    Top5 98.000000    
2022-12-29 10:29:06,365 - ==> Top1: 45.500    Top5: 98.000    Loss: 1.341

2022-12-29 10:29:06,365 - ==> Confusion:
[[26  3  0 11  1  0]
 [ 4 18  3 13  4  1]
 [ 0  4  6 10 11  0]
 [ 5  6  2 30  3  0]
 [ 3  8  2  7 10  0]
 [ 2  2  0  4  0  1]]

2022-12-29 10:29:06,368 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:06,368 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:06,390 - 

2022-12-29 10:29:06,390 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:07,220 - Epoch: [251][   10/  113]    Overall Loss 1.195575    Objective Loss 1.195575                                        LR 0.000006    Time 0.082817    
2022-12-29 10:29:07,853 - Epoch: [251][   20/  113]    Overall Loss 1.228892    Objective Loss 1.228892                                        LR 0.000006    Time 0.073048    
2022-12-29 10:29:08,488 - Epoch: [251][   30/  113]    Overall Loss 1.262579    Objective Loss 1.262579                                        LR 0.000006    Time 0.069847    
2022-12-29 10:29:09,122 - Epoch: [251][   40/  113]    Overall Loss 1.276347    Objective Loss 1.276347                                        LR 0.000006    Time 0.068215    
2022-12-29 10:29:09,753 - Epoch: [251][   50/  113]    Overall Loss 1.280571    Objective Loss 1.280571                                        LR 0.000006    Time 0.067184    
2022-12-29 10:29:10,383 - Epoch: [251][   60/  113]    Overall Loss 1.284998    Objective Loss 1.284998                                        LR 0.000006    Time 0.066473    
2022-12-29 10:29:11,009 - Epoch: [251][   70/  113]    Overall Loss 1.279279    Objective Loss 1.279279                                        LR 0.000006    Time 0.065920    
2022-12-29 10:29:11,638 - Epoch: [251][   80/  113]    Overall Loss 1.262792    Objective Loss 1.262792                                        LR 0.000006    Time 0.065540    
2022-12-29 10:29:12,270 - Epoch: [251][   90/  113]    Overall Loss 1.269247    Objective Loss 1.269247                                        LR 0.000006    Time 0.065277    
2022-12-29 10:29:12,902 - Epoch: [251][  100/  113]    Overall Loss 1.270582    Objective Loss 1.270582                                        LR 0.000006    Time 0.065061    
2022-12-29 10:29:13,528 - Epoch: [251][  110/  113]    Overall Loss 1.267919    Objective Loss 1.267919                                        LR 0.000006    Time 0.064828    
2022-12-29 10:29:13,701 - Epoch: [251][  113/  113]    Overall Loss 1.271990    Objective Loss 1.271990    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064640    
2022-12-29 10:29:13,759 - --- validate (epoch=251)-----------
2022-12-29 10:29:13,760 - 200 samples (16 per mini-batch)
2022-12-29 10:29:14,322 - Epoch: [251][   10/   13]    Loss 1.444034    Top1 45.625000    Top5 96.250000    
2022-12-29 10:29:14,408 - Epoch: [251][   13/   13]    Loss 1.415368    Top1 46.000000    Top5 97.000000    
2022-12-29 10:29:14,476 - ==> Top1: 46.000    Top5: 97.000    Loss: 1.415

2022-12-29 10:29:14,476 - ==> Confusion:
[[20  6  0  4  3  2]
 [ 1 12  2  9 14  0]
 [ 2  7 17  5  6  0]
 [ 2  1  1 30  4  0]
 [ 1  5  4 18 11  0]
 [ 2  3  1  1  4  2]]

2022-12-29 10:29:14,480 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:14,480 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:14,501 - 

2022-12-29 10:29:14,501 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:15,352 - Epoch: [252][   10/  113]    Overall Loss 1.207178    Objective Loss 1.207178                                        LR 0.000006    Time 0.084950    
2022-12-29 10:29:15,996 - Epoch: [252][   20/  113]    Overall Loss 1.244378    Objective Loss 1.244378                                        LR 0.000006    Time 0.074654    
2022-12-29 10:29:16,634 - Epoch: [252][   30/  113]    Overall Loss 1.197604    Objective Loss 1.197604                                        LR 0.000006    Time 0.071011    
2022-12-29 10:29:17,273 - Epoch: [252][   40/  113]    Overall Loss 1.179910    Objective Loss 1.179910                                        LR 0.000006    Time 0.069223    
2022-12-29 10:29:17,908 - Epoch: [252][   50/  113]    Overall Loss 1.196607    Objective Loss 1.196607                                        LR 0.000006    Time 0.068069    
2022-12-29 10:29:18,549 - Epoch: [252][   60/  113]    Overall Loss 1.187084    Objective Loss 1.187084                                        LR 0.000006    Time 0.067400    
2022-12-29 10:29:19,184 - Epoch: [252][   70/  113]    Overall Loss 1.192162    Objective Loss 1.192162                                        LR 0.000006    Time 0.066844    
2022-12-29 10:29:19,821 - Epoch: [252][   80/  113]    Overall Loss 1.188016    Objective Loss 1.188016                                        LR 0.000006    Time 0.066441    
2022-12-29 10:29:20,457 - Epoch: [252][   90/  113]    Overall Loss 1.203926    Objective Loss 1.203926                                        LR 0.000006    Time 0.066124    
2022-12-29 10:29:21,095 - Epoch: [252][  100/  113]    Overall Loss 1.217132    Objective Loss 1.217132                                        LR 0.000006    Time 0.065880    
2022-12-29 10:29:21,723 - Epoch: [252][  110/  113]    Overall Loss 1.220845    Objective Loss 1.220845                                        LR 0.000006    Time 0.065599    
2022-12-29 10:29:21,901 - Epoch: [252][  113/  113]    Overall Loss 1.220117    Objective Loss 1.220117    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065427    
2022-12-29 10:29:21,959 - --- validate (epoch=252)-----------
2022-12-29 10:29:21,959 - 200 samples (16 per mini-batch)
2022-12-29 10:29:22,524 - Epoch: [252][   10/   13]    Loss 1.317096    Top1 48.125000    Top5 96.875000    
2022-12-29 10:29:22,608 - Epoch: [252][   13/   13]    Loss 1.336191    Top1 48.000000    Top5 97.000000    
2022-12-29 10:29:22,661 - ==> Top1: 48.000    Top5: 97.000    Loss: 1.336

2022-12-29 10:29:22,662 - ==> Confusion:
[[18  2  1  4  8  0]
 [ 1 22  4  3  5  0]
 [ 2 12  6  6 13  0]
 [ 1  5  2 35 10  1]
 [ 2  3  3  7 15  2]
 [ 0  2  0  0  5  0]]

2022-12-29 10:29:22,665 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:22,665 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:22,688 - 

2022-12-29 10:29:22,688 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:23,528 - Epoch: [253][   10/  113]    Overall Loss 1.279148    Objective Loss 1.279148                                        LR 0.000006    Time 0.083878    
2022-12-29 10:29:24,169 - Epoch: [253][   20/  113]    Overall Loss 1.251878    Objective Loss 1.251878                                        LR 0.000006    Time 0.073964    
2022-12-29 10:29:24,812 - Epoch: [253][   30/  113]    Overall Loss 1.278972    Objective Loss 1.278972                                        LR 0.000006    Time 0.070717    
2022-12-29 10:29:25,450 - Epoch: [253][   40/  113]    Overall Loss 1.265700    Objective Loss 1.265700                                        LR 0.000006    Time 0.068868    
2022-12-29 10:29:26,086 - Epoch: [253][   50/  113]    Overall Loss 1.242338    Objective Loss 1.242338                                        LR 0.000006    Time 0.067815    
2022-12-29 10:29:26,717 - Epoch: [253][   60/  113]    Overall Loss 1.231053    Objective Loss 1.231053                                        LR 0.000006    Time 0.067015    
2022-12-29 10:29:27,350 - Epoch: [253][   70/  113]    Overall Loss 1.236008    Objective Loss 1.236008                                        LR 0.000006    Time 0.066472    
2022-12-29 10:29:27,982 - Epoch: [253][   80/  113]    Overall Loss 1.239511    Objective Loss 1.239511                                        LR 0.000006    Time 0.066063    
2022-12-29 10:29:28,618 - Epoch: [253][   90/  113]    Overall Loss 1.247819    Objective Loss 1.247819                                        LR 0.000006    Time 0.065784    
2022-12-29 10:29:29,247 - Epoch: [253][  100/  113]    Overall Loss 1.243491    Objective Loss 1.243491                                        LR 0.000006    Time 0.065494    
2022-12-29 10:29:29,880 - Epoch: [253][  110/  113]    Overall Loss 1.245211    Objective Loss 1.245211                                        LR 0.000006    Time 0.065283    
2022-12-29 10:29:30,052 - Epoch: [253][  113/  113]    Overall Loss 1.247313    Objective Loss 1.247313    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065075    
2022-12-29 10:29:30,116 - --- validate (epoch=253)-----------
2022-12-29 10:29:30,117 - 200 samples (16 per mini-batch)
2022-12-29 10:29:30,662 - Epoch: [253][   10/   13]    Loss 1.152939    Top1 55.000000    Top5 98.125000    
2022-12-29 10:29:30,746 - Epoch: [253][   13/   13]    Loss 1.121333    Top1 55.500000    Top5 98.500000    
2022-12-29 10:29:30,792 - ==> Top1: 55.500    Top5: 98.500    Loss: 1.121

2022-12-29 10:29:30,792 - ==> Confusion:
[[22  2  0  4  1  0]
 [ 0 24  7  4  4  1]
 [ 0  7 12  5  8  1]
 [ 3  7  1 38  3  0]
 [ 2 10  2  7 12  0]
 [ 0  4  0  2  4  3]]

2022-12-29 10:29:30,795 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:30,795 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:30,813 - 

2022-12-29 10:29:30,814 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:31,653 - Epoch: [254][   10/  113]    Overall Loss 1.294263    Objective Loss 1.294263                                        LR 0.000006    Time 0.083754    
2022-12-29 10:29:32,285 - Epoch: [254][   20/  113]    Overall Loss 1.258537    Objective Loss 1.258537                                        LR 0.000006    Time 0.073428    
2022-12-29 10:29:32,921 - Epoch: [254][   30/  113]    Overall Loss 1.243107    Objective Loss 1.243107                                        LR 0.000006    Time 0.070144    
2022-12-29 10:29:33,546 - Epoch: [254][   40/  113]    Overall Loss 1.217302    Objective Loss 1.217302                                        LR 0.000006    Time 0.068217    
2022-12-29 10:29:34,179 - Epoch: [254][   50/  113]    Overall Loss 1.213173    Objective Loss 1.213173                                        LR 0.000006    Time 0.067222    
2022-12-29 10:29:34,812 - Epoch: [254][   60/  113]    Overall Loss 1.224186    Objective Loss 1.224186                                        LR 0.000006    Time 0.066562    
2022-12-29 10:29:35,441 - Epoch: [254][   70/  113]    Overall Loss 1.222585    Objective Loss 1.222585                                        LR 0.000006    Time 0.066040    
2022-12-29 10:29:36,074 - Epoch: [254][   80/  113]    Overall Loss 1.223076    Objective Loss 1.223076                                        LR 0.000006    Time 0.065682    
2022-12-29 10:29:36,705 - Epoch: [254][   90/  113]    Overall Loss 1.234265    Objective Loss 1.234265                                        LR 0.000006    Time 0.065399    
2022-12-29 10:29:37,335 - Epoch: [254][  100/  113]    Overall Loss 1.219746    Objective Loss 1.219746                                        LR 0.000006    Time 0.065148    
2022-12-29 10:29:37,963 - Epoch: [254][  110/  113]    Overall Loss 1.222397    Objective Loss 1.222397                                        LR 0.000006    Time 0.064934    
2022-12-29 10:29:38,135 - Epoch: [254][  113/  113]    Overall Loss 1.221596    Objective Loss 1.221596    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064725    
2022-12-29 10:29:38,190 - --- validate (epoch=254)-----------
2022-12-29 10:29:38,191 - 200 samples (16 per mini-batch)
2022-12-29 10:29:38,750 - Epoch: [254][   10/   13]    Loss 1.228288    Top1 56.250000    Top5 95.625000    
2022-12-29 10:29:38,836 - Epoch: [254][   13/   13]    Loss 1.210702    Top1 53.500000    Top5 96.500000    
2022-12-29 10:29:38,897 - ==> Top1: 53.500    Top5: 96.500    Loss: 1.211

2022-12-29 10:29:38,898 - ==> Confusion:
[[22  4  0  5  2  0]
 [ 1 24  4  6  4  0]
 [ 0  5 12  5  6  0]
 [ 5  5  1 38  5  1]
 [ 3  6  2  9 11  1]
 [ 1  3  4  2  3  0]]

2022-12-29 10:29:38,902 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:38,902 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:38,923 - 

2022-12-29 10:29:38,923 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:39,891 - Epoch: [255][   10/  113]    Overall Loss 1.293674    Objective Loss 1.293674                                        LR 0.000006    Time 0.096610    
2022-12-29 10:29:40,525 - Epoch: [255][   20/  113]    Overall Loss 1.273142    Objective Loss 1.273142                                        LR 0.000006    Time 0.079953    
2022-12-29 10:29:41,159 - Epoch: [255][   30/  113]    Overall Loss 1.272697    Objective Loss 1.272697                                        LR 0.000006    Time 0.074420    
2022-12-29 10:29:41,788 - Epoch: [255][   40/  113]    Overall Loss 1.269414    Objective Loss 1.269414                                        LR 0.000006    Time 0.071550    
2022-12-29 10:29:42,428 - Epoch: [255][   50/  113]    Overall Loss 1.270105    Objective Loss 1.270105                                        LR 0.000006    Time 0.070029    
2022-12-29 10:29:43,065 - Epoch: [255][   60/  113]    Overall Loss 1.257131    Objective Loss 1.257131                                        LR 0.000006    Time 0.068966    
2022-12-29 10:29:43,702 - Epoch: [255][   70/  113]    Overall Loss 1.259667    Objective Loss 1.259667                                        LR 0.000006    Time 0.068199    
2022-12-29 10:29:44,335 - Epoch: [255][   80/  113]    Overall Loss 1.252376    Objective Loss 1.252376                                        LR 0.000006    Time 0.067585    
2022-12-29 10:29:44,969 - Epoch: [255][   90/  113]    Overall Loss 1.253358    Objective Loss 1.253358                                        LR 0.000006    Time 0.067108    
2022-12-29 10:29:45,595 - Epoch: [255][  100/  113]    Overall Loss 1.253527    Objective Loss 1.253527                                        LR 0.000006    Time 0.066652    
2022-12-29 10:29:46,217 - Epoch: [255][  110/  113]    Overall Loss 1.261278    Objective Loss 1.261278                                        LR 0.000006    Time 0.066243    
2022-12-29 10:29:46,392 - Epoch: [255][  113/  113]    Overall Loss 1.261355    Objective Loss 1.261355    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.066036    
2022-12-29 10:29:46,453 - --- validate (epoch=255)-----------
2022-12-29 10:29:46,454 - 200 samples (16 per mini-batch)
2022-12-29 10:29:47,000 - Epoch: [255][   10/   13]    Loss 1.286808    Top1 48.750000    Top5 98.125000    
2022-12-29 10:29:47,084 - Epoch: [255][   13/   13]    Loss 1.317953    Top1 50.500000    Top5 98.000000    
2022-12-29 10:29:47,132 - ==> Top1: 50.500    Top5: 98.000    Loss: 1.318

2022-12-29 10:29:47,133 - ==> Confusion:
[[13  4  2  3  2  1]
 [ 2 23  7  6 12  0]
 [ 1  7 14  2  8  0]
 [ 4  5  1 30  6  0]
 [ 2  6  3  5 20  0]
 [ 1  2  1  4  2  1]]

2022-12-29 10:29:47,136 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:47,137 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:47,147 - 

2022-12-29 10:29:47,147 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:47,983 - Epoch: [256][   10/  113]    Overall Loss 1.308040    Objective Loss 1.308040                                        LR 0.000006    Time 0.083446    
2022-12-29 10:29:48,619 - Epoch: [256][   20/  113]    Overall Loss 1.283645    Objective Loss 1.283645                                        LR 0.000006    Time 0.073493    
2022-12-29 10:29:49,251 - Epoch: [256][   30/  113]    Overall Loss 1.267403    Objective Loss 1.267403                                        LR 0.000006    Time 0.070066    
2022-12-29 10:29:49,882 - Epoch: [256][   40/  113]    Overall Loss 1.266301    Objective Loss 1.266301                                        LR 0.000006    Time 0.068307    
2022-12-29 10:29:50,515 - Epoch: [256][   50/  113]    Overall Loss 1.249111    Objective Loss 1.249111                                        LR 0.000006    Time 0.067286    
2022-12-29 10:29:51,152 - Epoch: [256][   60/  113]    Overall Loss 1.246679    Objective Loss 1.246679                                        LR 0.000006    Time 0.066688    
2022-12-29 10:29:51,790 - Epoch: [256][   70/  113]    Overall Loss 1.240417    Objective Loss 1.240417                                        LR 0.000006    Time 0.066275    
2022-12-29 10:29:52,425 - Epoch: [256][   80/  113]    Overall Loss 1.230816    Objective Loss 1.230816                                        LR 0.000006    Time 0.065918    
2022-12-29 10:29:53,060 - Epoch: [256][   90/  113]    Overall Loss 1.254912    Objective Loss 1.254912                                        LR 0.000006    Time 0.065643    
2022-12-29 10:29:53,690 - Epoch: [256][  100/  113]    Overall Loss 1.249276    Objective Loss 1.249276                                        LR 0.000006    Time 0.065374    
2022-12-29 10:29:54,322 - Epoch: [256][  110/  113]    Overall Loss 1.252264    Objective Loss 1.252264                                        LR 0.000006    Time 0.065172    
2022-12-29 10:29:54,494 - Epoch: [256][  113/  113]    Overall Loss 1.254137    Objective Loss 1.254137    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064957    
2022-12-29 10:29:54,541 - --- validate (epoch=256)-----------
2022-12-29 10:29:54,541 - 200 samples (16 per mini-batch)
2022-12-29 10:29:55,089 - Epoch: [256][   10/   13]    Loss 1.331595    Top1 51.250000    Top5 95.625000    
2022-12-29 10:29:55,173 - Epoch: [256][   13/   13]    Loss 1.259990    Top1 54.000000    Top5 96.500000    
2022-12-29 10:29:55,237 - ==> Top1: 54.000    Top5: 96.500    Loss: 1.260

2022-12-29 10:29:55,238 - ==> Confusion:
[[21  2  1  7  3  0]
 [ 0 19  2  8  4  0]
 [ 0  6  9  7  6  1]
 [ 3  4  1 37  2  1]
 [ 4  4  1 14 21  0]
 [ 1  3  2  3  2  1]]

2022-12-29 10:29:55,240 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:29:55,240 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:29:55,264 - 

2022-12-29 10:29:55,265 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:29:56,110 - Epoch: [257][   10/  113]    Overall Loss 1.305796    Objective Loss 1.305796                                        LR 0.000006    Time 0.084461    
2022-12-29 10:29:56,740 - Epoch: [257][   20/  113]    Overall Loss 1.284303    Objective Loss 1.284303                                        LR 0.000006    Time 0.073697    
2022-12-29 10:29:57,369 - Epoch: [257][   30/  113]    Overall Loss 1.263244    Objective Loss 1.263244                                        LR 0.000006    Time 0.070079    
2022-12-29 10:29:57,997 - Epoch: [257][   40/  113]    Overall Loss 1.253579    Objective Loss 1.253579                                        LR 0.000006    Time 0.068229    
2022-12-29 10:29:58,633 - Epoch: [257][   50/  113]    Overall Loss 1.245597    Objective Loss 1.245597                                        LR 0.000006    Time 0.067306    
2022-12-29 10:29:59,264 - Epoch: [257][   60/  113]    Overall Loss 1.256059    Objective Loss 1.256059                                        LR 0.000006    Time 0.066589    
2022-12-29 10:29:59,893 - Epoch: [257][   70/  113]    Overall Loss 1.255576    Objective Loss 1.255576                                        LR 0.000006    Time 0.066066    
2022-12-29 10:30:00,530 - Epoch: [257][   80/  113]    Overall Loss 1.269551    Objective Loss 1.269551                                        LR 0.000006    Time 0.065755    
2022-12-29 10:30:01,161 - Epoch: [257][   90/  113]    Overall Loss 1.268432    Objective Loss 1.268432                                        LR 0.000006    Time 0.065458    
2022-12-29 10:30:01,790 - Epoch: [257][  100/  113]    Overall Loss 1.260494    Objective Loss 1.260494                                        LR 0.000006    Time 0.065194    
2022-12-29 10:30:02,427 - Epoch: [257][  110/  113]    Overall Loss 1.268987    Objective Loss 1.268987                                        LR 0.000006    Time 0.065057    
2022-12-29 10:30:02,603 - Epoch: [257][  113/  113]    Overall Loss 1.268502    Objective Loss 1.268502    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064887    
2022-12-29 10:30:02,654 - --- validate (epoch=257)-----------
2022-12-29 10:30:02,654 - 200 samples (16 per mini-batch)
2022-12-29 10:30:03,196 - Epoch: [257][   10/   13]    Loss 1.285798    Top1 51.875000    Top5 99.375000    
2022-12-29 10:30:03,281 - Epoch: [257][   13/   13]    Loss 1.253512    Top1 53.500000    Top5 99.500000    
2022-12-29 10:30:03,330 - ==> Top1: 53.500    Top5: 99.500    Loss: 1.254

2022-12-29 10:30:03,331 - ==> Confusion:
[[23  1  1  8  1  0]
 [ 2 17  0  6 13  0]
 [ 2  8 11  6  9  0]
 [ 3  1  1 37  4  0]
 [ 4  4  2  7 19  1]
 [ 1  4  0  2  2  0]]

2022-12-29 10:30:03,335 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:03,335 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:03,357 - 

2022-12-29 10:30:03,358 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:04,187 - Epoch: [258][   10/  113]    Overall Loss 1.397316    Objective Loss 1.397316                                        LR 0.000006    Time 0.082837    
2022-12-29 10:30:04,823 - Epoch: [258][   20/  113]    Overall Loss 1.306042    Objective Loss 1.306042                                        LR 0.000006    Time 0.073162    
2022-12-29 10:30:05,459 - Epoch: [258][   30/  113]    Overall Loss 1.320973    Objective Loss 1.320973                                        LR 0.000006    Time 0.069987    
2022-12-29 10:30:06,096 - Epoch: [258][   40/  113]    Overall Loss 1.276378    Objective Loss 1.276378                                        LR 0.000006    Time 0.068393    
2022-12-29 10:30:06,737 - Epoch: [258][   50/  113]    Overall Loss 1.262391    Objective Loss 1.262391                                        LR 0.000006    Time 0.067517    
2022-12-29 10:30:07,372 - Epoch: [258][   60/  113]    Overall Loss 1.261046    Objective Loss 1.261046                                        LR 0.000006    Time 0.066849    
2022-12-29 10:30:08,004 - Epoch: [258][   70/  113]    Overall Loss 1.261524    Objective Loss 1.261524                                        LR 0.000006    Time 0.066314    
2022-12-29 10:30:08,633 - Epoch: [258][   80/  113]    Overall Loss 1.271282    Objective Loss 1.271282                                        LR 0.000006    Time 0.065878    
2022-12-29 10:30:09,271 - Epoch: [258][   90/  113]    Overall Loss 1.275837    Objective Loss 1.275837                                        LR 0.000006    Time 0.065644    
2022-12-29 10:30:09,905 - Epoch: [258][  100/  113]    Overall Loss 1.266977    Objective Loss 1.266977                                        LR 0.000006    Time 0.065418    
2022-12-29 10:30:10,530 - Epoch: [258][  110/  113]    Overall Loss 1.273426    Objective Loss 1.273426                                        LR 0.000006    Time 0.065144    
2022-12-29 10:30:10,706 - Epoch: [258][  113/  113]    Overall Loss 1.274118    Objective Loss 1.274118    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.064972    
2022-12-29 10:30:10,776 - --- validate (epoch=258)-----------
2022-12-29 10:30:10,776 - 200 samples (16 per mini-batch)
2022-12-29 10:30:11,328 - Epoch: [258][   10/   13]    Loss 1.346114    Top1 48.125000    Top5 99.375000    
2022-12-29 10:30:11,412 - Epoch: [258][   13/   13]    Loss 1.317753    Top1 49.000000    Top5 98.500000    
2022-12-29 10:30:11,467 - ==> Top1: 49.000    Top5: 98.500    Loss: 1.318

2022-12-29 10:30:11,467 - ==> Confusion:
[[19  2  0  8  2  0]
 [ 4 32  2 11  3  0]
 [ 3  7  4  9  7  0]
 [ 6  8  1 31  3  0]
 [ 0  9  2 10 11  0]
 [ 0  1  0  3  1  1]]

2022-12-29 10:30:11,470 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:11,470 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:11,483 - 

2022-12-29 10:30:11,484 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:12,315 - Epoch: [259][   10/  113]    Overall Loss 1.372282    Objective Loss 1.372282                                        LR 0.000006    Time 0.083041    
2022-12-29 10:30:12,955 - Epoch: [259][   20/  113]    Overall Loss 1.336913    Objective Loss 1.336913                                        LR 0.000006    Time 0.073491    
2022-12-29 10:30:13,590 - Epoch: [259][   30/  113]    Overall Loss 1.280846    Objective Loss 1.280846                                        LR 0.000006    Time 0.070133    
2022-12-29 10:30:14,221 - Epoch: [259][   40/  113]    Overall Loss 1.275303    Objective Loss 1.275303                                        LR 0.000006    Time 0.068370    
2022-12-29 10:30:14,853 - Epoch: [259][   50/  113]    Overall Loss 1.262735    Objective Loss 1.262735                                        LR 0.000006    Time 0.067327    
2022-12-29 10:30:15,486 - Epoch: [259][   60/  113]    Overall Loss 1.282985    Objective Loss 1.282985                                        LR 0.000006    Time 0.066655    
2022-12-29 10:30:16,119 - Epoch: [259][   70/  113]    Overall Loss 1.266695    Objective Loss 1.266695                                        LR 0.000006    Time 0.066168    
2022-12-29 10:30:16,755 - Epoch: [259][   80/  113]    Overall Loss 1.272833    Objective Loss 1.272833                                        LR 0.000006    Time 0.065831    
2022-12-29 10:30:17,387 - Epoch: [259][   90/  113]    Overall Loss 1.275563    Objective Loss 1.275563                                        LR 0.000006    Time 0.065544    
2022-12-29 10:30:18,021 - Epoch: [259][  100/  113]    Overall Loss 1.277394    Objective Loss 1.277394                                        LR 0.000006    Time 0.065320    
2022-12-29 10:30:18,646 - Epoch: [259][  110/  113]    Overall Loss 1.278696    Objective Loss 1.278696                                        LR 0.000006    Time 0.065062    
2022-12-29 10:30:18,821 - Epoch: [259][  113/  113]    Overall Loss 1.282342    Objective Loss 1.282342    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064875    
2022-12-29 10:30:18,876 - --- validate (epoch=259)-----------
2022-12-29 10:30:18,876 - 200 samples (16 per mini-batch)
2022-12-29 10:30:19,427 - Epoch: [259][   10/   13]    Loss 1.267092    Top1 50.625000    Top5 96.250000    
2022-12-29 10:30:19,513 - Epoch: [259][   13/   13]    Loss 1.292041    Top1 48.000000    Top5 96.500000    
2022-12-29 10:30:19,559 - ==> Top1: 48.000    Top5: 96.500    Loss: 1.292

2022-12-29 10:30:19,560 - ==> Confusion:
[[26  5  0  6  5  0]
 [ 2 22  1  3 13  0]
 [ 2 10  6  3  7  0]
 [ 0  3  3 29  9  2]
 [ 1 12  1  3 12  0]
 [ 0  5  3  1  4  1]]

2022-12-29 10:30:19,564 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:19,564 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:19,589 - 

2022-12-29 10:30:19,589 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:20,429 - Epoch: [260][   10/  113]    Overall Loss 1.220088    Objective Loss 1.220088                                        LR 0.000006    Time 0.083913    
2022-12-29 10:30:21,065 - Epoch: [260][   20/  113]    Overall Loss 1.244930    Objective Loss 1.244930                                        LR 0.000006    Time 0.073714    
2022-12-29 10:30:21,706 - Epoch: [260][   30/  113]    Overall Loss 1.283298    Objective Loss 1.283298                                        LR 0.000006    Time 0.070516    
2022-12-29 10:30:22,341 - Epoch: [260][   40/  113]    Overall Loss 1.306152    Objective Loss 1.306152                                        LR 0.000006    Time 0.068743    
2022-12-29 10:30:22,980 - Epoch: [260][   50/  113]    Overall Loss 1.290164    Objective Loss 1.290164                                        LR 0.000006    Time 0.067757    
2022-12-29 10:30:23,608 - Epoch: [260][   60/  113]    Overall Loss 1.284829    Objective Loss 1.284829                                        LR 0.000006    Time 0.066928    
2022-12-29 10:30:24,238 - Epoch: [260][   70/  113]    Overall Loss 1.298373    Objective Loss 1.298373                                        LR 0.000006    Time 0.066353    
2022-12-29 10:30:24,864 - Epoch: [260][   80/  113]    Overall Loss 1.287174    Objective Loss 1.287174                                        LR 0.000006    Time 0.065877    
2022-12-29 10:30:25,495 - Epoch: [260][   90/  113]    Overall Loss 1.270096    Objective Loss 1.270096                                        LR 0.000006    Time 0.065571    
2022-12-29 10:30:26,123 - Epoch: [260][  100/  113]    Overall Loss 1.271512    Objective Loss 1.271512                                        LR 0.000006    Time 0.065286    
2022-12-29 10:30:26,748 - Epoch: [260][  110/  113]    Overall Loss 1.268306    Objective Loss 1.268306                                        LR 0.000006    Time 0.065028    
2022-12-29 10:30:26,918 - Epoch: [260][  113/  113]    Overall Loss 1.276034    Objective Loss 1.276034    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.064808    
2022-12-29 10:30:26,979 - --- validate (epoch=260)-----------
2022-12-29 10:30:26,980 - 200 samples (16 per mini-batch)
2022-12-29 10:30:27,533 - Epoch: [260][   10/   13]    Loss 1.282966    Top1 46.875000    Top5 98.125000    
2022-12-29 10:30:27,621 - Epoch: [260][   13/   13]    Loss 1.228008    Top1 50.000000    Top5 98.500000    
2022-12-29 10:30:27,678 - ==> Top1: 50.000    Top5: 98.500    Loss: 1.228

2022-12-29 10:30:27,678 - ==> Confusion:
[[25  5  0  4  2  0]
 [ 1 14  2 12  9  0]
 [ 0  6  6  5  7  0]
 [ 2  1  3 38  3  0]
 [ 3  8  4 13 16  0]
 [ 3  3  2  0  2  1]]

2022-12-29 10:30:27,681 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:27,681 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:27,693 - 

2022-12-29 10:30:27,693 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:28,529 - Epoch: [261][   10/  113]    Overall Loss 1.394887    Objective Loss 1.394887                                        LR 0.000006    Time 0.083553    
2022-12-29 10:30:29,167 - Epoch: [261][   20/  113]    Overall Loss 1.317742    Objective Loss 1.317742                                        LR 0.000006    Time 0.073651    
2022-12-29 10:30:29,805 - Epoch: [261][   30/  113]    Overall Loss 1.257298    Objective Loss 1.257298                                        LR 0.000006    Time 0.070320    
2022-12-29 10:30:30,440 - Epoch: [261][   40/  113]    Overall Loss 1.277691    Objective Loss 1.277691                                        LR 0.000006    Time 0.068619    
2022-12-29 10:30:31,079 - Epoch: [261][   50/  113]    Overall Loss 1.270757    Objective Loss 1.270757                                        LR 0.000006    Time 0.067671    
2022-12-29 10:30:31,712 - Epoch: [261][   60/  113]    Overall Loss 1.264166    Objective Loss 1.264166                                        LR 0.000006    Time 0.066921    
2022-12-29 10:30:32,347 - Epoch: [261][   70/  113]    Overall Loss 1.265168    Objective Loss 1.265168                                        LR 0.000006    Time 0.066436    
2022-12-29 10:30:32,989 - Epoch: [261][   80/  113]    Overall Loss 1.257021    Objective Loss 1.257021                                        LR 0.000006    Time 0.066142    
2022-12-29 10:30:33,622 - Epoch: [261][   90/  113]    Overall Loss 1.257710    Objective Loss 1.257710                                        LR 0.000006    Time 0.065820    
2022-12-29 10:30:34,253 - Epoch: [261][  100/  113]    Overall Loss 1.254657    Objective Loss 1.254657                                        LR 0.000006    Time 0.065550    
2022-12-29 10:30:34,879 - Epoch: [261][  110/  113]    Overall Loss 1.252458    Objective Loss 1.252458                                        LR 0.000006    Time 0.065275    
2022-12-29 10:30:35,050 - Epoch: [261][  113/  113]    Overall Loss 1.256130    Objective Loss 1.256130    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065055    
2022-12-29 10:30:35,092 - --- validate (epoch=261)-----------
2022-12-29 10:30:35,092 - 200 samples (16 per mini-batch)
2022-12-29 10:30:35,629 - Epoch: [261][   10/   13]    Loss 1.212648    Top1 52.500000    Top5 98.750000    
2022-12-29 10:30:35,714 - Epoch: [261][   13/   13]    Loss 1.253483    Top1 50.500000    Top5 98.000000    
2022-12-29 10:30:35,774 - ==> Top1: 50.500    Top5: 98.000    Loss: 1.253

2022-12-29 10:30:35,774 - ==> Confusion:
[[11  1  0 10  0  0]
 [ 3 18  2 16  2  1]
 [ 2  3 16  6  3  1]
 [ 7  2  0 43  7  0]
 [ 5 11  5  8 12  0]
 [ 1  1  0  1  1  1]]

2022-12-29 10:30:35,777 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:35,777 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:35,796 - 

2022-12-29 10:30:35,796 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:36,645 - Epoch: [262][   10/  113]    Overall Loss 1.196337    Objective Loss 1.196337                                        LR 0.000006    Time 0.084647    
2022-12-29 10:30:37,281 - Epoch: [262][   20/  113]    Overall Loss 1.224218    Objective Loss 1.224218                                        LR 0.000006    Time 0.074113    
2022-12-29 10:30:37,914 - Epoch: [262][   30/  113]    Overall Loss 1.223957    Objective Loss 1.223957                                        LR 0.000006    Time 0.070498    
2022-12-29 10:30:38,548 - Epoch: [262][   40/  113]    Overall Loss 1.223806    Objective Loss 1.223806                                        LR 0.000006    Time 0.068702    
2022-12-29 10:30:39,184 - Epoch: [262][   50/  113]    Overall Loss 1.245804    Objective Loss 1.245804                                        LR 0.000006    Time 0.067672    
2022-12-29 10:30:39,821 - Epoch: [262][   60/  113]    Overall Loss 1.242236    Objective Loss 1.242236                                        LR 0.000006    Time 0.067003    
2022-12-29 10:30:40,461 - Epoch: [262][   70/  113]    Overall Loss 1.232546    Objective Loss 1.232546                                        LR 0.000006    Time 0.066572    
2022-12-29 10:30:41,096 - Epoch: [262][   80/  113]    Overall Loss 1.239408    Objective Loss 1.239408                                        LR 0.000006    Time 0.066177    
2022-12-29 10:30:41,733 - Epoch: [262][   90/  113]    Overall Loss 1.239343    Objective Loss 1.239343                                        LR 0.000006    Time 0.065893    
2022-12-29 10:30:42,370 - Epoch: [262][  100/  113]    Overall Loss 1.239965    Objective Loss 1.239965                                        LR 0.000006    Time 0.065673    
2022-12-29 10:30:43,009 - Epoch: [262][  110/  113]    Overall Loss 1.246714    Objective Loss 1.246714                                        LR 0.000006    Time 0.065504    
2022-12-29 10:30:43,185 - Epoch: [262][  113/  113]    Overall Loss 1.237484    Objective Loss 1.237484    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.065319    
2022-12-29 10:30:43,233 - --- validate (epoch=262)-----------
2022-12-29 10:30:43,233 - 200 samples (16 per mini-batch)
2022-12-29 10:30:43,776 - Epoch: [262][   10/   13]    Loss 1.299413    Top1 51.250000    Top5 96.250000    
2022-12-29 10:30:43,860 - Epoch: [262][   13/   13]    Loss 1.250774    Top1 52.500000    Top5 97.000000    
2022-12-29 10:30:43,910 - ==> Top1: 52.500    Top5: 97.000    Loss: 1.251

2022-12-29 10:30:43,910 - ==> Confusion:
[[18  0  0  8  2  0]
 [ 5 20  6 13  7  0]
 [ 0  4 14  5  7  0]
 [ 3  5  1 37  1  0]
 [ 1  5  2  6 15  0]
 [ 0  4  0  5  5  1]]

2022-12-29 10:30:43,913 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:43,913 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:43,927 - 

2022-12-29 10:30:43,928 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:44,756 - Epoch: [263][   10/  113]    Overall Loss 1.350090    Objective Loss 1.350090                                        LR 0.000006    Time 0.082770    
2022-12-29 10:30:45,391 - Epoch: [263][   20/  113]    Overall Loss 1.302223    Objective Loss 1.302223                                        LR 0.000006    Time 0.073080    
2022-12-29 10:30:46,029 - Epoch: [263][   30/  113]    Overall Loss 1.268319    Objective Loss 1.268319                                        LR 0.000006    Time 0.069995    
2022-12-29 10:30:46,665 - Epoch: [263][   40/  113]    Overall Loss 1.240217    Objective Loss 1.240217                                        LR 0.000006    Time 0.068377    
2022-12-29 10:30:47,297 - Epoch: [263][   50/  113]    Overall Loss 1.235476    Objective Loss 1.235476                                        LR 0.000006    Time 0.067323    
2022-12-29 10:30:47,925 - Epoch: [263][   60/  113]    Overall Loss 1.245193    Objective Loss 1.245193                                        LR 0.000006    Time 0.066564    
2022-12-29 10:30:48,556 - Epoch: [263][   70/  113]    Overall Loss 1.243969    Objective Loss 1.243969                                        LR 0.000006    Time 0.066065    
2022-12-29 10:30:49,181 - Epoch: [263][   80/  113]    Overall Loss 1.228094    Objective Loss 1.228094                                        LR 0.000006    Time 0.065617    
2022-12-29 10:30:49,813 - Epoch: [263][   90/  113]    Overall Loss 1.232596    Objective Loss 1.232596                                        LR 0.000006    Time 0.065336    
2022-12-29 10:30:50,443 - Epoch: [263][  100/  113]    Overall Loss 1.235904    Objective Loss 1.235904                                        LR 0.000006    Time 0.065101    
2022-12-29 10:30:51,072 - Epoch: [263][  110/  113]    Overall Loss 1.241221    Objective Loss 1.241221                                        LR 0.000006    Time 0.064896    
2022-12-29 10:30:51,247 - Epoch: [263][  113/  113]    Overall Loss 1.247369    Objective Loss 1.247369    Top1 50.000000    Top5 87.500000    LR 0.000006    Time 0.064720    
2022-12-29 10:30:51,306 - --- validate (epoch=263)-----------
2022-12-29 10:30:51,306 - 200 samples (16 per mini-batch)
2022-12-29 10:30:51,867 - Epoch: [263][   10/   13]    Loss 1.265805    Top1 53.125000    Top5 95.625000    
2022-12-29 10:30:51,953 - Epoch: [263][   13/   13]    Loss 1.298402    Top1 49.500000    Top5 95.000000    
2022-12-29 10:30:52,001 - ==> Top1: 49.500    Top5: 95.000    Loss: 1.298

2022-12-29 10:30:52,001 - ==> Confusion:
[[21  2  0  7  0  0]
 [ 1 16  2 15  6  0]
 [ 2  6 10 10  8  1]
 [ 7  1  1 35  2  1]
 [ 1  4  1 11 16  0]
 [ 1  3  2  5  1  1]]

2022-12-29 10:30:52,003 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:30:52,004 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:30:52,022 - 

2022-12-29 10:30:52,022 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:30:52,861 - Epoch: [264][   10/  113]    Overall Loss 1.296331    Objective Loss 1.296331                                        LR 0.000006    Time 0.083738    
2022-12-29 10:30:53,498 - Epoch: [264][   20/  113]    Overall Loss 1.265829    Objective Loss 1.265829                                        LR 0.000006    Time 0.073707    
2022-12-29 10:30:54,137 - Epoch: [264][   30/  113]    Overall Loss 1.236876    Objective Loss 1.236876                                        LR 0.000006    Time 0.070429    
2022-12-29 10:30:54,773 - Epoch: [264][   40/  113]    Overall Loss 1.246832    Objective Loss 1.246832                                        LR 0.000006    Time 0.068698    
2022-12-29 10:30:55,408 - Epoch: [264][   50/  113]    Overall Loss 1.246614    Objective Loss 1.246614                                        LR 0.000006    Time 0.067659    
2022-12-29 10:30:56,044 - Epoch: [264][   60/  113]    Overall Loss 1.247156    Objective Loss 1.247156                                        LR 0.000006    Time 0.066973    
2022-12-29 10:30:56,679 - Epoch: [264][   70/  113]    Overall Loss 1.256765    Objective Loss 1.256765                                        LR 0.000006    Time 0.066465    
2022-12-29 10:30:57,310 - Epoch: [264][   80/  113]    Overall Loss 1.245762    Objective Loss 1.245762                                        LR 0.000006    Time 0.066040    
2022-12-29 10:30:57,946 - Epoch: [264][   90/  113]    Overall Loss 1.235727    Objective Loss 1.235727                                        LR 0.000006    Time 0.065766    
2022-12-29 10:30:58,577 - Epoch: [264][  100/  113]    Overall Loss 1.238911    Objective Loss 1.238911                                        LR 0.000006    Time 0.065494    
2022-12-29 10:30:59,201 - Epoch: [264][  110/  113]    Overall Loss 1.237860    Objective Loss 1.237860                                        LR 0.000006    Time 0.065206    
2022-12-29 10:30:59,379 - Epoch: [264][  113/  113]    Overall Loss 1.237013    Objective Loss 1.237013    Top1 62.500000    Top5 95.833333    LR 0.000006    Time 0.065045    
2022-12-29 10:30:59,432 - --- validate (epoch=264)-----------
2022-12-29 10:30:59,432 - 200 samples (16 per mini-batch)
2022-12-29 10:30:59,976 - Epoch: [264][   10/   13]    Loss 1.261214    Top1 51.875000    Top5 99.375000    
2022-12-29 10:31:00,062 - Epoch: [264][   13/   13]    Loss 1.320701    Top1 47.500000    Top5 98.500000    
2022-12-29 10:31:00,112 - ==> Top1: 47.500    Top5: 98.500    Loss: 1.321

2022-12-29 10:31:00,112 - ==> Confusion:
[[15  8  1  3  0  0]
 [ 4 25  4  3  6  1]
 [ 0 12 14  7  2  0]
 [ 8  1  0 26  3  1]
 [ 5 15  3  9 12  0]
 [ 1  5  2  0  1  3]]

2022-12-29 10:31:00,115 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:00,115 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:00,137 - 

2022-12-29 10:31:00,138 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:00,972 - Epoch: [265][   10/  113]    Overall Loss 1.183054    Objective Loss 1.183054                                        LR 0.000006    Time 0.083302    
2022-12-29 10:31:01,612 - Epoch: [265][   20/  113]    Overall Loss 1.216858    Objective Loss 1.216858                                        LR 0.000006    Time 0.073584    
2022-12-29 10:31:02,244 - Epoch: [265][   30/  113]    Overall Loss 1.234912    Objective Loss 1.234912                                        LR 0.000006    Time 0.070121    
2022-12-29 10:31:02,880 - Epoch: [265][   40/  113]    Overall Loss 1.265524    Objective Loss 1.265524                                        LR 0.000006    Time 0.068472    
2022-12-29 10:31:03,516 - Epoch: [265][   50/  113]    Overall Loss 1.259280    Objective Loss 1.259280                                        LR 0.000006    Time 0.067482    
2022-12-29 10:31:04,149 - Epoch: [265][   60/  113]    Overall Loss 1.254495    Objective Loss 1.254495                                        LR 0.000006    Time 0.066794    
2022-12-29 10:31:04,784 - Epoch: [265][   70/  113]    Overall Loss 1.261695    Objective Loss 1.261695                                        LR 0.000006    Time 0.066303    
2022-12-29 10:31:05,413 - Epoch: [265][   80/  113]    Overall Loss 1.234857    Objective Loss 1.234857                                        LR 0.000006    Time 0.065876    
2022-12-29 10:31:06,047 - Epoch: [265][   90/  113]    Overall Loss 1.228022    Objective Loss 1.228022                                        LR 0.000006    Time 0.065593    
2022-12-29 10:31:06,675 - Epoch: [265][  100/  113]    Overall Loss 1.233087    Objective Loss 1.233087                                        LR 0.000006    Time 0.065316    
2022-12-29 10:31:07,300 - Epoch: [265][  110/  113]    Overall Loss 1.240699    Objective Loss 1.240699                                        LR 0.000006    Time 0.065050    
2022-12-29 10:31:07,473 - Epoch: [265][  113/  113]    Overall Loss 1.242997    Objective Loss 1.242997    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064851    
2022-12-29 10:31:07,516 - --- validate (epoch=265)-----------
2022-12-29 10:31:07,516 - 200 samples (16 per mini-batch)
2022-12-29 10:31:08,060 - Epoch: [265][   10/   13]    Loss 1.408959    Top1 47.500000    Top5 96.875000    
2022-12-29 10:31:08,144 - Epoch: [265][   13/   13]    Loss 1.365823    Top1 46.500000    Top5 97.500000    
2022-12-29 10:31:08,190 - ==> Top1: 46.500    Top5: 97.500    Loss: 1.366

2022-12-29 10:31:08,190 - ==> Confusion:
[[19  3  0  5  4  0]
 [ 4 21  6 11  7  0]
 [ 0 10  6  4  3  0]
 [ 1  3  2 33  6  2]
 [ 2 11  3  7 13  0]
 [ 2  4  3  2  2  1]]

2022-12-29 10:31:08,192 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:08,193 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:08,212 - 

2022-12-29 10:31:08,212 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:09,059 - Epoch: [266][   10/  113]    Overall Loss 1.257511    Objective Loss 1.257511                                        LR 0.000006    Time 0.084496    
2022-12-29 10:31:09,701 - Epoch: [266][   20/  113]    Overall Loss 1.224069    Objective Loss 1.224069                                        LR 0.000006    Time 0.074318    
2022-12-29 10:31:10,341 - Epoch: [266][   30/  113]    Overall Loss 1.259996    Objective Loss 1.259996                                        LR 0.000006    Time 0.070870    
2022-12-29 10:31:10,978 - Epoch: [266][   40/  113]    Overall Loss 1.245678    Objective Loss 1.245678                                        LR 0.000006    Time 0.069049    
2022-12-29 10:31:11,613 - Epoch: [266][   50/  113]    Overall Loss 1.240579    Objective Loss 1.240579                                        LR 0.000006    Time 0.067946    
2022-12-29 10:31:12,248 - Epoch: [266][   60/  113]    Overall Loss 1.242620    Objective Loss 1.242620                                        LR 0.000006    Time 0.067187    
2022-12-29 10:31:12,885 - Epoch: [266][   70/  113]    Overall Loss 1.245159    Objective Loss 1.245159                                        LR 0.000006    Time 0.066691    
2022-12-29 10:31:13,520 - Epoch: [266][   80/  113]    Overall Loss 1.227730    Objective Loss 1.227730                                        LR 0.000006    Time 0.066276    
2022-12-29 10:31:14,153 - Epoch: [266][   90/  113]    Overall Loss 1.228881    Objective Loss 1.228881                                        LR 0.000006    Time 0.065945    
2022-12-29 10:31:14,783 - Epoch: [266][  100/  113]    Overall Loss 1.230885    Objective Loss 1.230885                                        LR 0.000006    Time 0.065644    
2022-12-29 10:31:15,415 - Epoch: [266][  110/  113]    Overall Loss 1.246567    Objective Loss 1.246567                                        LR 0.000006    Time 0.065418    
2022-12-29 10:31:15,587 - Epoch: [266][  113/  113]    Overall Loss 1.249359    Objective Loss 1.249359    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.065201    
2022-12-29 10:31:15,638 - --- validate (epoch=266)-----------
2022-12-29 10:31:15,639 - 200 samples (16 per mini-batch)
2022-12-29 10:31:16,191 - Epoch: [266][   10/   13]    Loss 1.144639    Top1 57.500000    Top5 96.875000    
2022-12-29 10:31:16,275 - Epoch: [266][   13/   13]    Loss 1.182105    Top1 54.500000    Top5 97.500000    
2022-12-29 10:31:16,323 - ==> Top1: 54.500    Top5: 97.500    Loss: 1.182

2022-12-29 10:31:16,323 - ==> Confusion:
[[23  1  1  3  2  0]
 [ 1 19  1  5  8  1]
 [ 1 10 13  7  8  0]
 [ 3  4  0 38  8  0]
 [ 3  3  5 11 14  0]
 [ 0  0  1  4  0  2]]

2022-12-29 10:31:16,326 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:16,326 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:16,339 - 

2022-12-29 10:31:16,339 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:17,171 - Epoch: [267][   10/  113]    Overall Loss 1.328749    Objective Loss 1.328749                                        LR 0.000006    Time 0.083076    
2022-12-29 10:31:17,809 - Epoch: [267][   20/  113]    Overall Loss 1.247281    Objective Loss 1.247281                                        LR 0.000006    Time 0.073400    
2022-12-29 10:31:18,449 - Epoch: [267][   30/  113]    Overall Loss 1.268560    Objective Loss 1.268560                                        LR 0.000006    Time 0.070257    
2022-12-29 10:31:19,087 - Epoch: [267][   40/  113]    Overall Loss 1.277039    Objective Loss 1.277039                                        LR 0.000006    Time 0.068624    
2022-12-29 10:31:19,719 - Epoch: [267][   50/  113]    Overall Loss 1.268277    Objective Loss 1.268277                                        LR 0.000006    Time 0.067526    
2022-12-29 10:31:20,355 - Epoch: [267][   60/  113]    Overall Loss 1.268681    Objective Loss 1.268681                                        LR 0.000006    Time 0.066879    
2022-12-29 10:31:20,986 - Epoch: [267][   70/  113]    Overall Loss 1.267896    Objective Loss 1.267896                                        LR 0.000006    Time 0.066329    
2022-12-29 10:31:21,618 - Epoch: [267][   80/  113]    Overall Loss 1.251595    Objective Loss 1.251595                                        LR 0.000006    Time 0.065934    
2022-12-29 10:31:22,250 - Epoch: [267][   90/  113]    Overall Loss 1.244827    Objective Loss 1.244827                                        LR 0.000006    Time 0.065623    
2022-12-29 10:31:22,885 - Epoch: [267][  100/  113]    Overall Loss 1.244875    Objective Loss 1.244875                                        LR 0.000006    Time 0.065410    
2022-12-29 10:31:23,515 - Epoch: [267][  110/  113]    Overall Loss 1.250383    Objective Loss 1.250383                                        LR 0.000006    Time 0.065181    
2022-12-29 10:31:23,686 - Epoch: [267][  113/  113]    Overall Loss 1.258953    Objective Loss 1.258953    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.064961    
2022-12-29 10:31:23,733 - --- validate (epoch=267)-----------
2022-12-29 10:31:23,734 - 200 samples (16 per mini-batch)
2022-12-29 10:31:24,275 - Epoch: [267][   10/   13]    Loss 1.214079    Top1 53.750000    Top5 97.500000    
2022-12-29 10:31:24,364 - Epoch: [267][   13/   13]    Loss 1.213986    Top1 55.000000    Top5 98.000000    
2022-12-29 10:31:24,413 - ==> Top1: 55.000    Top5: 98.000    Loss: 1.214

2022-12-29 10:31:24,414 - ==> Confusion:
[[23  3  0  7  3  0]
 [ 2 24  2  6  4  0]
 [ 0  6 11  9  7  0]
 [ 6  7  0 34  1  1]
 [ 2  6  2  8 15  0]
 [ 0  1  2  3  2  3]]

2022-12-29 10:31:24,415 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:24,416 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:24,434 - 

2022-12-29 10:31:24,434 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:25,263 - Epoch: [268][   10/  113]    Overall Loss 1.335962    Objective Loss 1.335962                                        LR 0.000006    Time 0.082751    
2022-12-29 10:31:25,901 - Epoch: [268][   20/  113]    Overall Loss 1.313975    Objective Loss 1.313975                                        LR 0.000006    Time 0.073272    
2022-12-29 10:31:26,541 - Epoch: [268][   30/  113]    Overall Loss 1.303071    Objective Loss 1.303071                                        LR 0.000006    Time 0.070153    
2022-12-29 10:31:27,177 - Epoch: [268][   40/  113]    Overall Loss 1.298905    Objective Loss 1.298905                                        LR 0.000006    Time 0.068516    
2022-12-29 10:31:27,813 - Epoch: [268][   50/  113]    Overall Loss 1.274547    Objective Loss 1.274547                                        LR 0.000006    Time 0.067513    
2022-12-29 10:31:28,445 - Epoch: [268][   60/  113]    Overall Loss 1.285301    Objective Loss 1.285301                                        LR 0.000006    Time 0.066788    
2022-12-29 10:31:29,081 - Epoch: [268][   70/  113]    Overall Loss 1.283234    Objective Loss 1.283234                                        LR 0.000006    Time 0.066329    
2022-12-29 10:31:29,718 - Epoch: [268][   80/  113]    Overall Loss 1.275098    Objective Loss 1.275098                                        LR 0.000006    Time 0.065987    
2022-12-29 10:31:30,354 - Epoch: [268][   90/  113]    Overall Loss 1.269667    Objective Loss 1.269667                                        LR 0.000006    Time 0.065720    
2022-12-29 10:31:30,989 - Epoch: [268][  100/  113]    Overall Loss 1.267852    Objective Loss 1.267852                                        LR 0.000006    Time 0.065495    
2022-12-29 10:31:31,619 - Epoch: [268][  110/  113]    Overall Loss 1.267936    Objective Loss 1.267936                                        LR 0.000006    Time 0.065261    
2022-12-29 10:31:31,800 - Epoch: [268][  113/  113]    Overall Loss 1.266969    Objective Loss 1.266969    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065128    
2022-12-29 10:31:31,854 - --- validate (epoch=268)-----------
2022-12-29 10:31:31,854 - 200 samples (16 per mini-batch)
2022-12-29 10:31:32,401 - Epoch: [268][   10/   13]    Loss 1.354792    Top1 49.375000    Top5 97.500000    
2022-12-29 10:31:32,493 - Epoch: [268][   13/   13]    Loss 1.301848    Top1 48.500000    Top5 97.500000    
2022-12-29 10:31:32,556 - ==> Top1: 48.500    Top5: 97.500    Loss: 1.302

2022-12-29 10:31:32,556 - ==> Confusion:
[[18  3  0  4  3  0]
 [ 2 18  5  5  5  0]
 [ 1  7 11  9  6  0]
 [ 2  6  2 30  8  0]
 [ 3 12  4  6 17  0]
 [ 0  4  1  0  5  3]]

2022-12-29 10:31:32,559 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:32,559 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:32,571 - 

2022-12-29 10:31:32,572 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:33,407 - Epoch: [269][   10/  113]    Overall Loss 1.396111    Objective Loss 1.396111                                        LR 0.000006    Time 0.083495    
2022-12-29 10:31:34,046 - Epoch: [269][   20/  113]    Overall Loss 1.304702    Objective Loss 1.304702                                        LR 0.000006    Time 0.073647    
2022-12-29 10:31:34,684 - Epoch: [269][   30/  113]    Overall Loss 1.329022    Objective Loss 1.329022                                        LR 0.000006    Time 0.070336    
2022-12-29 10:31:35,321 - Epoch: [269][   40/  113]    Overall Loss 1.318372    Objective Loss 1.318372                                        LR 0.000006    Time 0.068678    
2022-12-29 10:31:35,960 - Epoch: [269][   50/  113]    Overall Loss 1.302937    Objective Loss 1.302937                                        LR 0.000006    Time 0.067712    
2022-12-29 10:31:36,594 - Epoch: [269][   60/  113]    Overall Loss 1.284865    Objective Loss 1.284865                                        LR 0.000006    Time 0.066989    
2022-12-29 10:31:37,226 - Epoch: [269][   70/  113]    Overall Loss 1.265035    Objective Loss 1.265035                                        LR 0.000006    Time 0.066440    
2022-12-29 10:31:37,856 - Epoch: [269][   80/  113]    Overall Loss 1.250091    Objective Loss 1.250091                                        LR 0.000006    Time 0.066000    
2022-12-29 10:31:38,487 - Epoch: [269][   90/  113]    Overall Loss 1.259380    Objective Loss 1.259380                                        LR 0.000006    Time 0.065679    
2022-12-29 10:31:39,117 - Epoch: [269][  100/  113]    Overall Loss 1.261054    Objective Loss 1.261054                                        LR 0.000006    Time 0.065407    
2022-12-29 10:31:39,739 - Epoch: [269][  110/  113]    Overall Loss 1.262639    Objective Loss 1.262639                                        LR 0.000006    Time 0.065109    
2022-12-29 10:31:39,912 - Epoch: [269][  113/  113]    Overall Loss 1.260698    Objective Loss 1.260698    Top1 66.666667    Top5 100.000000    LR 0.000006    Time 0.064912    
2022-12-29 10:31:39,962 - --- validate (epoch=269)-----------
2022-12-29 10:31:39,962 - 200 samples (16 per mini-batch)
2022-12-29 10:31:40,514 - Epoch: [269][   10/   13]    Loss 1.236951    Top1 54.375000    Top5 97.500000    
2022-12-29 10:31:40,600 - Epoch: [269][   13/   13]    Loss 1.260029    Top1 52.000000    Top5 98.000000    
2022-12-29 10:31:40,666 - ==> Top1: 52.000    Top5: 98.000    Loss: 1.260

2022-12-29 10:31:40,667 - ==> Confusion:
[[19  3  0  7  3  0]
 [ 4 28  5  2  6  0]
 [ 2  7  8 10  7  0]
 [ 2  5  1 34  4  0]
 [ 3 10  1  7 12  1]
 [ 1  3  0  1  1  3]]

2022-12-29 10:31:40,669 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:40,670 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:40,688 - 

2022-12-29 10:31:40,689 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:41,531 - Epoch: [270][   10/  113]    Overall Loss 1.121825    Objective Loss 1.121825                                        LR 0.000006    Time 0.084065    
2022-12-29 10:31:42,165 - Epoch: [270][   20/  113]    Overall Loss 1.194643    Objective Loss 1.194643                                        LR 0.000006    Time 0.073703    
2022-12-29 10:31:42,801 - Epoch: [270][   30/  113]    Overall Loss 1.203382    Objective Loss 1.203382                                        LR 0.000006    Time 0.070317    
2022-12-29 10:31:43,429 - Epoch: [270][   40/  113]    Overall Loss 1.231807    Objective Loss 1.231807                                        LR 0.000006    Time 0.068437    
2022-12-29 10:31:44,060 - Epoch: [270][   50/  113]    Overall Loss 1.236905    Objective Loss 1.236905                                        LR 0.000006    Time 0.067356    
2022-12-29 10:31:44,688 - Epoch: [270][   60/  113]    Overall Loss 1.248465    Objective Loss 1.248465                                        LR 0.000006    Time 0.066581    
2022-12-29 10:31:45,318 - Epoch: [270][   70/  113]    Overall Loss 1.254609    Objective Loss 1.254609                                        LR 0.000006    Time 0.066069    
2022-12-29 10:31:45,944 - Epoch: [270][   80/  113]    Overall Loss 1.242160    Objective Loss 1.242160                                        LR 0.000006    Time 0.065625    
2022-12-29 10:31:46,576 - Epoch: [270][   90/  113]    Overall Loss 1.248129    Objective Loss 1.248129                                        LR 0.000006    Time 0.065355    
2022-12-29 10:31:47,206 - Epoch: [270][  100/  113]    Overall Loss 1.258169    Objective Loss 1.258169                                        LR 0.000006    Time 0.065111    
2022-12-29 10:31:47,831 - Epoch: [270][  110/  113]    Overall Loss 1.253957    Objective Loss 1.253957                                        LR 0.000006    Time 0.064869    
2022-12-29 10:31:48,002 - Epoch: [270][  113/  113]    Overall Loss 1.253529    Objective Loss 1.253529    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064663    
2022-12-29 10:31:48,066 - --- validate (epoch=270)-----------
2022-12-29 10:31:48,066 - 200 samples (16 per mini-batch)
2022-12-29 10:31:48,616 - Epoch: [270][   10/   13]    Loss 1.248525    Top1 48.125000    Top5 99.375000    
2022-12-29 10:31:48,701 - Epoch: [270][   13/   13]    Loss 1.253026    Top1 48.500000    Top5 99.500000    
2022-12-29 10:31:48,745 - ==> Top1: 48.500    Top5: 99.500    Loss: 1.253

2022-12-29 10:31:48,745 - ==> Confusion:
[[23  2  3 10  4  0]
 [ 1 18  0 10 12  0]
 [ 1  5 10  9  5  0]
 [ 4  4  0 26  8  1]
 [ 1  4  6  5 16  0]
 [ 1  0  0  4  3  4]]

2022-12-29 10:31:48,748 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:48,748 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:48,768 - 

2022-12-29 10:31:48,769 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:49,597 - Epoch: [271][   10/  113]    Overall Loss 1.282336    Objective Loss 1.282336                                        LR 0.000006    Time 0.082752    
2022-12-29 10:31:50,236 - Epoch: [271][   20/  113]    Overall Loss 1.195876    Objective Loss 1.195876                                        LR 0.000006    Time 0.073295    
2022-12-29 10:31:50,871 - Epoch: [271][   30/  113]    Overall Loss 1.199959    Objective Loss 1.199959                                        LR 0.000006    Time 0.069995    
2022-12-29 10:31:51,505 - Epoch: [271][   40/  113]    Overall Loss 1.218008    Objective Loss 1.218008                                        LR 0.000006    Time 0.068337    
2022-12-29 10:31:52,137 - Epoch: [271][   50/  113]    Overall Loss 1.201806    Objective Loss 1.201806                                        LR 0.000006    Time 0.067303    
2022-12-29 10:31:52,770 - Epoch: [271][   60/  113]    Overall Loss 1.206747    Objective Loss 1.206747                                        LR 0.000006    Time 0.066630    
2022-12-29 10:31:53,405 - Epoch: [271][   70/  113]    Overall Loss 1.222965    Objective Loss 1.222965                                        LR 0.000006    Time 0.066179    
2022-12-29 10:31:54,040 - Epoch: [271][   80/  113]    Overall Loss 1.217663    Objective Loss 1.217663                                        LR 0.000006    Time 0.065833    
2022-12-29 10:31:54,674 - Epoch: [271][   90/  113]    Overall Loss 1.233615    Objective Loss 1.233615                                        LR 0.000006    Time 0.065558    
2022-12-29 10:31:55,304 - Epoch: [271][  100/  113]    Overall Loss 1.246970    Objective Loss 1.246970                                        LR 0.000006    Time 0.065302    
2022-12-29 10:31:55,933 - Epoch: [271][  110/  113]    Overall Loss 1.246168    Objective Loss 1.246168                                        LR 0.000006    Time 0.065075    
2022-12-29 10:31:56,104 - Epoch: [271][  113/  113]    Overall Loss 1.247671    Objective Loss 1.247671    Top1 66.666667    Top5 100.000000    LR 0.000006    Time 0.064853    
2022-12-29 10:31:56,165 - --- validate (epoch=271)-----------
2022-12-29 10:31:56,165 - 200 samples (16 per mini-batch)
2022-12-29 10:31:56,704 - Epoch: [271][   10/   13]    Loss 1.235762    Top1 52.500000    Top5 96.875000    
2022-12-29 10:31:56,791 - Epoch: [271][   13/   13]    Loss 1.222026    Top1 53.000000    Top5 97.000000    
2022-12-29 10:31:56,839 - ==> Top1: 53.000    Top5: 97.000    Loss: 1.222

2022-12-29 10:31:56,840 - ==> Confusion:
[[21  1  0  7  3  1]
 [ 4 30  2  8  7  1]
 [ 1  2 10  8  6  1]
 [ 5  4  2 30  4  1]
 [ 1  6  3 10 14  1]
 [ 2  1  0  2  0  1]]

2022-12-29 10:31:56,842 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:31:56,842 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:31:56,861 - 

2022-12-29 10:31:56,862 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:31:57,699 - Epoch: [272][   10/  113]    Overall Loss 1.342643    Objective Loss 1.342643                                        LR 0.000006    Time 0.083531    
2022-12-29 10:31:58,335 - Epoch: [272][   20/  113]    Overall Loss 1.284722    Objective Loss 1.284722                                        LR 0.000006    Time 0.073550    
2022-12-29 10:31:58,971 - Epoch: [272][   30/  113]    Overall Loss 1.253721    Objective Loss 1.253721                                        LR 0.000006    Time 0.070223    
2022-12-29 10:31:59,605 - Epoch: [272][   40/  113]    Overall Loss 1.224633    Objective Loss 1.224633                                        LR 0.000006    Time 0.068500    
2022-12-29 10:32:00,239 - Epoch: [272][   50/  113]    Overall Loss 1.219408    Objective Loss 1.219408                                        LR 0.000006    Time 0.067477    
2022-12-29 10:32:00,874 - Epoch: [272][   60/  113]    Overall Loss 1.241791    Objective Loss 1.241791                                        LR 0.000006    Time 0.066800    
2022-12-29 10:32:01,511 - Epoch: [272][   70/  113]    Overall Loss 1.243928    Objective Loss 1.243928                                        LR 0.000006    Time 0.066347    
2022-12-29 10:32:02,141 - Epoch: [272][   80/  113]    Overall Loss 1.241708    Objective Loss 1.241708                                        LR 0.000006    Time 0.065933    
2022-12-29 10:32:02,780 - Epoch: [272][   90/  113]    Overall Loss 1.245777    Objective Loss 1.245777                                        LR 0.000006    Time 0.065701    
2022-12-29 10:32:03,414 - Epoch: [272][  100/  113]    Overall Loss 1.249049    Objective Loss 1.249049                                        LR 0.000006    Time 0.065457    
2022-12-29 10:32:04,042 - Epoch: [272][  110/  113]    Overall Loss 1.240971    Objective Loss 1.240971                                        LR 0.000006    Time 0.065216    
2022-12-29 10:32:04,218 - Epoch: [272][  113/  113]    Overall Loss 1.244093    Objective Loss 1.244093    Top1 66.666667    Top5 100.000000    LR 0.000006    Time 0.065034    
2022-12-29 10:32:04,254 - --- validate (epoch=272)-----------
2022-12-29 10:32:04,254 - 200 samples (16 per mini-batch)
2022-12-29 10:32:04,807 - Epoch: [272][   10/   13]    Loss 1.255133    Top1 51.250000    Top5 96.250000    
2022-12-29 10:32:04,891 - Epoch: [272][   13/   13]    Loss 1.243766    Top1 52.000000    Top5 96.500000    
2022-12-29 10:32:04,948 - ==> Top1: 52.000    Top5: 96.500    Loss: 1.244

2022-12-29 10:32:04,948 - ==> Confusion:
[[16  1  0  6  2  1]
 [ 5 22  2  3 12  1]
 [ 1  5 13  5 11  0]
 [ 3  3  1 34  3  0]
 [ 3  6  2  6 18  0]
 [ 1  3  2  5  3  1]]

2022-12-29 10:32:04,950 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:04,950 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:04,969 - 

2022-12-29 10:32:04,969 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:05,807 - Epoch: [273][   10/  113]    Overall Loss 1.208958    Objective Loss 1.208958                                        LR 0.000006    Time 0.083567    
2022-12-29 10:32:06,441 - Epoch: [273][   20/  113]    Overall Loss 1.276779    Objective Loss 1.276779                                        LR 0.000006    Time 0.073492    
2022-12-29 10:32:07,079 - Epoch: [273][   30/  113]    Overall Loss 1.278470    Objective Loss 1.278470                                        LR 0.000006    Time 0.070227    
2022-12-29 10:32:07,717 - Epoch: [273][   40/  113]    Overall Loss 1.261250    Objective Loss 1.261250                                        LR 0.000006    Time 0.068619    
2022-12-29 10:32:08,356 - Epoch: [273][   50/  113]    Overall Loss 1.243211    Objective Loss 1.243211                                        LR 0.000006    Time 0.067656    
2022-12-29 10:32:08,994 - Epoch: [273][   60/  113]    Overall Loss 1.246802    Objective Loss 1.246802                                        LR 0.000006    Time 0.067012    
2022-12-29 10:32:09,633 - Epoch: [273][   70/  113]    Overall Loss 1.250859    Objective Loss 1.250859                                        LR 0.000006    Time 0.066561    
2022-12-29 10:32:10,266 - Epoch: [273][   80/  113]    Overall Loss 1.252715    Objective Loss 1.252715                                        LR 0.000006    Time 0.066138    
2022-12-29 10:32:10,901 - Epoch: [273][   90/  113]    Overall Loss 1.253608    Objective Loss 1.253608                                        LR 0.000006    Time 0.065850    
2022-12-29 10:32:11,537 - Epoch: [273][  100/  113]    Overall Loss 1.250659    Objective Loss 1.250659                                        LR 0.000006    Time 0.065618    
2022-12-29 10:32:12,168 - Epoch: [273][  110/  113]    Overall Loss 1.249401    Objective Loss 1.249401                                        LR 0.000006    Time 0.065384    
2022-12-29 10:32:12,343 - Epoch: [273][  113/  113]    Overall Loss 1.253878    Objective Loss 1.253878    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.065195    
2022-12-29 10:32:12,390 - --- validate (epoch=273)-----------
2022-12-29 10:32:12,391 - 200 samples (16 per mini-batch)
2022-12-29 10:32:12,947 - Epoch: [273][   10/   13]    Loss 1.276693    Top1 48.750000    Top5 96.875000    
2022-12-29 10:32:13,033 - Epoch: [273][   13/   13]    Loss 1.285554    Top1 50.000000    Top5 95.500000    
2022-12-29 10:32:13,077 - ==> Top1: 50.000    Top5: 95.500    Loss: 1.286

2022-12-29 10:32:13,078 - ==> Confusion:
[[19  2  0  3  6  0]
 [ 1 23  5  7  6  1]
 [ 0  4 11  5 15  0]
 [ 2  4  3 30  7  2]
 [ 0  4  6  7 15  0]
 [ 0  4  0  4  2  2]]

2022-12-29 10:32:13,081 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:13,081 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:13,094 - 

2022-12-29 10:32:13,094 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:13,925 - Epoch: [274][   10/  113]    Overall Loss 1.157410    Objective Loss 1.157410                                        LR 0.000006    Time 0.082949    
2022-12-29 10:32:14,560 - Epoch: [274][   20/  113]    Overall Loss 1.194181    Objective Loss 1.194181                                        LR 0.000006    Time 0.073197    
2022-12-29 10:32:15,199 - Epoch: [274][   30/  113]    Overall Loss 1.243473    Objective Loss 1.243473                                        LR 0.000006    Time 0.070106    
2022-12-29 10:32:15,840 - Epoch: [274][   40/  113]    Overall Loss 1.244569    Objective Loss 1.244569                                        LR 0.000006    Time 0.068574    
2022-12-29 10:32:16,477 - Epoch: [274][   50/  113]    Overall Loss 1.249488    Objective Loss 1.249488                                        LR 0.000006    Time 0.067593    
2022-12-29 10:32:17,108 - Epoch: [274][   60/  113]    Overall Loss 1.259705    Objective Loss 1.259705                                        LR 0.000006    Time 0.066833    
2022-12-29 10:32:17,741 - Epoch: [274][   70/  113]    Overall Loss 1.268364    Objective Loss 1.268364                                        LR 0.000006    Time 0.066335    
2022-12-29 10:32:18,375 - Epoch: [274][   80/  113]    Overall Loss 1.270325    Objective Loss 1.270325                                        LR 0.000006    Time 0.065963    
2022-12-29 10:32:19,009 - Epoch: [274][   90/  113]    Overall Loss 1.264257    Objective Loss 1.264257                                        LR 0.000006    Time 0.065667    
2022-12-29 10:32:19,644 - Epoch: [274][  100/  113]    Overall Loss 1.264894    Objective Loss 1.264894                                        LR 0.000006    Time 0.065446    
2022-12-29 10:32:20,277 - Epoch: [274][  110/  113]    Overall Loss 1.278472    Objective Loss 1.278472                                        LR 0.000006    Time 0.065243    
2022-12-29 10:32:20,450 - Epoch: [274][  113/  113]    Overall Loss 1.284815    Objective Loss 1.284815    Top1 33.333333    Top5 91.666667    LR 0.000006    Time 0.065037    
2022-12-29 10:32:20,513 - --- validate (epoch=274)-----------
2022-12-29 10:32:20,514 - 200 samples (16 per mini-batch)
2022-12-29 10:32:21,063 - Epoch: [274][   10/   13]    Loss 1.246962    Top1 50.625000    Top5 96.875000    
2022-12-29 10:32:21,152 - Epoch: [274][   13/   13]    Loss 1.195664    Top1 52.500000    Top5 97.500000    
2022-12-29 10:32:21,219 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.196

2022-12-29 10:32:21,220 - ==> Confusion:
[[22  3  1  7  0  0]
 [ 2 23  5  5  6  0]
 [ 2  8 16  2  8  1]
 [ 3  4  3 30  6  1]
 [ 2 15  3  3 14  0]
 [ 0  2  0  2  1  0]]

2022-12-29 10:32:21,222 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:21,222 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:21,244 - 

2022-12-29 10:32:21,244 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:22,082 - Epoch: [275][   10/  113]    Overall Loss 1.268828    Objective Loss 1.268828                                        LR 0.000006    Time 0.083747    
2022-12-29 10:32:22,724 - Epoch: [275][   20/  113]    Overall Loss 1.288047    Objective Loss 1.288047                                        LR 0.000006    Time 0.073950    
2022-12-29 10:32:23,361 - Epoch: [275][   30/  113]    Overall Loss 1.272323    Objective Loss 1.272323                                        LR 0.000006    Time 0.070494    
2022-12-29 10:32:24,000 - Epoch: [275][   40/  113]    Overall Loss 1.286323    Objective Loss 1.286323                                        LR 0.000006    Time 0.068852    
2022-12-29 10:32:24,634 - Epoch: [275][   50/  113]    Overall Loss 1.291068    Objective Loss 1.291068                                        LR 0.000006    Time 0.067750    
2022-12-29 10:32:25,267 - Epoch: [275][   60/  113]    Overall Loss 1.289411    Objective Loss 1.289411                                        LR 0.000006    Time 0.066992    
2022-12-29 10:32:25,902 - Epoch: [275][   70/  113]    Overall Loss 1.287139    Objective Loss 1.287139                                        LR 0.000006    Time 0.066482    
2022-12-29 10:32:26,531 - Epoch: [275][   80/  113]    Overall Loss 1.299737    Objective Loss 1.299737                                        LR 0.000006    Time 0.066030    
2022-12-29 10:32:27,163 - Epoch: [275][   90/  113]    Overall Loss 1.301048    Objective Loss 1.301048                                        LR 0.000006    Time 0.065715    
2022-12-29 10:32:27,795 - Epoch: [275][  100/  113]    Overall Loss 1.298910    Objective Loss 1.298910                                        LR 0.000006    Time 0.065455    
2022-12-29 10:32:28,421 - Epoch: [275][  110/  113]    Overall Loss 1.302483    Objective Loss 1.302483                                        LR 0.000006    Time 0.065192    
2022-12-29 10:32:28,590 - Epoch: [275][  113/  113]    Overall Loss 1.294220    Objective Loss 1.294220    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064950    
2022-12-29 10:32:28,654 - --- validate (epoch=275)-----------
2022-12-29 10:32:28,654 - 200 samples (16 per mini-batch)
2022-12-29 10:32:29,204 - Epoch: [275][   10/   13]    Loss 1.355203    Top1 49.375000    Top5 96.250000    
2022-12-29 10:32:29,287 - Epoch: [275][   13/   13]    Loss 1.315391    Top1 51.500000    Top5 96.000000    
2022-12-29 10:32:29,334 - ==> Top1: 51.500    Top5: 96.000    Loss: 1.315

2022-12-29 10:32:29,334 - ==> Confusion:
[[19  7  0 13  2  0]
 [ 1 26  1  7  2  1]
 [ 3  6  8  3  5  0]
 [ 6  2  0 33  2  0]
 [ 5  4  1 15 14  0]
 [ 5  1  0  3  2  3]]

2022-12-29 10:32:29,337 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:29,337 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:29,347 - 

2022-12-29 10:32:29,347 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:30,181 - Epoch: [276][   10/  113]    Overall Loss 1.257495    Objective Loss 1.257495                                        LR 0.000006    Time 0.083324    
2022-12-29 10:32:30,815 - Epoch: [276][   20/  113]    Overall Loss 1.293393    Objective Loss 1.293393                                        LR 0.000006    Time 0.073342    
2022-12-29 10:32:31,451 - Epoch: [276][   30/  113]    Overall Loss 1.317450    Objective Loss 1.317450                                        LR 0.000006    Time 0.070056    
2022-12-29 10:32:32,094 - Epoch: [276][   40/  113]    Overall Loss 1.303477    Objective Loss 1.303477                                        LR 0.000006    Time 0.068620    
2022-12-29 10:32:32,733 - Epoch: [276][   50/  113]    Overall Loss 1.293070    Objective Loss 1.293070                                        LR 0.000006    Time 0.067671    
2022-12-29 10:32:33,368 - Epoch: [276][   60/  113]    Overall Loss 1.288703    Objective Loss 1.288703                                        LR 0.000006    Time 0.066957    
2022-12-29 10:32:34,005 - Epoch: [276][   70/  113]    Overall Loss 1.293084    Objective Loss 1.293084                                        LR 0.000006    Time 0.066485    
2022-12-29 10:32:34,637 - Epoch: [276][   80/  113]    Overall Loss 1.280124    Objective Loss 1.280124                                        LR 0.000006    Time 0.066075    
2022-12-29 10:32:35,276 - Epoch: [276][   90/  113]    Overall Loss 1.278847    Objective Loss 1.278847                                        LR 0.000006    Time 0.065821    
2022-12-29 10:32:35,914 - Epoch: [276][  100/  113]    Overall Loss 1.279900    Objective Loss 1.279900                                        LR 0.000006    Time 0.065618    
2022-12-29 10:32:36,547 - Epoch: [276][  110/  113]    Overall Loss 1.276514    Objective Loss 1.276514                                        LR 0.000006    Time 0.065402    
2022-12-29 10:32:36,718 - Epoch: [276][  113/  113]    Overall Loss 1.282458    Objective Loss 1.282458    Top1 29.166667    Top5 100.000000    LR 0.000006    Time 0.065176    
2022-12-29 10:32:36,771 - --- validate (epoch=276)-----------
2022-12-29 10:32:36,771 - 200 samples (16 per mini-batch)
2022-12-29 10:32:37,314 - Epoch: [276][   10/   13]    Loss 1.291168    Top1 49.375000    Top5 96.875000    
2022-12-29 10:32:37,400 - Epoch: [276][   13/   13]    Loss 1.318847    Top1 49.000000    Top5 97.500000    
2022-12-29 10:32:37,460 - ==> Top1: 49.000    Top5: 97.500    Loss: 1.319

2022-12-29 10:32:37,460 - ==> Confusion:
[[20  1  0  2  6  0]
 [ 3 29  4  4  6  0]
 [ 0 10  5  6  5  1]
 [ 2  2  0 28  5  1]
 [ 2  9  5 12 14  0]
 [ 3  3  3  4  3  2]]

2022-12-29 10:32:37,465 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:37,466 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:37,488 - 

2022-12-29 10:32:37,489 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:38,328 - Epoch: [277][   10/  113]    Overall Loss 1.305867    Objective Loss 1.305867                                        LR 0.000006    Time 0.083811    
2022-12-29 10:32:38,968 - Epoch: [277][   20/  113]    Overall Loss 1.310367    Objective Loss 1.310367                                        LR 0.000006    Time 0.073866    
2022-12-29 10:32:39,600 - Epoch: [277][   30/  113]    Overall Loss 1.277655    Objective Loss 1.277655                                        LR 0.000006    Time 0.070320    
2022-12-29 10:32:40,237 - Epoch: [277][   40/  113]    Overall Loss 1.244247    Objective Loss 1.244247                                        LR 0.000006    Time 0.068647    
2022-12-29 10:32:40,872 - Epoch: [277][   50/  113]    Overall Loss 1.258164    Objective Loss 1.258164                                        LR 0.000006    Time 0.067599    
2022-12-29 10:32:41,502 - Epoch: [277][   60/  113]    Overall Loss 1.248566    Objective Loss 1.248566                                        LR 0.000006    Time 0.066832    
2022-12-29 10:32:42,139 - Epoch: [277][   70/  113]    Overall Loss 1.250498    Objective Loss 1.250498                                        LR 0.000006    Time 0.066373    
2022-12-29 10:32:42,773 - Epoch: [277][   80/  113]    Overall Loss 1.241462    Objective Loss 1.241462                                        LR 0.000006    Time 0.065998    
2022-12-29 10:32:43,410 - Epoch: [277][   90/  113]    Overall Loss 1.245053    Objective Loss 1.245053                                        LR 0.000006    Time 0.065739    
2022-12-29 10:32:44,044 - Epoch: [277][  100/  113]    Overall Loss 1.236685    Objective Loss 1.236685                                        LR 0.000006    Time 0.065499    
2022-12-29 10:32:44,672 - Epoch: [277][  110/  113]    Overall Loss 1.248154    Objective Loss 1.248154                                        LR 0.000006    Time 0.065249    
2022-12-29 10:32:44,843 - Epoch: [277][  113/  113]    Overall Loss 1.250596    Objective Loss 1.250596    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065026    
2022-12-29 10:32:44,906 - --- validate (epoch=277)-----------
2022-12-29 10:32:44,906 - 200 samples (16 per mini-batch)
2022-12-29 10:32:45,456 - Epoch: [277][   10/   13]    Loss 1.339100    Top1 45.000000    Top5 98.750000    
2022-12-29 10:32:45,540 - Epoch: [277][   13/   13]    Loss 1.306975    Top1 47.000000    Top5 98.500000    
2022-12-29 10:32:45,597 - ==> Top1: 47.000    Top5: 98.500    Loss: 1.307

2022-12-29 10:32:45,598 - ==> Confusion:
[[18  5  0 11  1  0]
 [ 1 19  2 16  7  0]
 [ 0  8 16  4  8  0]
 [ 5  3  1 26  2  2]
 [ 0 10  3  9 14  0]
 [ 1  0  2  1  4  1]]

2022-12-29 10:32:45,601 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:45,602 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:45,625 - 

2022-12-29 10:32:45,625 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:46,470 - Epoch: [278][   10/  113]    Overall Loss 1.382402    Objective Loss 1.382402                                        LR 0.000006    Time 0.084338    
2022-12-29 10:32:47,107 - Epoch: [278][   20/  113]    Overall Loss 1.337207    Objective Loss 1.337207                                        LR 0.000006    Time 0.073995    
2022-12-29 10:32:47,745 - Epoch: [278][   30/  113]    Overall Loss 1.303074    Objective Loss 1.303074                                        LR 0.000006    Time 0.070580    
2022-12-29 10:32:48,385 - Epoch: [278][   40/  113]    Overall Loss 1.293281    Objective Loss 1.293281                                        LR 0.000006    Time 0.068923    
2022-12-29 10:32:49,023 - Epoch: [278][   50/  113]    Overall Loss 1.298346    Objective Loss 1.298346                                        LR 0.000006    Time 0.067891    
2022-12-29 10:32:49,658 - Epoch: [278][   60/  113]    Overall Loss 1.305399    Objective Loss 1.305399                                        LR 0.000006    Time 0.067154    
2022-12-29 10:32:50,302 - Epoch: [278][   70/  113]    Overall Loss 1.289060    Objective Loss 1.289060                                        LR 0.000006    Time 0.066757    
2022-12-29 10:32:50,937 - Epoch: [278][   80/  113]    Overall Loss 1.287029    Objective Loss 1.287029                                        LR 0.000006    Time 0.066344    
2022-12-29 10:32:51,578 - Epoch: [278][   90/  113]    Overall Loss 1.296023    Objective Loss 1.296023                                        LR 0.000006    Time 0.066083    
2022-12-29 10:32:52,212 - Epoch: [278][  100/  113]    Overall Loss 1.289081    Objective Loss 1.289081                                        LR 0.000006    Time 0.065812    
2022-12-29 10:32:52,846 - Epoch: [278][  110/  113]    Overall Loss 1.288248    Objective Loss 1.288248                                        LR 0.000006    Time 0.065591    
2022-12-29 10:32:53,020 - Epoch: [278][  113/  113]    Overall Loss 1.284877    Objective Loss 1.284877    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.065386    
2022-12-29 10:32:53,059 - --- validate (epoch=278)-----------
2022-12-29 10:32:53,059 - 200 samples (16 per mini-batch)
2022-12-29 10:32:53,613 - Epoch: [278][   10/   13]    Loss 1.294261    Top1 47.500000    Top5 98.125000    
2022-12-29 10:32:53,700 - Epoch: [278][   13/   13]    Loss 1.370987    Top1 45.500000    Top5 97.500000    
2022-12-29 10:32:53,755 - ==> Top1: 45.500    Top5: 97.500    Loss: 1.371

2022-12-29 10:32:53,756 - ==> Confusion:
[[19  3  0  6  4  0]
 [ 2 25  3  7  4  0]
 [ 0  9  8  7 10  0]
 [ 4  3  2 23  5  0]
 [ 3 10  4 13 15  0]
 [ 0  3  2  1  4  1]]

2022-12-29 10:32:53,758 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:32:53,759 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:32:53,768 - 

2022-12-29 10:32:53,769 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:32:54,613 - Epoch: [279][   10/  113]    Overall Loss 1.238572    Objective Loss 1.238572                                        LR 0.000006    Time 0.084280    
2022-12-29 10:32:55,244 - Epoch: [279][   20/  113]    Overall Loss 1.254202    Objective Loss 1.254202                                        LR 0.000006    Time 0.073697    
2022-12-29 10:32:55,874 - Epoch: [279][   30/  113]    Overall Loss 1.272157    Objective Loss 1.272157                                        LR 0.000006    Time 0.070114    
2022-12-29 10:32:56,501 - Epoch: [279][   40/  113]    Overall Loss 1.249437    Objective Loss 1.249437                                        LR 0.000006    Time 0.068242    
2022-12-29 10:32:57,127 - Epoch: [279][   50/  113]    Overall Loss 1.239404    Objective Loss 1.239404                                        LR 0.000006    Time 0.067108    
2022-12-29 10:32:57,757 - Epoch: [279][   60/  113]    Overall Loss 1.234212    Objective Loss 1.234212                                        LR 0.000006    Time 0.066404    
2022-12-29 10:32:58,386 - Epoch: [279][   70/  113]    Overall Loss 1.234346    Objective Loss 1.234346                                        LR 0.000006    Time 0.065904    
2022-12-29 10:32:59,010 - Epoch: [279][   80/  113]    Overall Loss 1.223116    Objective Loss 1.223116                                        LR 0.000006    Time 0.065459    
2022-12-29 10:32:59,640 - Epoch: [279][   90/  113]    Overall Loss 1.244451    Objective Loss 1.244451                                        LR 0.000006    Time 0.065183    
2022-12-29 10:33:00,265 - Epoch: [279][  100/  113]    Overall Loss 1.242072    Objective Loss 1.242072                                        LR 0.000006    Time 0.064905    
2022-12-29 10:33:00,894 - Epoch: [279][  110/  113]    Overall Loss 1.231345    Objective Loss 1.231345                                        LR 0.000006    Time 0.064715    
2022-12-29 10:33:01,064 - Epoch: [279][  113/  113]    Overall Loss 1.233005    Objective Loss 1.233005    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.064499    
2022-12-29 10:33:01,126 - --- validate (epoch=279)-----------
2022-12-29 10:33:01,127 - 200 samples (16 per mini-batch)
2022-12-29 10:33:01,675 - Epoch: [279][   10/   13]    Loss 1.208080    Top1 54.375000    Top5 97.500000    
2022-12-29 10:33:01,759 - Epoch: [279][   13/   13]    Loss 1.194134    Top1 54.500000    Top5 97.500000    
2022-12-29 10:33:01,805 - ==> Top1: 54.500    Top5: 97.500    Loss: 1.194

2022-12-29 10:33:01,805 - ==> Confusion:
[[19  0  1  4  3  0]
 [ 2 33  5  4  7  0]
 [ 0  4 15  4  8  0]
 [ 5  2  2 27  4  0]
 [ 2 12  1 10 13  0]
 [ 1  1  3  4  2  2]]

2022-12-29 10:33:01,807 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:01,808 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:01,826 - 

2022-12-29 10:33:01,827 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:02,673 - Epoch: [280][   10/  113]    Overall Loss 1.381882    Objective Loss 1.381882                                        LR 0.000006    Time 0.084589    
2022-12-29 10:33:03,308 - Epoch: [280][   20/  113]    Overall Loss 1.301102    Objective Loss 1.301102                                        LR 0.000006    Time 0.074019    
2022-12-29 10:33:03,947 - Epoch: [280][   30/  113]    Overall Loss 1.286374    Objective Loss 1.286374                                        LR 0.000006    Time 0.070627    
2022-12-29 10:33:04,582 - Epoch: [280][   40/  113]    Overall Loss 1.268939    Objective Loss 1.268939                                        LR 0.000006    Time 0.068834    
2022-12-29 10:33:05,219 - Epoch: [280][   50/  113]    Overall Loss 1.261047    Objective Loss 1.261047                                        LR 0.000006    Time 0.067779    
2022-12-29 10:33:05,860 - Epoch: [280][   60/  113]    Overall Loss 1.264044    Objective Loss 1.264044                                        LR 0.000006    Time 0.067168    
2022-12-29 10:33:06,493 - Epoch: [280][   70/  113]    Overall Loss 1.280221    Objective Loss 1.280221                                        LR 0.000006    Time 0.066612    
2022-12-29 10:33:07,122 - Epoch: [280][   80/  113]    Overall Loss 1.281297    Objective Loss 1.281297                                        LR 0.000006    Time 0.066142    
2022-12-29 10:33:07,759 - Epoch: [280][   90/  113]    Overall Loss 1.275861    Objective Loss 1.275861                                        LR 0.000006    Time 0.065865    
2022-12-29 10:33:08,393 - Epoch: [280][  100/  113]    Overall Loss 1.268087    Objective Loss 1.268087                                        LR 0.000006    Time 0.065615    
2022-12-29 10:33:09,026 - Epoch: [280][  110/  113]    Overall Loss 1.276472    Objective Loss 1.276472                                        LR 0.000006    Time 0.065390    
2022-12-29 10:33:09,196 - Epoch: [280][  113/  113]    Overall Loss 1.273002    Objective Loss 1.273002    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.065159    
2022-12-29 10:33:09,252 - --- validate (epoch=280)-----------
2022-12-29 10:33:09,252 - 200 samples (16 per mini-batch)
2022-12-29 10:33:09,803 - Epoch: [280][   10/   13]    Loss 1.280728    Top1 46.250000    Top5 99.375000    
2022-12-29 10:33:09,889 - Epoch: [280][   13/   13]    Loss 1.225795    Top1 50.500000    Top5 99.500000    
2022-12-29 10:33:09,948 - ==> Top1: 50.500    Top5: 99.500    Loss: 1.226

2022-12-29 10:33:09,949 - ==> Confusion:
[[17  3  2  3  1  0]
 [ 2 27  4  5  8  0]
 [ 2 12 11  7  7  1]
 [ 4  1  3 35  3  0]
 [ 0  7  6 10 11  0]
 [ 0  4  1  1  2  0]]

2022-12-29 10:33:09,951 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:09,951 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:09,960 - 

2022-12-29 10:33:09,960 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:10,798 - Epoch: [281][   10/  113]    Overall Loss 1.309703    Objective Loss 1.309703                                        LR 0.000006    Time 0.083701    
2022-12-29 10:33:11,432 - Epoch: [281][   20/  113]    Overall Loss 1.330899    Objective Loss 1.330899                                        LR 0.000006    Time 0.073519    
2022-12-29 10:33:12,072 - Epoch: [281][   30/  113]    Overall Loss 1.302552    Objective Loss 1.302552                                        LR 0.000006    Time 0.070332    
2022-12-29 10:33:12,709 - Epoch: [281][   40/  113]    Overall Loss 1.318926    Objective Loss 1.318926                                        LR 0.000006    Time 0.068656    
2022-12-29 10:33:13,339 - Epoch: [281][   50/  113]    Overall Loss 1.321653    Objective Loss 1.321653                                        LR 0.000006    Time 0.067503    
2022-12-29 10:33:13,969 - Epoch: [281][   60/  113]    Overall Loss 1.315939    Objective Loss 1.315939                                        LR 0.000006    Time 0.066743    
2022-12-29 10:33:14,597 - Epoch: [281][   70/  113]    Overall Loss 1.287504    Objective Loss 1.287504                                        LR 0.000006    Time 0.066183    
2022-12-29 10:33:15,225 - Epoch: [281][   80/  113]    Overall Loss 1.280734    Objective Loss 1.280734                                        LR 0.000006    Time 0.065753    
2022-12-29 10:33:15,856 - Epoch: [281][   90/  113]    Overall Loss 1.287617    Objective Loss 1.287617                                        LR 0.000006    Time 0.065454    
2022-12-29 10:33:16,490 - Epoch: [281][  100/  113]    Overall Loss 1.291261    Objective Loss 1.291261                                        LR 0.000006    Time 0.065242    
2022-12-29 10:33:17,116 - Epoch: [281][  110/  113]    Overall Loss 1.279905    Objective Loss 1.279905                                        LR 0.000006    Time 0.064995    
2022-12-29 10:33:17,286 - Epoch: [281][  113/  113]    Overall Loss 1.285722    Objective Loss 1.285722    Top1 33.333333    Top5 95.833333    LR 0.000006    Time 0.064771    
2022-12-29 10:33:17,351 - --- validate (epoch=281)-----------
2022-12-29 10:33:17,351 - 200 samples (16 per mini-batch)
2022-12-29 10:33:17,900 - Epoch: [281][   10/   13]    Loss 1.194384    Top1 51.875000    Top5 98.125000    
2022-12-29 10:33:17,984 - Epoch: [281][   13/   13]    Loss 1.201825    Top1 53.000000    Top5 98.500000    
2022-12-29 10:33:18,046 - ==> Top1: 53.000    Top5: 98.500    Loss: 1.202

2022-12-29 10:33:18,047 - ==> Confusion:
[[24  3  1  2  0  0]
 [ 4 28  4  5  8  0]
 [ 2 10  8  7 10  0]
 [ 3  4  0 30  4  0]
 [ 7  6  3  5 15  0]
 [ 0  3  0  3  0  1]]

2022-12-29 10:33:18,050 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:18,051 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:18,064 - 

2022-12-29 10:33:18,064 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:18,905 - Epoch: [282][   10/  113]    Overall Loss 1.265531    Objective Loss 1.265531                                        LR 0.000006    Time 0.084014    
2022-12-29 10:33:19,538 - Epoch: [282][   20/  113]    Overall Loss 1.215474    Objective Loss 1.215474                                        LR 0.000006    Time 0.073629    
2022-12-29 10:33:20,177 - Epoch: [282][   30/  113]    Overall Loss 1.192957    Objective Loss 1.192957                                        LR 0.000006    Time 0.070367    
2022-12-29 10:33:20,812 - Epoch: [282][   40/  113]    Overall Loss 1.202496    Objective Loss 1.202496                                        LR 0.000006    Time 0.068634    
2022-12-29 10:33:21,442 - Epoch: [282][   50/  113]    Overall Loss 1.213077    Objective Loss 1.213077                                        LR 0.000006    Time 0.067498    
2022-12-29 10:33:22,076 - Epoch: [282][   60/  113]    Overall Loss 1.226178    Objective Loss 1.226178                                        LR 0.000006    Time 0.066805    
2022-12-29 10:33:22,714 - Epoch: [282][   70/  113]    Overall Loss 1.233941    Objective Loss 1.233941                                        LR 0.000006    Time 0.066373    
2022-12-29 10:33:23,343 - Epoch: [282][   80/  113]    Overall Loss 1.233991    Objective Loss 1.233991                                        LR 0.000006    Time 0.065941    
2022-12-29 10:33:23,980 - Epoch: [282][   90/  113]    Overall Loss 1.234478    Objective Loss 1.234478                                        LR 0.000006    Time 0.065679    
2022-12-29 10:33:24,609 - Epoch: [282][  100/  113]    Overall Loss 1.243215    Objective Loss 1.243215                                        LR 0.000006    Time 0.065400    
2022-12-29 10:33:25,235 - Epoch: [282][  110/  113]    Overall Loss 1.256761    Objective Loss 1.256761                                        LR 0.000006    Time 0.065136    
2022-12-29 10:33:25,407 - Epoch: [282][  113/  113]    Overall Loss 1.253698    Objective Loss 1.253698    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064929    
2022-12-29 10:33:25,465 - --- validate (epoch=282)-----------
2022-12-29 10:33:25,466 - 200 samples (16 per mini-batch)
2022-12-29 10:33:26,012 - Epoch: [282][   10/   13]    Loss 1.317179    Top1 48.750000    Top5 96.875000    
2022-12-29 10:33:26,097 - Epoch: [282][   13/   13]    Loss 1.290033    Top1 49.500000    Top5 97.000000    
2022-12-29 10:33:26,161 - ==> Top1: 49.500    Top5: 97.000    Loss: 1.290

2022-12-29 10:33:26,161 - ==> Confusion:
[[17  4  0  8  1  0]
 [ 2 22  1  7  8  1]
 [ 4  7  8  5 10  0]
 [ 3  5  1 30  7  0]
 [ 3  9  2  2 21  0]
 [ 3  1  1  1  5  1]]

2022-12-29 10:33:26,165 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:26,165 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:26,177 - 

2022-12-29 10:33:26,177 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:27,021 - Epoch: [283][   10/  113]    Overall Loss 1.270387    Objective Loss 1.270387                                        LR 0.000006    Time 0.084296    
2022-12-29 10:33:27,657 - Epoch: [283][   20/  113]    Overall Loss 1.263195    Objective Loss 1.263195                                        LR 0.000006    Time 0.073906    
2022-12-29 10:33:28,294 - Epoch: [283][   30/  113]    Overall Loss 1.275755    Objective Loss 1.275755                                        LR 0.000006    Time 0.070485    
2022-12-29 10:33:28,927 - Epoch: [283][   40/  113]    Overall Loss 1.233596    Objective Loss 1.233596                                        LR 0.000006    Time 0.068685    
2022-12-29 10:33:29,559 - Epoch: [283][   50/  113]    Overall Loss 1.251286    Objective Loss 1.251286                                        LR 0.000006    Time 0.067588    
2022-12-29 10:33:30,192 - Epoch: [283][   60/  113]    Overall Loss 1.240925    Objective Loss 1.240925                                        LR 0.000006    Time 0.066854    
2022-12-29 10:33:30,825 - Epoch: [283][   70/  113]    Overall Loss 1.260242    Objective Loss 1.260242                                        LR 0.000006    Time 0.066351    
2022-12-29 10:33:31,459 - Epoch: [283][   80/  113]    Overall Loss 1.249909    Objective Loss 1.249909                                        LR 0.000006    Time 0.065966    
2022-12-29 10:33:32,095 - Epoch: [283][   90/  113]    Overall Loss 1.240468    Objective Loss 1.240468                                        LR 0.000006    Time 0.065702    
2022-12-29 10:33:32,734 - Epoch: [283][  100/  113]    Overall Loss 1.255829    Objective Loss 1.255829                                        LR 0.000006    Time 0.065518    
2022-12-29 10:33:33,358 - Epoch: [283][  110/  113]    Overall Loss 1.261349    Objective Loss 1.261349                                        LR 0.000006    Time 0.065233    
2022-12-29 10:33:33,529 - Epoch: [283][  113/  113]    Overall Loss 1.259731    Objective Loss 1.259731    Top1 62.500000    Top5 95.833333    LR 0.000006    Time 0.065012    
2022-12-29 10:33:33,591 - --- validate (epoch=283)-----------
2022-12-29 10:33:33,591 - 200 samples (16 per mini-batch)
2022-12-29 10:33:34,138 - Epoch: [283][   10/   13]    Loss 1.236209    Top1 51.875000    Top5 97.500000    
2022-12-29 10:33:34,226 - Epoch: [283][   13/   13]    Loss 1.258406    Top1 52.000000    Top5 96.500000    
2022-12-29 10:33:34,284 - ==> Top1: 52.000    Top5: 96.500    Loss: 1.258

2022-12-29 10:33:34,284 - ==> Confusion:
[[20  5  0  4  1  0]
 [ 0 27  3  8  5  0]
 [ 2  8  9  4  9  0]
 [ 4 11  3 35  2  0]
 [ 2  8  4  7 11  0]
 [ 0  3  1  1  1  2]]

2022-12-29 10:33:34,287 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:34,288 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:34,299 - 

2022-12-29 10:33:34,300 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:35,130 - Epoch: [284][   10/  113]    Overall Loss 1.292019    Objective Loss 1.292019                                        LR 0.000006    Time 0.082970    
2022-12-29 10:33:35,764 - Epoch: [284][   20/  113]    Overall Loss 1.272425    Objective Loss 1.272425                                        LR 0.000006    Time 0.073127    
2022-12-29 10:33:36,400 - Epoch: [284][   30/  113]    Overall Loss 1.248910    Objective Loss 1.248910                                        LR 0.000006    Time 0.069932    
2022-12-29 10:33:37,032 - Epoch: [284][   40/  113]    Overall Loss 1.216632    Objective Loss 1.216632                                        LR 0.000006    Time 0.068239    
2022-12-29 10:33:37,664 - Epoch: [284][   50/  113]    Overall Loss 1.216588    Objective Loss 1.216588                                        LR 0.000006    Time 0.067223    
2022-12-29 10:33:38,300 - Epoch: [284][   60/  113]    Overall Loss 1.231951    Objective Loss 1.231951                                        LR 0.000006    Time 0.066616    
2022-12-29 10:33:38,937 - Epoch: [284][   70/  113]    Overall Loss 1.227309    Objective Loss 1.227309                                        LR 0.000006    Time 0.066190    
2022-12-29 10:33:39,570 - Epoch: [284][   80/  113]    Overall Loss 1.212976    Objective Loss 1.212976                                        LR 0.000006    Time 0.065830    
2022-12-29 10:33:40,210 - Epoch: [284][   90/  113]    Overall Loss 1.211025    Objective Loss 1.211025                                        LR 0.000006    Time 0.065615    
2022-12-29 10:33:40,838 - Epoch: [284][  100/  113]    Overall Loss 1.215957    Objective Loss 1.215957                                        LR 0.000006    Time 0.065336    
2022-12-29 10:33:41,463 - Epoch: [284][  110/  113]    Overall Loss 1.233533    Objective Loss 1.233533                                        LR 0.000006    Time 0.065069    
2022-12-29 10:33:41,640 - Epoch: [284][  113/  113]    Overall Loss 1.234480    Objective Loss 1.234480    Top1 58.333333    Top5 91.666667    LR 0.000006    Time 0.064908    
2022-12-29 10:33:41,695 - --- validate (epoch=284)-----------
2022-12-29 10:33:41,696 - 200 samples (16 per mini-batch)
2022-12-29 10:33:42,251 - Epoch: [284][   10/   13]    Loss 1.230896    Top1 51.250000    Top5 98.750000    
2022-12-29 10:33:42,337 - Epoch: [284][   13/   13]    Loss 1.212113    Top1 51.000000    Top5 99.000000    
2022-12-29 10:33:42,398 - ==> Top1: 51.000    Top5: 99.000    Loss: 1.212

2022-12-29 10:33:42,399 - ==> Confusion:
[[19  3  0  7  0  0]
 [ 4 20  3  7  5  0]
 [ 2  5 12  4  7  1]
 [ 7  3  2 35  4  0]
 [ 5  5  6 11 16  0]
 [ 2  2  2  0  1  0]]

2022-12-29 10:33:42,402 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:42,402 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:42,422 - 

2022-12-29 10:33:42,423 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:43,267 - Epoch: [285][   10/  113]    Overall Loss 1.282823    Objective Loss 1.282823                                        LR 0.000006    Time 0.084351    
2022-12-29 10:33:43,903 - Epoch: [285][   20/  113]    Overall Loss 1.242021    Objective Loss 1.242021                                        LR 0.000006    Time 0.073954    
2022-12-29 10:33:44,537 - Epoch: [285][   30/  113]    Overall Loss 1.262952    Objective Loss 1.262952                                        LR 0.000006    Time 0.070417    
2022-12-29 10:33:45,165 - Epoch: [285][   40/  113]    Overall Loss 1.238026    Objective Loss 1.238026                                        LR 0.000006    Time 0.068493    
2022-12-29 10:33:45,792 - Epoch: [285][   50/  113]    Overall Loss 1.249369    Objective Loss 1.249369                                        LR 0.000006    Time 0.067340    
2022-12-29 10:33:46,432 - Epoch: [285][   60/  113]    Overall Loss 1.249680    Objective Loss 1.249680                                        LR 0.000006    Time 0.066764    
2022-12-29 10:33:47,065 - Epoch: [285][   70/  113]    Overall Loss 1.248867    Objective Loss 1.248867                                        LR 0.000006    Time 0.066266    
2022-12-29 10:33:47,699 - Epoch: [285][   80/  113]    Overall Loss 1.246283    Objective Loss 1.246283                                        LR 0.000006    Time 0.065903    
2022-12-29 10:33:48,334 - Epoch: [285][   90/  113]    Overall Loss 1.252983    Objective Loss 1.252983                                        LR 0.000006    Time 0.065626    
2022-12-29 10:33:48,965 - Epoch: [285][  100/  113]    Overall Loss 1.254145    Objective Loss 1.254145                                        LR 0.000006    Time 0.065372    
2022-12-29 10:33:49,594 - Epoch: [285][  110/  113]    Overall Loss 1.265021    Objective Loss 1.265021                                        LR 0.000006    Time 0.065145    
2022-12-29 10:33:49,765 - Epoch: [285][  113/  113]    Overall Loss 1.264674    Objective Loss 1.264674    Top1 58.333333    Top5 91.666667    LR 0.000006    Time 0.064925    
2022-12-29 10:33:49,817 - --- validate (epoch=285)-----------
2022-12-29 10:33:49,817 - 200 samples (16 per mini-batch)
2022-12-29 10:33:50,376 - Epoch: [285][   10/   13]    Loss 1.342767    Top1 47.500000    Top5 95.625000    
2022-12-29 10:33:50,464 - Epoch: [285][   13/   13]    Loss 1.345058    Top1 48.000000    Top5 96.500000    
2022-12-29 10:33:50,524 - ==> Top1: 48.000    Top5: 96.500    Loss: 1.345

2022-12-29 10:33:50,525 - ==> Confusion:
[[13  2  0  4  2  1]
 [ 5 15  5  9 12  0]
 [ 1  5 10  7  3  0]
 [ 5  3  2 45  4  1]
 [ 1  7  5  8 10  1]
 [ 2  1  2  1  5  3]]

2022-12-29 10:33:50,526 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:50,527 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:50,544 - 

2022-12-29 10:33:50,544 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:51,378 - Epoch: [286][   10/  113]    Overall Loss 1.372960    Objective Loss 1.372960                                        LR 0.000006    Time 0.083285    
2022-12-29 10:33:52,013 - Epoch: [286][   20/  113]    Overall Loss 1.360907    Objective Loss 1.360907                                        LR 0.000006    Time 0.073373    
2022-12-29 10:33:52,653 - Epoch: [286][   30/  113]    Overall Loss 1.321810    Objective Loss 1.321810                                        LR 0.000006    Time 0.070239    
2022-12-29 10:33:53,286 - Epoch: [286][   40/  113]    Overall Loss 1.294942    Objective Loss 1.294942                                        LR 0.000006    Time 0.068502    
2022-12-29 10:33:53,917 - Epoch: [286][   50/  113]    Overall Loss 1.296194    Objective Loss 1.296194                                        LR 0.000006    Time 0.067406    
2022-12-29 10:33:54,554 - Epoch: [286][   60/  113]    Overall Loss 1.276756    Objective Loss 1.276756                                        LR 0.000006    Time 0.066779    
2022-12-29 10:33:55,187 - Epoch: [286][   70/  113]    Overall Loss 1.275574    Objective Loss 1.275574                                        LR 0.000006    Time 0.066276    
2022-12-29 10:33:55,819 - Epoch: [286][   80/  113]    Overall Loss 1.264922    Objective Loss 1.264922                                        LR 0.000006    Time 0.065883    
2022-12-29 10:33:56,455 - Epoch: [286][   90/  113]    Overall Loss 1.261108    Objective Loss 1.261108                                        LR 0.000006    Time 0.065634    
2022-12-29 10:33:57,093 - Epoch: [286][  100/  113]    Overall Loss 1.263091    Objective Loss 1.263091                                        LR 0.000006    Time 0.065446    
2022-12-29 10:33:57,718 - Epoch: [286][  110/  113]    Overall Loss 1.263036    Objective Loss 1.263036                                        LR 0.000006    Time 0.065173    
2022-12-29 10:33:57,891 - Epoch: [286][  113/  113]    Overall Loss 1.263767    Objective Loss 1.263767    Top1 66.666667    Top5 95.833333    LR 0.000006    Time 0.064970    
2022-12-29 10:33:57,941 - --- validate (epoch=286)-----------
2022-12-29 10:33:57,941 - 200 samples (16 per mini-batch)
2022-12-29 10:33:58,487 - Epoch: [286][   10/   13]    Loss 1.279000    Top1 57.500000    Top5 97.500000    
2022-12-29 10:33:58,575 - Epoch: [286][   13/   13]    Loss 1.305505    Top1 55.500000    Top5 98.000000    
2022-12-29 10:33:58,636 - ==> Top1: 55.500    Top5: 98.000    Loss: 1.306

2022-12-29 10:33:58,636 - ==> Confusion:
[[22  3  1  3  0  0]
 [ 1 21  2  4  6  0]
 [ 3  6 17  7  7  0]
 [ 6  5  5 32  5  0]
 [ 5  7  5  2 14  0]
 [ 1  1  1  2  1  5]]

2022-12-29 10:33:58,639 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:33:58,639 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:33:58,663 - 

2022-12-29 10:33:58,664 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:33:59,493 - Epoch: [287][   10/  113]    Overall Loss 1.354302    Objective Loss 1.354302                                        LR 0.000006    Time 0.082795    
2022-12-29 10:34:00,129 - Epoch: [287][   20/  113]    Overall Loss 1.294084    Objective Loss 1.294084                                        LR 0.000006    Time 0.073198    
2022-12-29 10:34:00,767 - Epoch: [287][   30/  113]    Overall Loss 1.282294    Objective Loss 1.282294                                        LR 0.000006    Time 0.070050    
2022-12-29 10:34:01,403 - Epoch: [287][   40/  113]    Overall Loss 1.277044    Objective Loss 1.277044                                        LR 0.000006    Time 0.068426    
2022-12-29 10:34:02,035 - Epoch: [287][   50/  113]    Overall Loss 1.264301    Objective Loss 1.264301                                        LR 0.000006    Time 0.067355    
2022-12-29 10:34:02,672 - Epoch: [287][   60/  113]    Overall Loss 1.251241    Objective Loss 1.251241                                        LR 0.000006    Time 0.066749    
2022-12-29 10:34:03,304 - Epoch: [287][   70/  113]    Overall Loss 1.253912    Objective Loss 1.253912                                        LR 0.000006    Time 0.066232    
2022-12-29 10:34:03,934 - Epoch: [287][   80/  113]    Overall Loss 1.250551    Objective Loss 1.250551                                        LR 0.000006    Time 0.065825    
2022-12-29 10:34:04,570 - Epoch: [287][   90/  113]    Overall Loss 1.255170    Objective Loss 1.255170                                        LR 0.000006    Time 0.065571    
2022-12-29 10:34:05,203 - Epoch: [287][  100/  113]    Overall Loss 1.256729    Objective Loss 1.256729                                        LR 0.000006    Time 0.065336    
2022-12-29 10:34:05,834 - Epoch: [287][  110/  113]    Overall Loss 1.259236    Objective Loss 1.259236                                        LR 0.000006    Time 0.065129    
2022-12-29 10:34:06,005 - Epoch: [287][  113/  113]    Overall Loss 1.267844    Objective Loss 1.267844    Top1 33.333333    Top5 95.833333    LR 0.000006    Time 0.064913    
2022-12-29 10:34:06,067 - --- validate (epoch=287)-----------
2022-12-29 10:34:06,067 - 200 samples (16 per mini-batch)
2022-12-29 10:34:06,615 - Epoch: [287][   10/   13]    Loss 1.285026    Top1 49.375000    Top5 97.500000    
2022-12-29 10:34:06,699 - Epoch: [287][   13/   13]    Loss 1.302093    Top1 49.500000    Top5 98.000000    
2022-12-29 10:34:06,750 - ==> Top1: 49.500    Top5: 98.000    Loss: 1.302

2022-12-29 10:34:06,750 - ==> Confusion:
[[22  3  0  5  6  0]
 [ 2 19  5  1 10  0]
 [ 0 16 11  4  6  0]
 [ 5  2  3 30  7  0]
 [ 2  7  4  5 15  0]
 [ 0  1  0  3  4  2]]

2022-12-29 10:34:06,755 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:06,755 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:06,777 - 

2022-12-29 10:34:06,777 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:07,605 - Epoch: [288][   10/  113]    Overall Loss 1.314521    Objective Loss 1.314521                                        LR 0.000006    Time 0.082663    
2022-12-29 10:34:08,237 - Epoch: [288][   20/  113]    Overall Loss 1.237966    Objective Loss 1.237966                                        LR 0.000006    Time 0.072917    
2022-12-29 10:34:08,876 - Epoch: [288][   30/  113]    Overall Loss 1.248499    Objective Loss 1.248499                                        LR 0.000006    Time 0.069894    
2022-12-29 10:34:09,510 - Epoch: [288][   40/  113]    Overall Loss 1.231155    Objective Loss 1.231155                                        LR 0.000006    Time 0.068256    
2022-12-29 10:34:10,139 - Epoch: [288][   50/  113]    Overall Loss 1.221831    Objective Loss 1.221831                                        LR 0.000006    Time 0.067176    
2022-12-29 10:34:10,776 - Epoch: [288][   60/  113]    Overall Loss 1.225551    Objective Loss 1.225551                                        LR 0.000006    Time 0.066584    
2022-12-29 10:34:11,408 - Epoch: [288][   70/  113]    Overall Loss 1.259661    Objective Loss 1.259661                                        LR 0.000006    Time 0.066085    
2022-12-29 10:34:12,040 - Epoch: [288][   80/  113]    Overall Loss 1.253022    Objective Loss 1.253022                                        LR 0.000006    Time 0.065720    
2022-12-29 10:34:12,673 - Epoch: [288][   90/  113]    Overall Loss 1.262683    Objective Loss 1.262683                                        LR 0.000006    Time 0.065451    
2022-12-29 10:34:13,302 - Epoch: [288][  100/  113]    Overall Loss 1.261081    Objective Loss 1.261081                                        LR 0.000006    Time 0.065196    
2022-12-29 10:34:13,930 - Epoch: [288][  110/  113]    Overall Loss 1.262941    Objective Loss 1.262941                                        LR 0.000006    Time 0.064968    
2022-12-29 10:34:14,105 - Epoch: [288][  113/  113]    Overall Loss 1.264611    Objective Loss 1.264611    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064788    
2022-12-29 10:34:14,155 - --- validate (epoch=288)-----------
2022-12-29 10:34:14,156 - 200 samples (16 per mini-batch)
2022-12-29 10:34:14,696 - Epoch: [288][   10/   13]    Loss 1.326365    Top1 49.375000    Top5 98.125000    
2022-12-29 10:34:14,780 - Epoch: [288][   13/   13]    Loss 1.246588    Top1 52.000000    Top5 97.500000    
2022-12-29 10:34:14,834 - ==> Top1: 52.000    Top5: 97.500    Loss: 1.247

2022-12-29 10:34:14,834 - ==> Confusion:
[[16  2  2  4  2  0]
 [ 2 29  4  3  5  0]
 [ 0 12  7  5  4  0]
 [ 2  7  0 37  4  0]
 [ 1 17  0 11 14  2]
 [ 2  2  1  2  0  1]]

2022-12-29 10:34:14,837 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:14,838 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:14,861 - 

2022-12-29 10:34:14,862 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:15,687 - Epoch: [289][   10/  113]    Overall Loss 1.152551    Objective Loss 1.152551                                        LR 0.000006    Time 0.082439    
2022-12-29 10:34:16,320 - Epoch: [289][   20/  113]    Overall Loss 1.168907    Objective Loss 1.168907                                        LR 0.000006    Time 0.072839    
2022-12-29 10:34:16,952 - Epoch: [289][   30/  113]    Overall Loss 1.170782    Objective Loss 1.170782                                        LR 0.000006    Time 0.069612    
2022-12-29 10:34:17,577 - Epoch: [289][   40/  113]    Overall Loss 1.191284    Objective Loss 1.191284                                        LR 0.000006    Time 0.067832    
2022-12-29 10:34:18,204 - Epoch: [289][   50/  113]    Overall Loss 1.217100    Objective Loss 1.217100                                        LR 0.000006    Time 0.066787    
2022-12-29 10:34:18,834 - Epoch: [289][   60/  113]    Overall Loss 1.230634    Objective Loss 1.230634                                        LR 0.000006    Time 0.066152    
2022-12-29 10:34:19,463 - Epoch: [289][   70/  113]    Overall Loss 1.225905    Objective Loss 1.225905                                        LR 0.000006    Time 0.065673    
2022-12-29 10:34:20,089 - Epoch: [289][   80/  113]    Overall Loss 1.226108    Objective Loss 1.226108                                        LR 0.000006    Time 0.065287    
2022-12-29 10:34:20,722 - Epoch: [289][   90/  113]    Overall Loss 1.240909    Objective Loss 1.240909                                        LR 0.000006    Time 0.065061    
2022-12-29 10:34:21,357 - Epoch: [289][  100/  113]    Overall Loss 1.228999    Objective Loss 1.228999                                        LR 0.000006    Time 0.064896    
2022-12-29 10:34:21,984 - Epoch: [289][  110/  113]    Overall Loss 1.236637    Objective Loss 1.236637                                        LR 0.000006    Time 0.064690    
2022-12-29 10:34:22,154 - Epoch: [289][  113/  113]    Overall Loss 1.235690    Objective Loss 1.235690    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064481    
2022-12-29 10:34:22,209 - --- validate (epoch=289)-----------
2022-12-29 10:34:22,209 - 200 samples (16 per mini-batch)
2022-12-29 10:34:22,765 - Epoch: [289][   10/   13]    Loss 1.369031    Top1 42.500000    Top5 98.125000    
2022-12-29 10:34:22,850 - Epoch: [289][   13/   13]    Loss 1.331690    Top1 43.000000    Top5 98.000000    
2022-12-29 10:34:22,903 - ==> Top1: 43.000    Top5: 98.000    Loss: 1.332

2022-12-29 10:34:22,904 - ==> Confusion:
[[18  2  0  4  0  0]
 [ 6 18  4  4  7  0]
 [ 1 12 12  5  7  0]
 [ 5  6  3 24  5  0]
 [ 1 12  9  8 14  0]
 [ 0  6  1  4  2  0]]

2022-12-29 10:34:22,906 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:22,906 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:22,915 - 

2022-12-29 10:34:22,916 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:23,739 - Epoch: [290][   10/  113]    Overall Loss 1.279539    Objective Loss 1.279539                                        LR 0.000006    Time 0.082190    
2022-12-29 10:34:24,377 - Epoch: [290][   20/  113]    Overall Loss 1.304085    Objective Loss 1.304085                                        LR 0.000006    Time 0.072987    
2022-12-29 10:34:25,017 - Epoch: [290][   30/  113]    Overall Loss 1.285855    Objective Loss 1.285855                                        LR 0.000006    Time 0.069989    
2022-12-29 10:34:25,657 - Epoch: [290][   40/  113]    Overall Loss 1.289916    Objective Loss 1.289916                                        LR 0.000006    Time 0.068483    
2022-12-29 10:34:26,285 - Epoch: [290][   50/  113]    Overall Loss 1.282336    Objective Loss 1.282336                                        LR 0.000006    Time 0.067330    
2022-12-29 10:34:26,916 - Epoch: [290][   60/  113]    Overall Loss 1.278091    Objective Loss 1.278091                                        LR 0.000006    Time 0.066614    
2022-12-29 10:34:27,547 - Epoch: [290][   70/  113]    Overall Loss 1.276248    Objective Loss 1.276248                                        LR 0.000006    Time 0.066115    
2022-12-29 10:34:28,177 - Epoch: [290][   80/  113]    Overall Loss 1.274774    Objective Loss 1.274774                                        LR 0.000006    Time 0.065707    
2022-12-29 10:34:28,812 - Epoch: [290][   90/  113]    Overall Loss 1.278825    Objective Loss 1.278825                                        LR 0.000006    Time 0.065457    
2022-12-29 10:34:29,444 - Epoch: [290][  100/  113]    Overall Loss 1.281654    Objective Loss 1.281654                                        LR 0.000006    Time 0.065233    
2022-12-29 10:34:30,065 - Epoch: [290][  110/  113]    Overall Loss 1.290317    Objective Loss 1.290317                                        LR 0.000006    Time 0.064940    
2022-12-29 10:34:30,240 - Epoch: [290][  113/  113]    Overall Loss 1.285545    Objective Loss 1.285545    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064765    
2022-12-29 10:34:30,287 - --- validate (epoch=290)-----------
2022-12-29 10:34:30,288 - 200 samples (16 per mini-batch)
2022-12-29 10:34:30,837 - Epoch: [290][   10/   13]    Loss 1.259838    Top1 48.750000    Top5 97.500000    
2022-12-29 10:34:30,922 - Epoch: [290][   13/   13]    Loss 1.265717    Top1 48.000000    Top5 97.500000    
2022-12-29 10:34:30,976 - ==> Top1: 48.000    Top5: 97.500    Loss: 1.266

2022-12-29 10:34:30,976 - ==> Confusion:
[[21  3  1 10  2  0]
 [ 2 23  3  3  7  2]
 [ 1  7  8  3  8  0]
 [ 7  5  2 38  4  0]
 [ 1 15  4  3  5  0]
 [ 2  3  2  2  2  1]]

2022-12-29 10:34:30,978 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:30,979 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:31,001 - 

2022-12-29 10:34:31,002 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:31,826 - Epoch: [291][   10/  113]    Overall Loss 1.289044    Objective Loss 1.289044                                        LR 0.000006    Time 0.082204    
2022-12-29 10:34:32,455 - Epoch: [291][   20/  113]    Overall Loss 1.251914    Objective Loss 1.251914                                        LR 0.000006    Time 0.072529    
2022-12-29 10:34:33,091 - Epoch: [291][   30/  113]    Overall Loss 1.263896    Objective Loss 1.263896                                        LR 0.000006    Time 0.069538    
2022-12-29 10:34:33,719 - Epoch: [291][   40/  113]    Overall Loss 1.249129    Objective Loss 1.249129                                        LR 0.000006    Time 0.067848    
2022-12-29 10:34:34,344 - Epoch: [291][   50/  113]    Overall Loss 1.242617    Objective Loss 1.242617                                        LR 0.000006    Time 0.066780    
2022-12-29 10:34:34,974 - Epoch: [291][   60/  113]    Overall Loss 1.244793    Objective Loss 1.244793                                        LR 0.000006    Time 0.066135    
2022-12-29 10:34:35,602 - Epoch: [291][   70/  113]    Overall Loss 1.246282    Objective Loss 1.246282                                        LR 0.000006    Time 0.065650    
2022-12-29 10:34:36,229 - Epoch: [291][   80/  113]    Overall Loss 1.241043    Objective Loss 1.241043                                        LR 0.000006    Time 0.065278    
2022-12-29 10:34:36,862 - Epoch: [291][   90/  113]    Overall Loss 1.246981    Objective Loss 1.246981                                        LR 0.000006    Time 0.065051    
2022-12-29 10:34:37,491 - Epoch: [291][  100/  113]    Overall Loss 1.246111    Objective Loss 1.246111                                        LR 0.000006    Time 0.064834    
2022-12-29 10:34:38,114 - Epoch: [291][  110/  113]    Overall Loss 1.248199    Objective Loss 1.248199                                        LR 0.000006    Time 0.064599    
2022-12-29 10:34:38,290 - Epoch: [291][  113/  113]    Overall Loss 1.255080    Objective Loss 1.255080    Top1 33.333333    Top5 100.000000    LR 0.000006    Time 0.064438    
2022-12-29 10:34:38,335 - --- validate (epoch=291)-----------
2022-12-29 10:34:38,335 - 200 samples (16 per mini-batch)
2022-12-29 10:34:38,893 - Epoch: [291][   10/   13]    Loss 1.234777    Top1 48.750000    Top5 98.750000    
2022-12-29 10:34:38,977 - Epoch: [291][   13/   13]    Loss 1.239112    Top1 49.500000    Top5 98.500000    
2022-12-29 10:34:39,030 - ==> Top1: 49.500    Top5: 98.500    Loss: 1.239

2022-12-29 10:34:39,030 - ==> Confusion:
[[16  2  0  7  1  0]
 [ 1 18  1 10  5  0]
 [ 2 11 12  5  9  0]
 [ 7  2  1 36  5  0]
 [ 6  8  2 10 16  0]
 [ 2  2  1  1  0  1]]

2022-12-29 10:34:39,033 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:39,033 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:39,043 - 

2022-12-29 10:34:39,043 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:39,868 - Epoch: [292][   10/  113]    Overall Loss 1.296569    Objective Loss 1.296569                                        LR 0.000006    Time 0.082374    
2022-12-29 10:34:40,503 - Epoch: [292][   20/  113]    Overall Loss 1.247143    Objective Loss 1.247143                                        LR 0.000006    Time 0.072920    
2022-12-29 10:34:41,140 - Epoch: [292][   30/  113]    Overall Loss 1.256997    Objective Loss 1.256997                                        LR 0.000006    Time 0.069817    
2022-12-29 10:34:41,776 - Epoch: [292][   40/  113]    Overall Loss 1.290340    Objective Loss 1.290340                                        LR 0.000006    Time 0.068273    
2022-12-29 10:34:42,405 - Epoch: [292][   50/  113]    Overall Loss 1.283574    Objective Loss 1.283574                                        LR 0.000006    Time 0.067180    
2022-12-29 10:34:43,041 - Epoch: [292][   60/  113]    Overall Loss 1.271911    Objective Loss 1.271911                                        LR 0.000006    Time 0.066570    
2022-12-29 10:34:43,669 - Epoch: [292][   70/  113]    Overall Loss 1.276438    Objective Loss 1.276438                                        LR 0.000006    Time 0.066029    
2022-12-29 10:34:44,296 - Epoch: [292][   80/  113]    Overall Loss 1.279398    Objective Loss 1.279398                                        LR 0.000006    Time 0.065602    
2022-12-29 10:34:44,925 - Epoch: [292][   90/  113]    Overall Loss 1.263567    Objective Loss 1.263567                                        LR 0.000006    Time 0.065265    
2022-12-29 10:34:45,551 - Epoch: [292][  100/  113]    Overall Loss 1.278104    Objective Loss 1.278104                                        LR 0.000006    Time 0.064996    
2022-12-29 10:34:46,179 - Epoch: [292][  110/  113]    Overall Loss 1.291488    Objective Loss 1.291488                                        LR 0.000006    Time 0.064789    
2022-12-29 10:34:46,349 - Epoch: [292][  113/  113]    Overall Loss 1.294473    Objective Loss 1.294473    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064567    
2022-12-29 10:34:46,406 - --- validate (epoch=292)-----------
2022-12-29 10:34:46,406 - 200 samples (16 per mini-batch)
2022-12-29 10:34:46,954 - Epoch: [292][   10/   13]    Loss 1.277666    Top1 48.750000    Top5 95.625000    
2022-12-29 10:34:47,039 - Epoch: [292][   13/   13]    Loss 1.299982    Top1 48.500000    Top5 96.000000    
2022-12-29 10:34:47,094 - ==> Top1: 48.500    Top5: 96.000    Loss: 1.300

2022-12-29 10:34:47,095 - ==> Confusion:
[[14  0  1  5  6  0]
 [ 5 14  6  7  7  2]
 [ 1  7  9  6 10  0]
 [ 3  3  3 41  3  0]
 [ 2  5  3  8 18  0]
 [ 1  2  1  0  6  1]]

2022-12-29 10:34:47,099 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:47,099 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:47,119 - 

2022-12-29 10:34:47,119 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:47,953 - Epoch: [293][   10/  113]    Overall Loss 1.282403    Objective Loss 1.282403                                        LR 0.000006    Time 0.083169    
2022-12-29 10:34:48,587 - Epoch: [293][   20/  113]    Overall Loss 1.276578    Objective Loss 1.276578                                        LR 0.000006    Time 0.073273    
2022-12-29 10:34:49,220 - Epoch: [293][   30/  113]    Overall Loss 1.273384    Objective Loss 1.273384                                        LR 0.000006    Time 0.069920    
2022-12-29 10:34:49,850 - Epoch: [293][   40/  113]    Overall Loss 1.243296    Objective Loss 1.243296                                        LR 0.000006    Time 0.068179    
2022-12-29 10:34:50,482 - Epoch: [293][   50/  113]    Overall Loss 1.228086    Objective Loss 1.228086                                        LR 0.000006    Time 0.067167    
2022-12-29 10:34:51,112 - Epoch: [293][   60/  113]    Overall Loss 1.226139    Objective Loss 1.226139                                        LR 0.000006    Time 0.066475    
2022-12-29 10:34:51,744 - Epoch: [293][   70/  113]    Overall Loss 1.232674    Objective Loss 1.232674                                        LR 0.000006    Time 0.065990    
2022-12-29 10:34:52,370 - Epoch: [293][   80/  113]    Overall Loss 1.240502    Objective Loss 1.240502                                        LR 0.000006    Time 0.065564    
2022-12-29 10:34:53,007 - Epoch: [293][   90/  113]    Overall Loss 1.246944    Objective Loss 1.246944                                        LR 0.000006    Time 0.065357    
2022-12-29 10:34:53,636 - Epoch: [293][  100/  113]    Overall Loss 1.250324    Objective Loss 1.250324                                        LR 0.000006    Time 0.065100    
2022-12-29 10:34:54,258 - Epoch: [293][  110/  113]    Overall Loss 1.242117    Objective Loss 1.242117                                        LR 0.000006    Time 0.064838    
2022-12-29 10:34:54,431 - Epoch: [293][  113/  113]    Overall Loss 1.246470    Objective Loss 1.246470    Top1 37.500000    Top5 91.666667    LR 0.000006    Time 0.064643    
2022-12-29 10:34:54,484 - --- validate (epoch=293)-----------
2022-12-29 10:34:54,485 - 200 samples (16 per mini-batch)
2022-12-29 10:34:55,039 - Epoch: [293][   10/   13]    Loss 1.182278    Top1 55.625000    Top5 98.125000    
2022-12-29 10:34:55,124 - Epoch: [293][   13/   13]    Loss 1.260788    Top1 52.000000    Top5 98.500000    
2022-12-29 10:34:55,185 - ==> Top1: 52.000    Top5: 98.500    Loss: 1.261

2022-12-29 10:34:55,186 - ==> Confusion:
[[19  3  0  5  2  0]
 [ 0 16  2 10  3  0]
 [ 1  8 16  5  8  0]
 [ 3  6  5 37  2  0]
 [ 4  9  6  4 13  0]
 [ 0  5  1  1  3  3]]

2022-12-29 10:34:55,189 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:34:55,189 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:34:55,203 - 

2022-12-29 10:34:55,204 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:34:56,042 - Epoch: [294][   10/  113]    Overall Loss 1.225403    Objective Loss 1.225403                                        LR 0.000006    Time 0.083767    
2022-12-29 10:34:56,679 - Epoch: [294][   20/  113]    Overall Loss 1.258579    Objective Loss 1.258579                                        LR 0.000006    Time 0.073714    
2022-12-29 10:34:57,315 - Epoch: [294][   30/  113]    Overall Loss 1.265242    Objective Loss 1.265242                                        LR 0.000006    Time 0.070319    
2022-12-29 10:34:57,953 - Epoch: [294][   40/  113]    Overall Loss 1.261988    Objective Loss 1.261988                                        LR 0.000006    Time 0.068676    
2022-12-29 10:34:58,581 - Epoch: [294][   50/  113]    Overall Loss 1.246169    Objective Loss 1.246169                                        LR 0.000006    Time 0.067493    
2022-12-29 10:34:59,210 - Epoch: [294][   60/  113]    Overall Loss 1.241000    Objective Loss 1.241000                                        LR 0.000006    Time 0.066725    
2022-12-29 10:34:59,840 - Epoch: [294][   70/  113]    Overall Loss 1.257009    Objective Loss 1.257009                                        LR 0.000006    Time 0.066176    
2022-12-29 10:35:00,470 - Epoch: [294][   80/  113]    Overall Loss 1.259655    Objective Loss 1.259655                                        LR 0.000006    Time 0.065782    
2022-12-29 10:35:01,103 - Epoch: [294][   90/  113]    Overall Loss 1.255912    Objective Loss 1.255912                                        LR 0.000006    Time 0.065498    
2022-12-29 10:35:01,735 - Epoch: [294][  100/  113]    Overall Loss 1.247948    Objective Loss 1.247948                                        LR 0.000006    Time 0.065261    
2022-12-29 10:35:02,361 - Epoch: [294][  110/  113]    Overall Loss 1.242560    Objective Loss 1.242560                                        LR 0.000006    Time 0.065013    
2022-12-29 10:35:02,533 - Epoch: [294][  113/  113]    Overall Loss 1.241144    Objective Loss 1.241144    Top1 62.500000    Top5 91.666667    LR 0.000006    Time 0.064805    
2022-12-29 10:35:02,591 - --- validate (epoch=294)-----------
2022-12-29 10:35:02,591 - 200 samples (16 per mini-batch)
2022-12-29 10:35:03,152 - Epoch: [294][   10/   13]    Loss 1.222664    Top1 57.500000    Top5 96.875000    
2022-12-29 10:35:03,237 - Epoch: [294][   13/   13]    Loss 1.235321    Top1 55.000000    Top5 97.500000    
2022-12-29 10:35:03,288 - ==> Top1: 55.000    Top5: 97.500    Loss: 1.235

2022-12-29 10:35:03,289 - ==> Confusion:
[[17  2  1  6  1  0]
 [ 4 25  6  4  7  1]
 [ 1 10 13  4  5  0]
 [ 2  6  2 36  3  0]
 [ 1 10  2  2 18  0]
 [ 0  4  0  2  4  1]]

2022-12-29 10:35:03,292 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:03,292 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:03,313 - 

2022-12-29 10:35:03,314 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:04,157 - Epoch: [295][   10/  113]    Overall Loss 1.210370    Objective Loss 1.210370                                        LR 0.000006    Time 0.084132    
2022-12-29 10:35:04,792 - Epoch: [295][   20/  113]    Overall Loss 1.191630    Objective Loss 1.191630                                        LR 0.000006    Time 0.073790    
2022-12-29 10:35:05,433 - Epoch: [295][   30/  113]    Overall Loss 1.198090    Objective Loss 1.198090                                        LR 0.000006    Time 0.070568    
2022-12-29 10:35:06,067 - Epoch: [295][   40/  113]    Overall Loss 1.226433    Objective Loss 1.226433                                        LR 0.000006    Time 0.068756    
2022-12-29 10:35:06,704 - Epoch: [295][   50/  113]    Overall Loss 1.226444    Objective Loss 1.226444                                        LR 0.000006    Time 0.067724    
2022-12-29 10:35:07,338 - Epoch: [295][   60/  113]    Overall Loss 1.236485    Objective Loss 1.236485                                        LR 0.000006    Time 0.066998    
2022-12-29 10:35:07,972 - Epoch: [295][   70/  113]    Overall Loss 1.236431    Objective Loss 1.236431                                        LR 0.000006    Time 0.066473    
2022-12-29 10:35:08,604 - Epoch: [295][   80/  113]    Overall Loss 1.233460    Objective Loss 1.233460                                        LR 0.000006    Time 0.066061    
2022-12-29 10:35:09,233 - Epoch: [295][   90/  113]    Overall Loss 1.234090    Objective Loss 1.234090                                        LR 0.000006    Time 0.065711    
2022-12-29 10:35:09,865 - Epoch: [295][  100/  113]    Overall Loss 1.233314    Objective Loss 1.233314                                        LR 0.000006    Time 0.065456    
2022-12-29 10:35:10,493 - Epoch: [295][  110/  113]    Overall Loss 1.235073    Objective Loss 1.235073                                        LR 0.000006    Time 0.065203    
2022-12-29 10:35:10,663 - Epoch: [295][  113/  113]    Overall Loss 1.240208    Objective Loss 1.240208    Top1 33.333333    Top5 100.000000    LR 0.000006    Time 0.064976    
2022-12-29 10:35:10,722 - --- validate (epoch=295)-----------
2022-12-29 10:35:10,722 - 200 samples (16 per mini-batch)
2022-12-29 10:35:11,283 - Epoch: [295][   10/   13]    Loss 1.243448    Top1 53.125000    Top5 97.500000    
2022-12-29 10:35:11,368 - Epoch: [295][   13/   13]    Loss 1.247787    Top1 52.500000    Top5 97.500000    
2022-12-29 10:35:11,426 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.248

2022-12-29 10:35:11,427 - ==> Confusion:
[[24  4  2  3  2  0]
 [ 0 20  0 10  9  1]
 [ 0 10  6  6  4  1]
 [ 4  7  0 38  1  0]
 [ 0  9  3 10 14  2]
 [ 0  4  0  2  1  3]]

2022-12-29 10:35:11,430 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:11,431 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:11,453 - 

2022-12-29 10:35:11,454 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:12,285 - Epoch: [296][   10/  113]    Overall Loss 1.305876    Objective Loss 1.305876                                        LR 0.000006    Time 0.083006    
2022-12-29 10:35:12,924 - Epoch: [296][   20/  113]    Overall Loss 1.270264    Objective Loss 1.270264                                        LR 0.000006    Time 0.073422    
2022-12-29 10:35:13,561 - Epoch: [296][   30/  113]    Overall Loss 1.283311    Objective Loss 1.283311                                        LR 0.000006    Time 0.070148    
2022-12-29 10:35:14,199 - Epoch: [296][   40/  113]    Overall Loss 1.280912    Objective Loss 1.280912                                        LR 0.000006    Time 0.068564    
2022-12-29 10:35:14,831 - Epoch: [296][   50/  113]    Overall Loss 1.255154    Objective Loss 1.255154                                        LR 0.000006    Time 0.067465    
2022-12-29 10:35:15,464 - Epoch: [296][   60/  113]    Overall Loss 1.233592    Objective Loss 1.233592                                        LR 0.000006    Time 0.066775    
2022-12-29 10:35:16,091 - Epoch: [296][   70/  113]    Overall Loss 1.227148    Objective Loss 1.227148                                        LR 0.000006    Time 0.066184    
2022-12-29 10:35:16,721 - Epoch: [296][   80/  113]    Overall Loss 1.223536    Objective Loss 1.223536                                        LR 0.000006    Time 0.065783    
2022-12-29 10:35:17,356 - Epoch: [296][   90/  113]    Overall Loss 1.233247    Objective Loss 1.233247                                        LR 0.000006    Time 0.065518    
2022-12-29 10:35:17,991 - Epoch: [296][  100/  113]    Overall Loss 1.245809    Objective Loss 1.245809                                        LR 0.000006    Time 0.065317    
2022-12-29 10:35:18,623 - Epoch: [296][  110/  113]    Overall Loss 1.244405    Objective Loss 1.244405                                        LR 0.000006    Time 0.065122    
2022-12-29 10:35:18,795 - Epoch: [296][  113/  113]    Overall Loss 1.245271    Objective Loss 1.245271    Top1 58.333333    Top5 91.666667    LR 0.000006    Time 0.064909    
2022-12-29 10:35:18,851 - --- validate (epoch=296)-----------
2022-12-29 10:35:18,852 - 200 samples (16 per mini-batch)
2022-12-29 10:35:19,393 - Epoch: [296][   10/   13]    Loss 1.305696    Top1 46.875000    Top5 96.250000    
2022-12-29 10:35:19,476 - Epoch: [296][   13/   13]    Loss 1.253816    Top1 48.000000    Top5 97.000000    
2022-12-29 10:35:19,522 - ==> Top1: 48.000    Top5: 97.000    Loss: 1.254

2022-12-29 10:35:19,522 - ==> Confusion:
[[16  4  1  3  4  0]
 [ 2 22  4  9  8  0]
 [ 0 14  8  7  3  0]
 [ 2  3  1 36  3  1]
 [ 2  7  3 10 11  1]
 [ 2  3  1  2  4  3]]

2022-12-29 10:35:19,526 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:19,526 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:19,546 - 

2022-12-29 10:35:19,547 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:20,376 - Epoch: [297][   10/  113]    Overall Loss 1.366869    Objective Loss 1.366869                                        LR 0.000006    Time 0.082881    
2022-12-29 10:35:21,014 - Epoch: [297][   20/  113]    Overall Loss 1.332386    Objective Loss 1.332386                                        LR 0.000006    Time 0.073314    
2022-12-29 10:35:21,653 - Epoch: [297][   30/  113]    Overall Loss 1.324580    Objective Loss 1.324580                                        LR 0.000006    Time 0.070159    
2022-12-29 10:35:22,290 - Epoch: [297][   40/  113]    Overall Loss 1.288997    Objective Loss 1.288997                                        LR 0.000006    Time 0.068522    
2022-12-29 10:35:22,928 - Epoch: [297][   50/  113]    Overall Loss 1.260129    Objective Loss 1.260129                                        LR 0.000006    Time 0.067566    
2022-12-29 10:35:23,562 - Epoch: [297][   60/  113]    Overall Loss 1.243138    Objective Loss 1.243138                                        LR 0.000006    Time 0.066869    
2022-12-29 10:35:24,199 - Epoch: [297][   70/  113]    Overall Loss 1.253403    Objective Loss 1.253403                                        LR 0.000006    Time 0.066404    
2022-12-29 10:35:24,833 - Epoch: [297][   80/  113]    Overall Loss 1.258760    Objective Loss 1.258760                                        LR 0.000006    Time 0.066020    
2022-12-29 10:35:25,471 - Epoch: [297][   90/  113]    Overall Loss 1.260788    Objective Loss 1.260788                                        LR 0.000006    Time 0.065775    
2022-12-29 10:35:26,105 - Epoch: [297][  100/  113]    Overall Loss 1.256693    Objective Loss 1.256693                                        LR 0.000006    Time 0.065528    
2022-12-29 10:35:26,729 - Epoch: [297][  110/  113]    Overall Loss 1.253528    Objective Loss 1.253528                                        LR 0.000006    Time 0.065243    
2022-12-29 10:35:26,901 - Epoch: [297][  113/  113]    Overall Loss 1.248430    Objective Loss 1.248430    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.065031    
2022-12-29 10:35:26,950 - --- validate (epoch=297)-----------
2022-12-29 10:35:26,950 - 200 samples (16 per mini-batch)
2022-12-29 10:35:27,493 - Epoch: [297][   10/   13]    Loss 1.310446    Top1 43.750000    Top5 94.375000    
2022-12-29 10:35:27,577 - Epoch: [297][   13/   13]    Loss 1.304537    Top1 43.500000    Top5 95.000000    
2022-12-29 10:35:27,638 - ==> Top1: 43.500    Top5: 95.000    Loss: 1.305

2022-12-29 10:35:27,638 - ==> Confusion:
[[15  0  0  5  4  0]
 [ 2 18  4  9 10  0]
 [ 0  9  8  7  7  0]
 [ 9  2  3 35  3  0]
 [ 4  6  4 12 10  1]
 [ 2  3  1  3  3  1]]

2022-12-29 10:35:27,640 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:27,641 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:27,662 - 

2022-12-29 10:35:27,662 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:28,496 - Epoch: [298][   10/  113]    Overall Loss 1.316405    Objective Loss 1.316405                                        LR 0.000006    Time 0.083258    
2022-12-29 10:35:29,130 - Epoch: [298][   20/  113]    Overall Loss 1.297730    Objective Loss 1.297730                                        LR 0.000006    Time 0.073311    
2022-12-29 10:35:29,766 - Epoch: [298][   30/  113]    Overall Loss 1.281054    Objective Loss 1.281054                                        LR 0.000006    Time 0.070045    
2022-12-29 10:35:30,391 - Epoch: [298][   40/  113]    Overall Loss 1.290910    Objective Loss 1.290910                                        LR 0.000006    Time 0.068159    
2022-12-29 10:35:31,017 - Epoch: [298][   50/  113]    Overall Loss 1.285749    Objective Loss 1.285749                                        LR 0.000006    Time 0.067046    
2022-12-29 10:35:31,649 - Epoch: [298][   60/  113]    Overall Loss 1.258326    Objective Loss 1.258326                                        LR 0.000006    Time 0.066391    
2022-12-29 10:35:32,277 - Epoch: [298][   70/  113]    Overall Loss 1.251180    Objective Loss 1.251180                                        LR 0.000006    Time 0.065867    
2022-12-29 10:35:32,903 - Epoch: [298][   80/  113]    Overall Loss 1.245989    Objective Loss 1.245989                                        LR 0.000006    Time 0.065453    
2022-12-29 10:35:33,535 - Epoch: [298][   90/  113]    Overall Loss 1.253334    Objective Loss 1.253334                                        LR 0.000006    Time 0.065203    
2022-12-29 10:35:34,161 - Epoch: [298][  100/  113]    Overall Loss 1.244109    Objective Loss 1.244109                                        LR 0.000006    Time 0.064933    
2022-12-29 10:35:34,787 - Epoch: [298][  110/  113]    Overall Loss 1.245886    Objective Loss 1.245886                                        LR 0.000006    Time 0.064715    
2022-12-29 10:35:34,962 - Epoch: [298][  113/  113]    Overall Loss 1.248808    Objective Loss 1.248808    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.064546    
2022-12-29 10:35:35,020 - --- validate (epoch=298)-----------
2022-12-29 10:35:35,020 - 200 samples (16 per mini-batch)
2022-12-29 10:35:35,561 - Epoch: [298][   10/   13]    Loss 1.190768    Top1 53.750000    Top5 98.125000    
2022-12-29 10:35:35,645 - Epoch: [298][   13/   13]    Loss 1.175170    Top1 55.000000    Top5 98.500000    
2022-12-29 10:35:35,697 - ==> Top1: 55.000    Top5: 98.500    Loss: 1.175

2022-12-29 10:35:35,697 - ==> Confusion:
[[25  3  0  8  1  0]
 [ 3 25  3  6  7  0]
 [ 0  4 10  6  4  0]
 [ 3  0  4 28  5  1]
 [ 1 13  2  5 20  0]
 [ 0  3  1  4  3  2]]

2022-12-29 10:35:35,700 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:35,700 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:35,711 - 

2022-12-29 10:35:35,711 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:36,546 - Epoch: [299][   10/  113]    Overall Loss 1.264062    Objective Loss 1.264062                                        LR 0.000006    Time 0.083406    
2022-12-29 10:35:37,182 - Epoch: [299][   20/  113]    Overall Loss 1.215133    Objective Loss 1.215133                                        LR 0.000006    Time 0.073475    
2022-12-29 10:35:37,825 - Epoch: [299][   30/  113]    Overall Loss 1.245005    Objective Loss 1.245005                                        LR 0.000006    Time 0.070398    
2022-12-29 10:35:38,457 - Epoch: [299][   40/  113]    Overall Loss 1.238876    Objective Loss 1.238876                                        LR 0.000006    Time 0.068589    
2022-12-29 10:35:39,092 - Epoch: [299][   50/  113]    Overall Loss 1.222730    Objective Loss 1.222730                                        LR 0.000006    Time 0.067565    
2022-12-29 10:35:39,728 - Epoch: [299][   60/  113]    Overall Loss 1.226556    Objective Loss 1.226556                                        LR 0.000006    Time 0.066901    
2022-12-29 10:35:40,367 - Epoch: [299][   70/  113]    Overall Loss 1.235971    Objective Loss 1.235971                                        LR 0.000006    Time 0.066460    
2022-12-29 10:35:40,999 - Epoch: [299][   80/  113]    Overall Loss 1.228147    Objective Loss 1.228147                                        LR 0.000006    Time 0.066051    
2022-12-29 10:35:41,636 - Epoch: [299][   90/  113]    Overall Loss 1.235109    Objective Loss 1.235109                                        LR 0.000006    Time 0.065778    
2022-12-29 10:35:42,272 - Epoch: [299][  100/  113]    Overall Loss 1.233368    Objective Loss 1.233368                                        LR 0.000006    Time 0.065557    
2022-12-29 10:35:42,906 - Epoch: [299][  110/  113]    Overall Loss 1.233500    Objective Loss 1.233500                                        LR 0.000006    Time 0.065359    
2022-12-29 10:35:43,083 - Epoch: [299][  113/  113]    Overall Loss 1.237288    Objective Loss 1.237288    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065189    
2022-12-29 10:35:43,136 - --- validate (epoch=299)-----------
2022-12-29 10:35:43,136 - 200 samples (16 per mini-batch)
2022-12-29 10:35:43,684 - Epoch: [299][   10/   13]    Loss 1.305893    Top1 47.500000    Top5 95.000000    
2022-12-29 10:35:43,768 - Epoch: [299][   13/   13]    Loss 1.288872    Top1 49.000000    Top5 95.500000    
2022-12-29 10:35:43,829 - ==> Top1: 49.000    Top5: 95.500    Loss: 1.289

2022-12-29 10:35:43,830 - ==> Confusion:
[[18  0  0  5  3  1]
 [ 1 24  5  7  8  0]
 [ 2  6  5  5  6  0]
 [ 5  2  4 29  5  1]
 [ 2  8  3 12 18  0]
 [ 2  2  0  2  5  4]]

2022-12-29 10:35:43,833 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:43,833 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:43,857 - 

2022-12-29 10:35:43,857 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:44,684 - Epoch: [300][   10/  113]    Overall Loss 1.240329    Objective Loss 1.240329                                        LR 0.000006    Time 0.082605    
2022-12-29 10:35:45,325 - Epoch: [300][   20/  113]    Overall Loss 1.248815    Objective Loss 1.248815                                        LR 0.000006    Time 0.073317    
2022-12-29 10:35:45,963 - Epoch: [300][   30/  113]    Overall Loss 1.260318    Objective Loss 1.260318                                        LR 0.000006    Time 0.070134    
2022-12-29 10:35:46,598 - Epoch: [300][   40/  113]    Overall Loss 1.234352    Objective Loss 1.234352                                        LR 0.000006    Time 0.068449    
2022-12-29 10:35:47,232 - Epoch: [300][   50/  113]    Overall Loss 1.223009    Objective Loss 1.223009                                        LR 0.000006    Time 0.067428    
2022-12-29 10:35:47,867 - Epoch: [300][   60/  113]    Overall Loss 1.225629    Objective Loss 1.225629                                        LR 0.000006    Time 0.066773    
2022-12-29 10:35:48,500 - Epoch: [300][   70/  113]    Overall Loss 1.232501    Objective Loss 1.232501                                        LR 0.000006    Time 0.066271    
2022-12-29 10:35:49,130 - Epoch: [300][   80/  113]    Overall Loss 1.222066    Objective Loss 1.222066                                        LR 0.000006    Time 0.065857    
2022-12-29 10:35:49,767 - Epoch: [300][   90/  113]    Overall Loss 1.226609    Objective Loss 1.226609                                        LR 0.000006    Time 0.065612    
2022-12-29 10:35:50,399 - Epoch: [300][  100/  113]    Overall Loss 1.220420    Objective Loss 1.220420                                        LR 0.000006    Time 0.065363    
2022-12-29 10:35:51,022 - Epoch: [300][  110/  113]    Overall Loss 1.229795    Objective Loss 1.229795                                        LR 0.000006    Time 0.065081    
2022-12-29 10:35:51,200 - Epoch: [300][  113/  113]    Overall Loss 1.240600    Objective Loss 1.240600    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.064922    
2022-12-29 10:35:51,255 - --- validate (epoch=300)-----------
2022-12-29 10:35:51,256 - 200 samples (16 per mini-batch)
2022-12-29 10:35:51,793 - Epoch: [300][   10/   13]    Loss 1.299084    Top1 51.250000    Top5 98.125000    
2022-12-29 10:35:51,879 - Epoch: [300][   13/   13]    Loss 1.334278    Top1 52.500000    Top5 98.500000    
2022-12-29 10:35:51,927 - ==> Top1: 52.500    Top5: 98.500    Loss: 1.334

2022-12-29 10:35:51,927 - ==> Confusion:
[[16  3  1  8  3  1]
 [ 0 19  5  7  4  0]
 [ 0  8 11  3  6  0]
 [ 1  6  0 42  3  1]
 [ 2  7  3 10 16  0]
 [ 3  2  1  2  5  1]]

2022-12-29 10:35:51,929 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:35:51,930 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:35:51,951 - 

2022-12-29 10:35:51,951 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:35:52,790 - Epoch: [301][   10/  113]    Overall Loss 1.351633    Objective Loss 1.351633                                        LR 0.000006    Time 0.083715    
2022-12-29 10:35:53,427 - Epoch: [301][   20/  113]    Overall Loss 1.296602    Objective Loss 1.296602                                        LR 0.000006    Time 0.073724    
2022-12-29 10:35:54,061 - Epoch: [301][   30/  113]    Overall Loss 1.241319    Objective Loss 1.241319                                        LR 0.000006    Time 0.070252    
2022-12-29 10:35:54,695 - Epoch: [301][   40/  113]    Overall Loss 1.241929    Objective Loss 1.241929                                        LR 0.000006    Time 0.068535    
2022-12-29 10:35:55,324 - Epoch: [301][   50/  113]    Overall Loss 1.220539    Objective Loss 1.220539                                        LR 0.000006    Time 0.067403    
2022-12-29 10:35:55,963 - Epoch: [301][   60/  113]    Overall Loss 1.236814    Objective Loss 1.236814                                        LR 0.000006    Time 0.066800    
2022-12-29 10:35:56,597 - Epoch: [301][   70/  113]    Overall Loss 1.256687    Objective Loss 1.256687                                        LR 0.000006    Time 0.066314    
2022-12-29 10:35:57,228 - Epoch: [301][   80/  113]    Overall Loss 1.234427    Objective Loss 1.234427                                        LR 0.000006    Time 0.065907    
2022-12-29 10:35:57,862 - Epoch: [301][   90/  113]    Overall Loss 1.243426    Objective Loss 1.243426                                        LR 0.000006    Time 0.065622    
2022-12-29 10:35:58,494 - Epoch: [301][  100/  113]    Overall Loss 1.243486    Objective Loss 1.243486                                        LR 0.000006    Time 0.065369    
2022-12-29 10:35:59,118 - Epoch: [301][  110/  113]    Overall Loss 1.245877    Objective Loss 1.245877                                        LR 0.000006    Time 0.065103    
2022-12-29 10:35:59,291 - Epoch: [301][  113/  113]    Overall Loss 1.247424    Objective Loss 1.247424    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064902    
2022-12-29 10:35:59,348 - --- validate (epoch=301)-----------
2022-12-29 10:35:59,349 - 200 samples (16 per mini-batch)
2022-12-29 10:35:59,909 - Epoch: [301][   10/   13]    Loss 1.236979    Top1 51.875000    Top5 98.125000    
2022-12-29 10:35:59,996 - Epoch: [301][   13/   13]    Loss 1.272923    Top1 51.000000    Top5 97.500000    
2022-12-29 10:36:00,053 - ==> Top1: 51.000    Top5: 97.500    Loss: 1.273

2022-12-29 10:36:00,053 - ==> Confusion:
[[19  5  0  4  1  0]
 [ 1 21  3  9  9  0]
 [ 2  5 12  5  5  0]
 [ 4  5  2 35  3  0]
 [ 2 17  4  6 14  0]
 [ 0  5  1  0  0  1]]

2022-12-29 10:36:00,057 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:36:00,058 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:00,069 - 

2022-12-29 10:36:00,069 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:00,911 - Epoch: [302][   10/  113]    Overall Loss 1.331509    Objective Loss 1.331509                                        LR 0.000006    Time 0.084132    
2022-12-29 10:36:01,548 - Epoch: [302][   20/  113]    Overall Loss 1.266621    Objective Loss 1.266621                                        LR 0.000006    Time 0.073855    
2022-12-29 10:36:02,188 - Epoch: [302][   30/  113]    Overall Loss 1.237953    Objective Loss 1.237953                                        LR 0.000006    Time 0.070545    
2022-12-29 10:36:02,826 - Epoch: [302][   40/  113]    Overall Loss 1.262851    Objective Loss 1.262851                                        LR 0.000006    Time 0.068866    
2022-12-29 10:36:03,463 - Epoch: [302][   50/  113]    Overall Loss 1.261784    Objective Loss 1.261784                                        LR 0.000006    Time 0.067813    
2022-12-29 10:36:04,099 - Epoch: [302][   60/  113]    Overall Loss 1.266034    Objective Loss 1.266034                                        LR 0.000006    Time 0.067106    
2022-12-29 10:36:04,733 - Epoch: [302][   70/  113]    Overall Loss 1.260481    Objective Loss 1.260481                                        LR 0.000006    Time 0.066565    
2022-12-29 10:36:05,364 - Epoch: [302][   80/  113]    Overall Loss 1.240790    Objective Loss 1.240790                                        LR 0.000006    Time 0.066130    
2022-12-29 10:36:05,999 - Epoch: [302][   90/  113]    Overall Loss 1.255980    Objective Loss 1.255980                                        LR 0.000006    Time 0.065828    
2022-12-29 10:36:06,628 - Epoch: [302][  100/  113]    Overall Loss 1.254644    Objective Loss 1.254644                                        LR 0.000006    Time 0.065531    
2022-12-29 10:36:07,257 - Epoch: [302][  110/  113]    Overall Loss 1.258719    Objective Loss 1.258719                                        LR 0.000006    Time 0.065289    
2022-12-29 10:36:07,434 - Epoch: [302][  113/  113]    Overall Loss 1.254342    Objective Loss 1.254342    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.065117    
2022-12-29 10:36:07,479 - --- validate (epoch=302)-----------
2022-12-29 10:36:07,479 - 200 samples (16 per mini-batch)
2022-12-29 10:36:08,030 - Epoch: [302][   10/   13]    Loss 1.263879    Top1 51.875000    Top5 98.125000    
2022-12-29 10:36:08,115 - Epoch: [302][   13/   13]    Loss 1.227823    Top1 52.000000    Top5 98.500000    
2022-12-29 10:36:08,160 - ==> Top1: 52.000    Top5: 98.500    Loss: 1.228

2022-12-29 10:36:08,160 - ==> Confusion:
[[16  0  0 10  3  0]
 [ 2 21  5  8  3  1]
 [ 1 12  9  2  3  0]
 [ 3  2  4 40  2  0]
 [ 4 12  6  7 14  0]
 [ 1  2  2  1  0  4]]

2022-12-29 10:36:08,163 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:36:08,164 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:08,185 - 

2022-12-29 10:36:08,185 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:09,011 - Epoch: [303][   10/  113]    Overall Loss 1.101409    Objective Loss 1.101409                                        LR 0.000006    Time 0.082447    
2022-12-29 10:36:09,647 - Epoch: [303][   20/  113]    Overall Loss 1.140083    Objective Loss 1.140083                                        LR 0.000006    Time 0.073014    
2022-12-29 10:36:10,283 - Epoch: [303][   30/  113]    Overall Loss 1.171733    Objective Loss 1.171733                                        LR 0.000006    Time 0.069850    
2022-12-29 10:36:10,916 - Epoch: [303][   40/  113]    Overall Loss 1.173433    Objective Loss 1.173433                                        LR 0.000006    Time 0.068201    
2022-12-29 10:36:11,551 - Epoch: [303][   50/  113]    Overall Loss 1.189711    Objective Loss 1.189711                                        LR 0.000006    Time 0.067267    
2022-12-29 10:36:12,186 - Epoch: [303][   60/  113]    Overall Loss 1.214199    Objective Loss 1.214199                                        LR 0.000006    Time 0.066629    
2022-12-29 10:36:12,828 - Epoch: [303][   70/  113]    Overall Loss 1.212977    Objective Loss 1.212977                                        LR 0.000006    Time 0.066264    
2022-12-29 10:36:13,461 - Epoch: [303][   80/  113]    Overall Loss 1.216869    Objective Loss 1.216869                                        LR 0.000006    Time 0.065888    
2022-12-29 10:36:14,099 - Epoch: [303][   90/  113]    Overall Loss 1.220775    Objective Loss 1.220775                                        LR 0.000006    Time 0.065654    
2022-12-29 10:36:14,734 - Epoch: [303][  100/  113]    Overall Loss 1.223374    Objective Loss 1.223374                                        LR 0.000006    Time 0.065432    
2022-12-29 10:36:15,353 - Epoch: [303][  110/  113]    Overall Loss 1.225700    Objective Loss 1.225700                                        LR 0.000006    Time 0.065107    
2022-12-29 10:36:15,529 - Epoch: [303][  113/  113]    Overall Loss 1.227762    Objective Loss 1.227762    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.064939    
2022-12-29 10:36:15,582 - --- validate (epoch=303)-----------
2022-12-29 10:36:15,583 - 200 samples (16 per mini-batch)
2022-12-29 10:36:16,137 - Epoch: [303][   10/   13]    Loss 1.282735    Top1 53.125000    Top5 96.250000    
2022-12-29 10:36:16,223 - Epoch: [303][   13/   13]    Loss 1.278122    Top1 53.000000    Top5 96.000000    
2022-12-29 10:36:16,282 - ==> Top1: 53.000    Top5: 96.000    Loss: 1.278

2022-12-29 10:36:16,282 - ==> Confusion:
[[19  2  1  3  6  1]
 [ 4 22  2  7  6  1]
 [ 2  5 12  6  6  0]
 [ 3  7  0 36  4  1]
 [ 4  7  1  6 17  0]
 [ 4  3  0  1  1  0]]

2022-12-29 10:36:16,286 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:36:16,286 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:16,309 - 

2022-12-29 10:36:16,309 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:17,138 - Epoch: [304][   10/  113]    Overall Loss 1.382380    Objective Loss 1.382380                                        LR 0.000006    Time 0.082727    
2022-12-29 10:36:17,775 - Epoch: [304][   20/  113]    Overall Loss 1.341901    Objective Loss 1.341901                                        LR 0.000006    Time 0.073185    
2022-12-29 10:36:18,408 - Epoch: [304][   30/  113]    Overall Loss 1.353057    Objective Loss 1.353057                                        LR 0.000006    Time 0.069878    
2022-12-29 10:36:19,041 - Epoch: [304][   40/  113]    Overall Loss 1.318568    Objective Loss 1.318568                                        LR 0.000006    Time 0.068221    
2022-12-29 10:36:19,677 - Epoch: [304][   50/  113]    Overall Loss 1.316616    Objective Loss 1.316616                                        LR 0.000006    Time 0.067273    
2022-12-29 10:36:20,311 - Epoch: [304][   60/  113]    Overall Loss 1.324253    Objective Loss 1.324253                                        LR 0.000006    Time 0.066618    
2022-12-29 10:36:20,945 - Epoch: [304][   70/  113]    Overall Loss 1.323941    Objective Loss 1.323941                                        LR 0.000006    Time 0.066157    
2022-12-29 10:36:21,575 - Epoch: [304][   80/  113]    Overall Loss 1.318654    Objective Loss 1.318654                                        LR 0.000006    Time 0.065753    
2022-12-29 10:36:22,206 - Epoch: [304][   90/  113]    Overall Loss 1.313183    Objective Loss 1.313183                                        LR 0.000006    Time 0.065456    
2022-12-29 10:36:22,843 - Epoch: [304][  100/  113]    Overall Loss 1.314546    Objective Loss 1.314546                                        LR 0.000006    Time 0.065273    
2022-12-29 10:36:23,472 - Epoch: [304][  110/  113]    Overall Loss 1.308052    Objective Loss 1.308052                                        LR 0.000006    Time 0.065056    
2022-12-29 10:36:23,644 - Epoch: [304][  113/  113]    Overall Loss 1.308079    Objective Loss 1.308079    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064843    
2022-12-29 10:36:23,694 - --- validate (epoch=304)-----------
2022-12-29 10:36:23,695 - 200 samples (16 per mini-batch)
2022-12-29 10:36:24,255 - Epoch: [304][   10/   13]    Loss 1.298100    Top1 45.000000    Top5 97.500000    
2022-12-29 10:36:24,339 - Epoch: [304][   13/   13]    Loss 1.313976    Top1 44.000000    Top5 96.000000    
2022-12-29 10:36:24,392 - ==> Top1: 44.000    Top5: 96.000    Loss: 1.314

2022-12-29 10:36:24,392 - ==> Confusion:
[[22  2  0  7  1  0]
 [ 2 17  2 14  6  0]
 [ 1 10  8  5  8  1]
 [ 4  1  3 29  3  1]
 [ 4  5  3 17  8  0]
 [ 2  5  1  4  0  4]]

2022-12-29 10:36:24,394 - ==> Best [Top1: 56.000   Top5: 99.500   Sparsity:0.00   Params: 289216 on epoch: 240]
2022-12-29 10:36:24,394 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:24,404 - 

2022-12-29 10:36:24,404 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:25,240 - Epoch: [305][   10/  113]    Overall Loss 1.271936    Objective Loss 1.271936                                        LR 0.000006    Time 0.083442    
2022-12-29 10:36:25,882 - Epoch: [305][   20/  113]    Overall Loss 1.261732    Objective Loss 1.261732                                        LR 0.000006    Time 0.073808    
2022-12-29 10:36:26,520 - Epoch: [305][   30/  113]    Overall Loss 1.245736    Objective Loss 1.245736                                        LR 0.000006    Time 0.070444    
2022-12-29 10:36:27,156 - Epoch: [305][   40/  113]    Overall Loss 1.249014    Objective Loss 1.249014                                        LR 0.000006    Time 0.068720    
2022-12-29 10:36:27,788 - Epoch: [305][   50/  113]    Overall Loss 1.254213    Objective Loss 1.254213                                        LR 0.000006    Time 0.067607    
2022-12-29 10:36:28,419 - Epoch: [305][   60/  113]    Overall Loss 1.271690    Objective Loss 1.271690                                        LR 0.000006    Time 0.066849    
2022-12-29 10:36:29,049 - Epoch: [305][   70/  113]    Overall Loss 1.280631    Objective Loss 1.280631                                        LR 0.000006    Time 0.066298    
2022-12-29 10:36:29,677 - Epoch: [305][   80/  113]    Overall Loss 1.268193    Objective Loss 1.268193                                        LR 0.000006    Time 0.065858    
2022-12-29 10:36:30,310 - Epoch: [305][   90/  113]    Overall Loss 1.260762    Objective Loss 1.260762                                        LR 0.000006    Time 0.065568    
2022-12-29 10:36:30,937 - Epoch: [305][  100/  113]    Overall Loss 1.250800    Objective Loss 1.250800                                        LR 0.000006    Time 0.065278    
2022-12-29 10:36:31,562 - Epoch: [305][  110/  113]    Overall Loss 1.245545    Objective Loss 1.245545                                        LR 0.000006    Time 0.065019    
2022-12-29 10:36:31,742 - Epoch: [305][  113/  113]    Overall Loss 1.247891    Objective Loss 1.247891    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064883    
2022-12-29 10:36:31,801 - --- validate (epoch=305)-----------
2022-12-29 10:36:31,801 - 200 samples (16 per mini-batch)
2022-12-29 10:36:32,349 - Epoch: [305][   10/   13]    Loss 1.112527    Top1 59.375000    Top5 96.875000    
2022-12-29 10:36:32,435 - Epoch: [305][   13/   13]    Loss 1.141712    Top1 59.000000    Top5 97.500000    
2022-12-29 10:36:32,495 - ==> Top1: 59.000    Top5: 97.500    Loss: 1.142

2022-12-29 10:36:32,496 - ==> Confusion:
[[18  4  1  4  4  0]
 [ 0 28  3  6  7  0]
 [ 0  8 16  2  5  0]
 [ 1  4  2 38  7  1]
 [ 1  7  3  5 17  0]
 [ 1  2  1  1  2  1]]

2022-12-29 10:36:32,498 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:36:32,498 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:32,525 - 

2022-12-29 10:36:32,526 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:33,370 - Epoch: [306][   10/  113]    Overall Loss 1.186684    Objective Loss 1.186684                                        LR 0.000006    Time 0.084247    
2022-12-29 10:36:34,007 - Epoch: [306][   20/  113]    Overall Loss 1.195397    Objective Loss 1.195397                                        LR 0.000006    Time 0.073951    
2022-12-29 10:36:34,642 - Epoch: [306][   30/  113]    Overall Loss 1.220478    Objective Loss 1.220478                                        LR 0.000006    Time 0.070448    
2022-12-29 10:36:35,276 - Epoch: [306][   40/  113]    Overall Loss 1.211701    Objective Loss 1.211701                                        LR 0.000006    Time 0.068674    
2022-12-29 10:36:35,908 - Epoch: [306][   50/  113]    Overall Loss 1.241628    Objective Loss 1.241628                                        LR 0.000006    Time 0.067571    
2022-12-29 10:36:36,535 - Epoch: [306][   60/  113]    Overall Loss 1.252563    Objective Loss 1.252563                                        LR 0.000006    Time 0.066749    
2022-12-29 10:36:37,160 - Epoch: [306][   70/  113]    Overall Loss 1.245577    Objective Loss 1.245577                                        LR 0.000006    Time 0.066139    
2022-12-29 10:36:37,794 - Epoch: [306][   80/  113]    Overall Loss 1.244075    Objective Loss 1.244075                                        LR 0.000006    Time 0.065789    
2022-12-29 10:36:38,425 - Epoch: [306][   90/  113]    Overall Loss 1.253603    Objective Loss 1.253603                                        LR 0.000006    Time 0.065481    
2022-12-29 10:36:39,057 - Epoch: [306][  100/  113]    Overall Loss 1.245919    Objective Loss 1.245919                                        LR 0.000006    Time 0.065247    
2022-12-29 10:36:39,683 - Epoch: [306][  110/  113]    Overall Loss 1.240135    Objective Loss 1.240135                                        LR 0.000006    Time 0.065004    
2022-12-29 10:36:39,861 - Epoch: [306][  113/  113]    Overall Loss 1.236194    Objective Loss 1.236194    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.064850    
2022-12-29 10:36:39,922 - --- validate (epoch=306)-----------
2022-12-29 10:36:39,922 - 200 samples (16 per mini-batch)
2022-12-29 10:36:40,468 - Epoch: [306][   10/   13]    Loss 1.209062    Top1 51.875000    Top5 98.750000    
2022-12-29 10:36:40,552 - Epoch: [306][   13/   13]    Loss 1.198164    Top1 53.000000    Top5 98.500000    
2022-12-29 10:36:40,608 - ==> Top1: 53.000    Top5: 98.500    Loss: 1.198

2022-12-29 10:36:40,608 - ==> Confusion:
[[20  3  0 10  1  0]
 [ 0 35  2  3  5  0]
 [ 1 10 11  7  6  0]
 [ 1  7  0 24  7  1]
 [ 3 13  3  4 15  0]
 [ 1  3  0  3  0  1]]

2022-12-29 10:36:40,612 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:36:40,612 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:40,624 - 

2022-12-29 10:36:40,624 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:41,446 - Epoch: [307][   10/  113]    Overall Loss 1.288311    Objective Loss 1.288311                                        LR 0.000006    Time 0.082071    
2022-12-29 10:36:42,096 - Epoch: [307][   20/  113]    Overall Loss 1.243781    Objective Loss 1.243781                                        LR 0.000006    Time 0.073517    
2022-12-29 10:36:42,732 - Epoch: [307][   30/  113]    Overall Loss 1.245362    Objective Loss 1.245362                                        LR 0.000006    Time 0.070203    
2022-12-29 10:36:43,374 - Epoch: [307][   40/  113]    Overall Loss 1.236927    Objective Loss 1.236927                                        LR 0.000006    Time 0.068696    
2022-12-29 10:36:44,004 - Epoch: [307][   50/  113]    Overall Loss 1.229848    Objective Loss 1.229848                                        LR 0.000006    Time 0.067534    
2022-12-29 10:36:44,635 - Epoch: [307][   60/  113]    Overall Loss 1.237657    Objective Loss 1.237657                                        LR 0.000006    Time 0.066796    
2022-12-29 10:36:45,265 - Epoch: [307][   70/  113]    Overall Loss 1.241926    Objective Loss 1.241926                                        LR 0.000006    Time 0.066245    
2022-12-29 10:36:45,901 - Epoch: [307][   80/  113]    Overall Loss 1.235549    Objective Loss 1.235549                                        LR 0.000006    Time 0.065901    
2022-12-29 10:36:46,530 - Epoch: [307][   90/  113]    Overall Loss 1.247087    Objective Loss 1.247087                                        LR 0.000006    Time 0.065570    
2022-12-29 10:36:47,159 - Epoch: [307][  100/  113]    Overall Loss 1.250462    Objective Loss 1.250462                                        LR 0.000006    Time 0.065295    
2022-12-29 10:36:47,782 - Epoch: [307][  110/  113]    Overall Loss 1.250722    Objective Loss 1.250722                                        LR 0.000006    Time 0.065021    
2022-12-29 10:36:47,956 - Epoch: [307][  113/  113]    Overall Loss 1.251451    Objective Loss 1.251451    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064826    
2022-12-29 10:36:48,013 - --- validate (epoch=307)-----------
2022-12-29 10:36:48,014 - 200 samples (16 per mini-batch)
2022-12-29 10:36:48,559 - Epoch: [307][   10/   13]    Loss 1.258656    Top1 51.250000    Top5 98.125000    
2022-12-29 10:36:48,645 - Epoch: [307][   13/   13]    Loss 1.181251    Top1 53.500000    Top5 98.500000    
2022-12-29 10:36:48,709 - ==> Top1: 53.500    Top5: 98.500    Loss: 1.181

2022-12-29 10:36:48,710 - ==> Confusion:
[[17  4  2  2  2  0]
 [ 2 27  2  5  8  0]
 [ 0  8 11  7  6  1]
 [ 6  5  4 30  3  0]
 [ 1  6  3 10 19  0]
 [ 1  3  0  1  1  3]]

2022-12-29 10:36:48,713 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:36:48,713 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:48,725 - 

2022-12-29 10:36:48,725 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:49,548 - Epoch: [308][   10/  113]    Overall Loss 1.249647    Objective Loss 1.249647                                        LR 0.000006    Time 0.082183    
2022-12-29 10:36:50,186 - Epoch: [308][   20/  113]    Overall Loss 1.221638    Objective Loss 1.221638                                        LR 0.000006    Time 0.072953    
2022-12-29 10:36:50,824 - Epoch: [308][   30/  113]    Overall Loss 1.239289    Objective Loss 1.239289                                        LR 0.000006    Time 0.069877    
2022-12-29 10:36:51,458 - Epoch: [308][   40/  113]    Overall Loss 1.257668    Objective Loss 1.257668                                        LR 0.000006    Time 0.068247    
2022-12-29 10:36:52,096 - Epoch: [308][   50/  113]    Overall Loss 1.247251    Objective Loss 1.247251                                        LR 0.000006    Time 0.067353    
2022-12-29 10:36:52,729 - Epoch: [308][   60/  113]    Overall Loss 1.277171    Objective Loss 1.277171                                        LR 0.000006    Time 0.066676    
2022-12-29 10:36:53,368 - Epoch: [308][   70/  113]    Overall Loss 1.281579    Objective Loss 1.281579                                        LR 0.000006    Time 0.066266    
2022-12-29 10:36:54,000 - Epoch: [308][   80/  113]    Overall Loss 1.272740    Objective Loss 1.272740                                        LR 0.000006    Time 0.065872    
2022-12-29 10:36:54,637 - Epoch: [308][   90/  113]    Overall Loss 1.272639    Objective Loss 1.272639                                        LR 0.000006    Time 0.065629    
2022-12-29 10:36:55,266 - Epoch: [308][  100/  113]    Overall Loss 1.271220    Objective Loss 1.271220                                        LR 0.000006    Time 0.065358    
2022-12-29 10:36:55,893 - Epoch: [308][  110/  113]    Overall Loss 1.267700    Objective Loss 1.267700                                        LR 0.000006    Time 0.065105    
2022-12-29 10:36:56,069 - Epoch: [308][  113/  113]    Overall Loss 1.265577    Objective Loss 1.265577    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064935    
2022-12-29 10:36:56,114 - --- validate (epoch=308)-----------
2022-12-29 10:36:56,115 - 200 samples (16 per mini-batch)
2022-12-29 10:36:56,656 - Epoch: [308][   10/   13]    Loss 1.313628    Top1 45.000000    Top5 98.125000    
2022-12-29 10:36:56,740 - Epoch: [308][   13/   13]    Loss 1.314901    Top1 48.000000    Top5 97.500000    
2022-12-29 10:36:56,796 - ==> Top1: 48.000    Top5: 97.500    Loss: 1.315

2022-12-29 10:36:56,796 - ==> Confusion:
[[17  9  0  4  1  0]
 [ 2 22  4  4 10  1]
 [ 1 10 15  2  7  1]
 [ 4  7  1 30  7  2]
 [ 1  5  2 10 11  0]
 [ 2  1  3  1  2  1]]

2022-12-29 10:36:56,799 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:36:56,799 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:36:56,809 - 

2022-12-29 10:36:56,809 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:36:57,630 - Epoch: [309][   10/  113]    Overall Loss 1.205623    Objective Loss 1.205623                                        LR 0.000006    Time 0.081995    
2022-12-29 10:36:58,267 - Epoch: [309][   20/  113]    Overall Loss 1.215459    Objective Loss 1.215459                                        LR 0.000006    Time 0.072818    
2022-12-29 10:36:58,897 - Epoch: [309][   30/  113]    Overall Loss 1.245854    Objective Loss 1.245854                                        LR 0.000006    Time 0.069532    
2022-12-29 10:36:59,530 - Epoch: [309][   40/  113]    Overall Loss 1.255317    Objective Loss 1.255317                                        LR 0.000006    Time 0.067951    
2022-12-29 10:37:00,168 - Epoch: [309][   50/  113]    Overall Loss 1.253749    Objective Loss 1.253749                                        LR 0.000006    Time 0.067117    
2022-12-29 10:37:00,798 - Epoch: [309][   60/  113]    Overall Loss 1.248123    Objective Loss 1.248123                                        LR 0.000006    Time 0.066419    
2022-12-29 10:37:01,429 - Epoch: [309][   70/  113]    Overall Loss 1.244514    Objective Loss 1.244514                                        LR 0.000006    Time 0.065941    
2022-12-29 10:37:02,061 - Epoch: [309][   80/  113]    Overall Loss 1.235968    Objective Loss 1.235968                                        LR 0.000006    Time 0.065592    
2022-12-29 10:37:02,691 - Epoch: [309][   90/  113]    Overall Loss 1.241521    Objective Loss 1.241521                                        LR 0.000006    Time 0.065302    
2022-12-29 10:37:03,330 - Epoch: [309][  100/  113]    Overall Loss 1.235217    Objective Loss 1.235217                                        LR 0.000006    Time 0.065158    
2022-12-29 10:37:03,959 - Epoch: [309][  110/  113]    Overall Loss 1.236143    Objective Loss 1.236143                                        LR 0.000006    Time 0.064946    
2022-12-29 10:37:04,133 - Epoch: [309][  113/  113]    Overall Loss 1.241972    Objective Loss 1.241972    Top1 33.333333    Top5 95.833333    LR 0.000006    Time 0.064758    
2022-12-29 10:37:04,201 - --- validate (epoch=309)-----------
2022-12-29 10:37:04,201 - 200 samples (16 per mini-batch)
2022-12-29 10:37:04,752 - Epoch: [309][   10/   13]    Loss 1.201461    Top1 54.375000    Top5 98.125000    
2022-12-29 10:37:04,837 - Epoch: [309][   13/   13]    Loss 1.183147    Top1 54.500000    Top5 98.500000    
2022-12-29 10:37:04,885 - ==> Top1: 54.500    Top5: 98.500    Loss: 1.183

2022-12-29 10:37:04,886 - ==> Confusion:
[[20  2  0  3  1  0]
 [ 4 17  0  7  5  0]
 [ 1 10 12  6  5  0]
 [ 7  8  1 41  4  1]
 [ 4  7  1  4 14  0]
 [ 2  5  0  2  1  5]]

2022-12-29 10:37:04,890 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:04,890 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:04,904 - 

2022-12-29 10:37:04,904 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:05,742 - Epoch: [310][   10/  113]    Overall Loss 1.286898    Objective Loss 1.286898                                        LR 0.000006    Time 0.083734    
2022-12-29 10:37:06,381 - Epoch: [310][   20/  113]    Overall Loss 1.257738    Objective Loss 1.257738                                        LR 0.000006    Time 0.073760    
2022-12-29 10:37:07,016 - Epoch: [310][   30/  113]    Overall Loss 1.242771    Objective Loss 1.242771                                        LR 0.000006    Time 0.070336    
2022-12-29 10:37:07,648 - Epoch: [310][   40/  113]    Overall Loss 1.237168    Objective Loss 1.237168                                        LR 0.000006    Time 0.068527    
2022-12-29 10:37:08,279 - Epoch: [310][   50/  113]    Overall Loss 1.216571    Objective Loss 1.216571                                        LR 0.000006    Time 0.067442    
2022-12-29 10:37:08,905 - Epoch: [310][   60/  113]    Overall Loss 1.232072    Objective Loss 1.232072                                        LR 0.000006    Time 0.066622    
2022-12-29 10:37:09,530 - Epoch: [310][   70/  113]    Overall Loss 1.235453    Objective Loss 1.235453                                        LR 0.000006    Time 0.066027    
2022-12-29 10:37:10,159 - Epoch: [310][   80/  113]    Overall Loss 1.233521    Objective Loss 1.233521                                        LR 0.000006    Time 0.065634    
2022-12-29 10:37:10,786 - Epoch: [310][   90/  113]    Overall Loss 1.251490    Objective Loss 1.251490                                        LR 0.000006    Time 0.065300    
2022-12-29 10:37:11,410 - Epoch: [310][  100/  113]    Overall Loss 1.242614    Objective Loss 1.242614                                        LR 0.000006    Time 0.065011    
2022-12-29 10:37:12,038 - Epoch: [310][  110/  113]    Overall Loss 1.247228    Objective Loss 1.247228                                        LR 0.000006    Time 0.064801    
2022-12-29 10:37:12,209 - Epoch: [310][  113/  113]    Overall Loss 1.247674    Objective Loss 1.247674    Top1 33.333333    Top5 95.833333    LR 0.000006    Time 0.064597    
2022-12-29 10:37:12,255 - --- validate (epoch=310)-----------
2022-12-29 10:37:12,256 - 200 samples (16 per mini-batch)
2022-12-29 10:37:12,811 - Epoch: [310][   10/   13]    Loss 1.287839    Top1 50.000000    Top5 97.500000    
2022-12-29 10:37:12,896 - Epoch: [310][   13/   13]    Loss 1.287177    Top1 50.500000    Top5 98.000000    
2022-12-29 10:37:12,953 - ==> Top1: 50.500    Top5: 98.000    Loss: 1.287

2022-12-29 10:37:12,954 - ==> Confusion:
[[16  1  0  4  4  0]
 [ 4 15  3  8 10  0]
 [ 1  9 16  2  8  0]
 [ 5  2  1 34 10  0]
 [ 0 11  2  4 18  0]
 [ 0  3  2  3  2  2]]

2022-12-29 10:37:12,956 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:12,957 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:12,970 - 

2022-12-29 10:37:12,970 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:13,800 - Epoch: [311][   10/  113]    Overall Loss 1.193516    Objective Loss 1.193516                                        LR 0.000006    Time 0.082820    
2022-12-29 10:37:14,439 - Epoch: [311][   20/  113]    Overall Loss 1.150760    Objective Loss 1.150760                                        LR 0.000006    Time 0.073370    
2022-12-29 10:37:15,078 - Epoch: [311][   30/  113]    Overall Loss 1.157803    Objective Loss 1.157803                                        LR 0.000006    Time 0.070178    
2022-12-29 10:37:15,712 - Epoch: [311][   40/  113]    Overall Loss 1.184226    Objective Loss 1.184226                                        LR 0.000006    Time 0.068470    
2022-12-29 10:37:16,350 - Epoch: [311][   50/  113]    Overall Loss 1.204361    Objective Loss 1.204361                                        LR 0.000006    Time 0.067530    
2022-12-29 10:37:16,984 - Epoch: [311][   60/  113]    Overall Loss 1.224926    Objective Loss 1.224926                                        LR 0.000006    Time 0.066831    
2022-12-29 10:37:17,617 - Epoch: [311][   70/  113]    Overall Loss 1.230888    Objective Loss 1.230888                                        LR 0.000006    Time 0.066329    
2022-12-29 10:37:18,257 - Epoch: [311][   80/  113]    Overall Loss 1.243355    Objective Loss 1.243355                                        LR 0.000006    Time 0.066022    
2022-12-29 10:37:18,890 - Epoch: [311][   90/  113]    Overall Loss 1.253675    Objective Loss 1.253675                                        LR 0.000006    Time 0.065724    
2022-12-29 10:37:19,529 - Epoch: [311][  100/  113]    Overall Loss 1.245815    Objective Loss 1.245815                                        LR 0.000006    Time 0.065529    
2022-12-29 10:37:20,156 - Epoch: [311][  110/  113]    Overall Loss 1.245216    Objective Loss 1.245216                                        LR 0.000006    Time 0.065268    
2022-12-29 10:37:20,329 - Epoch: [311][  113/  113]    Overall Loss 1.248738    Objective Loss 1.248738    Top1 62.500000    Top5 91.666667    LR 0.000006    Time 0.065063    
2022-12-29 10:37:20,386 - --- validate (epoch=311)-----------
2022-12-29 10:37:20,386 - 200 samples (16 per mini-batch)
2022-12-29 10:37:20,934 - Epoch: [311][   10/   13]    Loss 1.262735    Top1 49.375000    Top5 98.125000    
2022-12-29 10:37:21,020 - Epoch: [311][   13/   13]    Loss 1.252801    Top1 50.500000    Top5 98.500000    
2022-12-29 10:37:21,084 - ==> Top1: 50.500    Top5: 98.500    Loss: 1.253

2022-12-29 10:37:21,084 - ==> Confusion:
[[12  2  1  9  0  1]
 [ 2 19  4  5 13  0]
 [ 2  3 13  4  8  0]
 [ 4  3  0 40  6  1]
 [ 2  8  2 10 15  0]
 [ 0  5  0  2  2  2]]

2022-12-29 10:37:21,086 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:21,086 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:21,105 - 

2022-12-29 10:37:21,106 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:21,944 - Epoch: [312][   10/  113]    Overall Loss 1.270407    Objective Loss 1.270407                                        LR 0.000006    Time 0.083722    
2022-12-29 10:37:22,580 - Epoch: [312][   20/  113]    Overall Loss 1.196429    Objective Loss 1.196429                                        LR 0.000006    Time 0.073651    
2022-12-29 10:37:23,220 - Epoch: [312][   30/  113]    Overall Loss 1.209345    Objective Loss 1.209345                                        LR 0.000006    Time 0.070420    
2022-12-29 10:37:23,853 - Epoch: [312][   40/  113]    Overall Loss 1.207046    Objective Loss 1.207046                                        LR 0.000006    Time 0.068615    
2022-12-29 10:37:24,480 - Epoch: [312][   50/  113]    Overall Loss 1.206650    Objective Loss 1.206650                                        LR 0.000006    Time 0.067428    
2022-12-29 10:37:25,109 - Epoch: [312][   60/  113]    Overall Loss 1.249523    Objective Loss 1.249523                                        LR 0.000006    Time 0.066669    
2022-12-29 10:37:25,742 - Epoch: [312][   70/  113]    Overall Loss 1.238205    Objective Loss 1.238205                                        LR 0.000006    Time 0.066183    
2022-12-29 10:37:26,372 - Epoch: [312][   80/  113]    Overall Loss 1.227468    Objective Loss 1.227468                                        LR 0.000006    Time 0.065778    
2022-12-29 10:37:27,001 - Epoch: [312][   90/  113]    Overall Loss 1.231556    Objective Loss 1.231556                                        LR 0.000006    Time 0.065450    
2022-12-29 10:37:27,629 - Epoch: [312][  100/  113]    Overall Loss 1.227577    Objective Loss 1.227577                                        LR 0.000006    Time 0.065183    
2022-12-29 10:37:28,255 - Epoch: [312][  110/  113]    Overall Loss 1.241213    Objective Loss 1.241213                                        LR 0.000006    Time 0.064942    
2022-12-29 10:37:28,429 - Epoch: [312][  113/  113]    Overall Loss 1.239523    Objective Loss 1.239523    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064757    
2022-12-29 10:37:28,488 - --- validate (epoch=312)-----------
2022-12-29 10:37:28,489 - 200 samples (16 per mini-batch)
2022-12-29 10:37:29,032 - Epoch: [312][   10/   13]    Loss 1.365656    Top1 45.000000    Top5 97.500000    
2022-12-29 10:37:29,117 - Epoch: [312][   13/   13]    Loss 1.316913    Top1 46.500000    Top5 98.000000    
2022-12-29 10:37:29,181 - ==> Top1: 46.500    Top5: 98.000    Loss: 1.317

2022-12-29 10:37:29,182 - ==> Confusion:
[[21  4  1 11  2  0]
 [ 3 12  9  9  5  1]
 [ 0  4 11  6  6  0]
 [ 3  2  1 37  5  1]
 [ 3  6  3 11 11  0]
 [ 0  0  5  3  3  1]]

2022-12-29 10:37:29,186 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:29,186 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:29,201 - 

2022-12-29 10:37:29,201 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:30,029 - Epoch: [313][   10/  113]    Overall Loss 1.338633    Objective Loss 1.338633                                        LR 0.000006    Time 0.082641    
2022-12-29 10:37:30,667 - Epoch: [313][   20/  113]    Overall Loss 1.303456    Objective Loss 1.303456                                        LR 0.000006    Time 0.073208    
2022-12-29 10:37:31,302 - Epoch: [313][   30/  113]    Overall Loss 1.311013    Objective Loss 1.311013                                        LR 0.000006    Time 0.069971    
2022-12-29 10:37:31,940 - Epoch: [313][   40/  113]    Overall Loss 1.284684    Objective Loss 1.284684                                        LR 0.000006    Time 0.068398    
2022-12-29 10:37:32,568 - Epoch: [313][   50/  113]    Overall Loss 1.291546    Objective Loss 1.291546                                        LR 0.000006    Time 0.067276    
2022-12-29 10:37:33,205 - Epoch: [313][   60/  113]    Overall Loss 1.307767    Objective Loss 1.307767                                        LR 0.000006    Time 0.066666    
2022-12-29 10:37:33,839 - Epoch: [313][   70/  113]    Overall Loss 1.315366    Objective Loss 1.315366                                        LR 0.000006    Time 0.066195    
2022-12-29 10:37:34,470 - Epoch: [313][   80/  113]    Overall Loss 1.292731    Objective Loss 1.292731                                        LR 0.000006    Time 0.065800    
2022-12-29 10:37:35,106 - Epoch: [313][   90/  113]    Overall Loss 1.295567    Objective Loss 1.295567                                        LR 0.000006    Time 0.065558    
2022-12-29 10:37:35,740 - Epoch: [313][  100/  113]    Overall Loss 1.286478    Objective Loss 1.286478                                        LR 0.000006    Time 0.065333    
2022-12-29 10:37:36,370 - Epoch: [313][  110/  113]    Overall Loss 1.290488    Objective Loss 1.290488                                        LR 0.000006    Time 0.065118    
2022-12-29 10:37:36,538 - Epoch: [313][  113/  113]    Overall Loss 1.285737    Objective Loss 1.285737    Top1 50.000000    Top5 100.000000    LR 0.000006    Time 0.064873    
2022-12-29 10:37:36,601 - --- validate (epoch=313)-----------
2022-12-29 10:37:36,601 - 200 samples (16 per mini-batch)
2022-12-29 10:37:37,139 - Epoch: [313][   10/   13]    Loss 1.225145    Top1 51.875000    Top5 97.500000    
2022-12-29 10:37:37,222 - Epoch: [313][   13/   13]    Loss 1.217238    Top1 53.000000    Top5 98.000000    
2022-12-29 10:37:37,283 - ==> Top1: 53.000    Top5: 98.000    Loss: 1.217

2022-12-29 10:37:37,284 - ==> Confusion:
[[21  1  0  6  4  0]
 [ 1 21  1  9  9  1]
 [ 1  8  9  6  7  0]
 [ 4  3  3 38  1  0]
 [ 1  5  3 11 17  0]
 [ 3  0  0  1  5  0]]

2022-12-29 10:37:37,287 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:37,287 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:37,309 - 

2022-12-29 10:37:37,310 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:38,155 - Epoch: [314][   10/  113]    Overall Loss 1.294186    Objective Loss 1.294186                                        LR 0.000006    Time 0.084324    
2022-12-29 10:37:38,794 - Epoch: [314][   20/  113]    Overall Loss 1.268473    Objective Loss 1.268473                                        LR 0.000006    Time 0.074098    
2022-12-29 10:37:39,430 - Epoch: [314][   30/  113]    Overall Loss 1.269980    Objective Loss 1.269980                                        LR 0.000006    Time 0.070577    
2022-12-29 10:37:40,065 - Epoch: [314][   40/  113]    Overall Loss 1.280964    Objective Loss 1.280964                                        LR 0.000006    Time 0.068802    
2022-12-29 10:37:40,705 - Epoch: [314][   50/  113]    Overall Loss 1.272776    Objective Loss 1.272776                                        LR 0.000006    Time 0.067827    
2022-12-29 10:37:41,340 - Epoch: [314][   60/  113]    Overall Loss 1.244815    Objective Loss 1.244815                                        LR 0.000006    Time 0.067094    
2022-12-29 10:37:41,975 - Epoch: [314][   70/  113]    Overall Loss 1.247934    Objective Loss 1.247934                                        LR 0.000006    Time 0.066572    
2022-12-29 10:37:42,611 - Epoch: [314][   80/  113]    Overall Loss 1.248022    Objective Loss 1.248022                                        LR 0.000006    Time 0.066203    
2022-12-29 10:37:43,246 - Epoch: [314][   90/  113]    Overall Loss 1.239734    Objective Loss 1.239734                                        LR 0.000006    Time 0.065891    
2022-12-29 10:37:43,876 - Epoch: [314][  100/  113]    Overall Loss 1.231843    Objective Loss 1.231843                                        LR 0.000006    Time 0.065598    
2022-12-29 10:37:44,507 - Epoch: [314][  110/  113]    Overall Loss 1.230660    Objective Loss 1.230660                                        LR 0.000006    Time 0.065364    
2022-12-29 10:37:44,681 - Epoch: [314][  113/  113]    Overall Loss 1.234833    Objective Loss 1.234833    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065169    
2022-12-29 10:37:44,745 - --- validate (epoch=314)-----------
2022-12-29 10:37:44,746 - 200 samples (16 per mini-batch)
2022-12-29 10:37:45,292 - Epoch: [314][   10/   13]    Loss 1.180301    Top1 55.000000    Top5 97.500000    
2022-12-29 10:37:45,378 - Epoch: [314][   13/   13]    Loss 1.216750    Top1 53.000000    Top5 97.000000    
2022-12-29 10:37:45,441 - ==> Top1: 53.000    Top5: 97.000    Loss: 1.217

2022-12-29 10:37:45,442 - ==> Confusion:
[[18  3  0  5  2  0]
 [ 4 21  1  4  6  1]
 [ 1  7 11  5  6  1]
 [ 5  5  0 35  3  0]
 [ 3  4  4 11 19  0]
 [ 2  0  0  5  6  2]]

2022-12-29 10:37:45,445 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:45,445 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:45,468 - 

2022-12-29 10:37:45,469 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:46,312 - Epoch: [315][   10/  113]    Overall Loss 1.230831    Objective Loss 1.230831                                        LR 0.000006    Time 0.084231    
2022-12-29 10:37:46,949 - Epoch: [315][   20/  113]    Overall Loss 1.205223    Objective Loss 1.205223                                        LR 0.000006    Time 0.073910    
2022-12-29 10:37:47,594 - Epoch: [315][   30/  113]    Overall Loss 1.209723    Objective Loss 1.209723                                        LR 0.000006    Time 0.070753    
2022-12-29 10:37:48,229 - Epoch: [315][   40/  113]    Overall Loss 1.218175    Objective Loss 1.218175                                        LR 0.000006    Time 0.068933    
2022-12-29 10:37:48,868 - Epoch: [315][   50/  113]    Overall Loss 1.239400    Objective Loss 1.239400                                        LR 0.000006    Time 0.067923    
2022-12-29 10:37:49,502 - Epoch: [315][   60/  113]    Overall Loss 1.227960    Objective Loss 1.227960                                        LR 0.000006    Time 0.067154    
2022-12-29 10:37:50,136 - Epoch: [315][   70/  113]    Overall Loss 1.244879    Objective Loss 1.244879                                        LR 0.000006    Time 0.066617    
2022-12-29 10:37:50,767 - Epoch: [315][   80/  113]    Overall Loss 1.243421    Objective Loss 1.243421                                        LR 0.000006    Time 0.066171    
2022-12-29 10:37:51,401 - Epoch: [315][   90/  113]    Overall Loss 1.234013    Objective Loss 1.234013                                        LR 0.000006    Time 0.065856    
2022-12-29 10:37:52,030 - Epoch: [315][  100/  113]    Overall Loss 1.221726    Objective Loss 1.221726                                        LR 0.000006    Time 0.065557    
2022-12-29 10:37:52,655 - Epoch: [315][  110/  113]    Overall Loss 1.224566    Objective Loss 1.224566                                        LR 0.000006    Time 0.065276    
2022-12-29 10:37:52,831 - Epoch: [315][  113/  113]    Overall Loss 1.222305    Objective Loss 1.222305    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.065100    
2022-12-29 10:37:52,879 - --- validate (epoch=315)-----------
2022-12-29 10:37:52,880 - 200 samples (16 per mini-batch)
2022-12-29 10:37:53,436 - Epoch: [315][   10/   13]    Loss 1.251908    Top1 48.125000    Top5 98.125000    
2022-12-29 10:37:53,521 - Epoch: [315][   13/   13]    Loss 1.240337    Top1 47.000000    Top5 98.000000    
2022-12-29 10:37:53,587 - ==> Top1: 47.000    Top5: 98.000    Loss: 1.240

2022-12-29 10:37:53,588 - ==> Confusion:
[[13  2  1  6  0  0]
 [ 2 21  0  6 12  0]
 [ 1 10 11  5 12  0]
 [ 2  5  1 32 10  0]
 [ 3  8  4  9 15  0]
 [ 1  3  0  1  2  2]]

2022-12-29 10:37:53,590 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:37:53,591 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:37:53,613 - 

2022-12-29 10:37:53,614 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:37:54,446 - Epoch: [316][   10/  113]    Overall Loss 1.266398    Objective Loss 1.266398                                        LR 0.000006    Time 0.083107    
2022-12-29 10:37:55,084 - Epoch: [316][   20/  113]    Overall Loss 1.259659    Objective Loss 1.259659                                        LR 0.000006    Time 0.073430    
2022-12-29 10:37:55,724 - Epoch: [316][   30/  113]    Overall Loss 1.257035    Objective Loss 1.257035                                        LR 0.000006    Time 0.070237    
2022-12-29 10:37:56,359 - Epoch: [316][   40/  113]    Overall Loss 1.247558    Objective Loss 1.247558                                        LR 0.000006    Time 0.068553    
2022-12-29 10:37:56,998 - Epoch: [316][   50/  113]    Overall Loss 1.228497    Objective Loss 1.228497                                        LR 0.000006    Time 0.067604    
2022-12-29 10:37:57,632 - Epoch: [316][   60/  113]    Overall Loss 1.234756    Objective Loss 1.234756                                        LR 0.000006    Time 0.066894    
2022-12-29 10:37:58,264 - Epoch: [316][   70/  113]    Overall Loss 1.227552    Objective Loss 1.227552                                        LR 0.000006    Time 0.066371    
2022-12-29 10:37:58,899 - Epoch: [316][   80/  113]    Overall Loss 1.227123    Objective Loss 1.227123                                        LR 0.000006    Time 0.066002    
2022-12-29 10:37:59,532 - Epoch: [316][   90/  113]    Overall Loss 1.228008    Objective Loss 1.228008                                        LR 0.000006    Time 0.065693    
2022-12-29 10:38:00,169 - Epoch: [316][  100/  113]    Overall Loss 1.234805    Objective Loss 1.234805                                        LR 0.000006    Time 0.065496    
2022-12-29 10:38:00,791 - Epoch: [316][  110/  113]    Overall Loss 1.241906    Objective Loss 1.241906                                        LR 0.000006    Time 0.065193    
2022-12-29 10:38:00,967 - Epoch: [316][  113/  113]    Overall Loss 1.241296    Objective Loss 1.241296    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065015    
2022-12-29 10:38:01,026 - --- validate (epoch=316)-----------
2022-12-29 10:38:01,026 - 200 samples (16 per mini-batch)
2022-12-29 10:38:01,574 - Epoch: [316][   10/   13]    Loss 1.346045    Top1 44.375000    Top5 98.125000    
2022-12-29 10:38:01,663 - Epoch: [316][   13/   13]    Loss 1.262464    Top1 48.000000    Top5 98.500000    
2022-12-29 10:38:01,723 - ==> Top1: 48.000    Top5: 98.500    Loss: 1.262

2022-12-29 10:38:01,724 - ==> Confusion:
[[16  5  0  6  3  0]
 [ 5 17  4  9  4  0]
 [ 1 10 18  2  7  0]
 [ 5  8  3 29  5  0]
 [ 4  6  1 10 14  0]
 [ 2  0  2  1  1  2]]

2022-12-29 10:38:01,726 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:01,727 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:01,750 - 

2022-12-29 10:38:01,750 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:02,600 - Epoch: [317][   10/  113]    Overall Loss 1.322371    Objective Loss 1.322371                                        LR 0.000006    Time 0.084754    
2022-12-29 10:38:03,239 - Epoch: [317][   20/  113]    Overall Loss 1.241820    Objective Loss 1.241820                                        LR 0.000006    Time 0.074308    
2022-12-29 10:38:03,879 - Epoch: [317][   30/  113]    Overall Loss 1.204393    Objective Loss 1.204393                                        LR 0.000006    Time 0.070852    
2022-12-29 10:38:04,514 - Epoch: [317][   40/  113]    Overall Loss 1.228598    Objective Loss 1.228598                                        LR 0.000006    Time 0.068992    
2022-12-29 10:38:05,147 - Epoch: [317][   50/  113]    Overall Loss 1.203015    Objective Loss 1.203015                                        LR 0.000006    Time 0.067852    
2022-12-29 10:38:05,781 - Epoch: [317][   60/  113]    Overall Loss 1.224415    Objective Loss 1.224415                                        LR 0.000006    Time 0.067102    
2022-12-29 10:38:06,414 - Epoch: [317][   70/  113]    Overall Loss 1.230155    Objective Loss 1.230155                                        LR 0.000006    Time 0.066557    
2022-12-29 10:38:07,049 - Epoch: [317][   80/  113]    Overall Loss 1.221464    Objective Loss 1.221464                                        LR 0.000006    Time 0.066168    
2022-12-29 10:38:07,685 - Epoch: [317][   90/  113]    Overall Loss 1.240756    Objective Loss 1.240756                                        LR 0.000006    Time 0.065878    
2022-12-29 10:38:08,319 - Epoch: [317][  100/  113]    Overall Loss 1.245522    Objective Loss 1.245522                                        LR 0.000006    Time 0.065623    
2022-12-29 10:38:08,946 - Epoch: [317][  110/  113]    Overall Loss 1.261003    Objective Loss 1.261003                                        LR 0.000006    Time 0.065346    
2022-12-29 10:38:09,121 - Epoch: [317][  113/  113]    Overall Loss 1.270043    Objective Loss 1.270043    Top1 29.166667    Top5 91.666667    LR 0.000006    Time 0.065159    
2022-12-29 10:38:09,187 - --- validate (epoch=317)-----------
2022-12-29 10:38:09,187 - 200 samples (16 per mini-batch)
2022-12-29 10:38:09,732 - Epoch: [317][   10/   13]    Loss 1.279457    Top1 54.375000    Top5 93.750000    
2022-12-29 10:38:09,815 - Epoch: [317][   13/   13]    Loss 1.295929    Top1 53.000000    Top5 93.500000    
2022-12-29 10:38:09,862 - ==> Top1: 53.000    Top5: 93.500    Loss: 1.296

2022-12-29 10:38:09,863 - ==> Confusion:
[[28  3  1  7  2  0]
 [ 3 20  1  7  3  0]
 [ 1  7 11  6  9  0]
 [ 6  2  2 30  4  1]
 [ 1  2  3  6 15  3]
 [ 6  1  0  4  3  2]]

2022-12-29 10:38:09,866 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:09,867 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:09,886 - 

2022-12-29 10:38:09,887 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:10,715 - Epoch: [318][   10/  113]    Overall Loss 1.411598    Objective Loss 1.411598                                        LR 0.000006    Time 0.082595    
2022-12-29 10:38:11,355 - Epoch: [318][   20/  113]    Overall Loss 1.351428    Objective Loss 1.351428                                        LR 0.000006    Time 0.073260    
2022-12-29 10:38:11,989 - Epoch: [318][   30/  113]    Overall Loss 1.342171    Objective Loss 1.342171                                        LR 0.000006    Time 0.069960    
2022-12-29 10:38:12,623 - Epoch: [318][   40/  113]    Overall Loss 1.292922    Objective Loss 1.292922                                        LR 0.000006    Time 0.068314    
2022-12-29 10:38:13,261 - Epoch: [318][   50/  113]    Overall Loss 1.277845    Objective Loss 1.277845                                        LR 0.000006    Time 0.067401    
2022-12-29 10:38:13,895 - Epoch: [318][   60/  113]    Overall Loss 1.271541    Objective Loss 1.271541                                        LR 0.000006    Time 0.066722    
2022-12-29 10:38:14,529 - Epoch: [318][   70/  113]    Overall Loss 1.269409    Objective Loss 1.269409                                        LR 0.000006    Time 0.066249    
2022-12-29 10:38:15,158 - Epoch: [318][   80/  113]    Overall Loss 1.267821    Objective Loss 1.267821                                        LR 0.000006    Time 0.065824    
2022-12-29 10:38:15,795 - Epoch: [318][   90/  113]    Overall Loss 1.266236    Objective Loss 1.266236                                        LR 0.000006    Time 0.065577    
2022-12-29 10:38:16,427 - Epoch: [318][  100/  113]    Overall Loss 1.255633    Objective Loss 1.255633                                        LR 0.000006    Time 0.065335    
2022-12-29 10:38:17,059 - Epoch: [318][  110/  113]    Overall Loss 1.262388    Objective Loss 1.262388                                        LR 0.000006    Time 0.065141    
2022-12-29 10:38:17,226 - Epoch: [318][  113/  113]    Overall Loss 1.266189    Objective Loss 1.266189    Top1 50.000000    Top5 91.666667    LR 0.000006    Time 0.064887    
2022-12-29 10:38:17,281 - --- validate (epoch=318)-----------
2022-12-29 10:38:17,281 - 200 samples (16 per mini-batch)
2022-12-29 10:38:17,824 - Epoch: [318][   10/   13]    Loss 1.305043    Top1 50.625000    Top5 96.875000    
2022-12-29 10:38:17,909 - Epoch: [318][   13/   13]    Loss 1.322569    Top1 48.500000    Top5 96.500000    
2022-12-29 10:38:17,970 - ==> Top1: 48.500    Top5: 96.500    Loss: 1.323

2022-12-29 10:38:17,971 - ==> Confusion:
[[19  4  0  7  1  0]
 [ 3 26  4  3  4  1]
 [ 2  7  9  6  7  0]
 [ 5  9  3 27  1  2]
 [ 3 13  5  6 15  0]
 [ 2  2  1  1  1  1]]

2022-12-29 10:38:17,977 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:17,977 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:17,999 - 

2022-12-29 10:38:17,999 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:18,828 - Epoch: [319][   10/  113]    Overall Loss 1.190392    Objective Loss 1.190392                                        LR 0.000006    Time 0.082740    
2022-12-29 10:38:19,464 - Epoch: [319][   20/  113]    Overall Loss 1.179549    Objective Loss 1.179549                                        LR 0.000006    Time 0.072920    
2022-12-29 10:38:20,100 - Epoch: [319][   30/  113]    Overall Loss 1.224556    Objective Loss 1.224556                                        LR 0.000006    Time 0.069791    
2022-12-29 10:38:20,735 - Epoch: [319][   40/  113]    Overall Loss 1.241147    Objective Loss 1.241147                                        LR 0.000006    Time 0.068224    
2022-12-29 10:38:21,368 - Epoch: [319][   50/  113]    Overall Loss 1.241613    Objective Loss 1.241613                                        LR 0.000006    Time 0.067215    
2022-12-29 10:38:22,001 - Epoch: [319][   60/  113]    Overall Loss 1.228029    Objective Loss 1.228029                                        LR 0.000006    Time 0.066559    
2022-12-29 10:38:22,637 - Epoch: [319][   70/  113]    Overall Loss 1.218297    Objective Loss 1.218297                                        LR 0.000006    Time 0.066129    
2022-12-29 10:38:23,268 - Epoch: [319][   80/  113]    Overall Loss 1.221740    Objective Loss 1.221740                                        LR 0.000006    Time 0.065741    
2022-12-29 10:38:23,901 - Epoch: [319][   90/  113]    Overall Loss 1.235009    Objective Loss 1.235009                                        LR 0.000006    Time 0.065465    
2022-12-29 10:38:24,532 - Epoch: [319][  100/  113]    Overall Loss 1.235627    Objective Loss 1.235627                                        LR 0.000006    Time 0.065226    
2022-12-29 10:38:25,160 - Epoch: [319][  110/  113]    Overall Loss 1.231619    Objective Loss 1.231619                                        LR 0.000006    Time 0.065001    
2022-12-29 10:38:25,335 - Epoch: [319][  113/  113]    Overall Loss 1.230312    Objective Loss 1.230312    Top1 62.500000    Top5 95.833333    LR 0.000006    Time 0.064821    
2022-12-29 10:38:25,400 - --- validate (epoch=319)-----------
2022-12-29 10:38:25,400 - 200 samples (16 per mini-batch)
2022-12-29 10:38:25,960 - Epoch: [319][   10/   13]    Loss 1.220434    Top1 52.500000    Top5 99.375000    
2022-12-29 10:38:26,045 - Epoch: [319][   13/   13]    Loss 1.210777    Top1 52.500000    Top5 99.500000    
2022-12-29 10:38:26,094 - ==> Top1: 52.500    Top5: 99.500    Loss: 1.211

2022-12-29 10:38:26,094 - ==> Confusion:
[[24  6  0  6  2  0]
 [ 0 20  3  8  5  1]
 [ 0 12 13  4  7  0]
 [ 4  4  2 32  2  0]
 [ 2  7  0 10 15  1]
 [ 1  2  0  2  4  1]]

2022-12-29 10:38:26,098 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:26,098 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:26,109 - 

2022-12-29 10:38:26,109 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:26,944 - Epoch: [320][   10/  113]    Overall Loss 1.290023    Objective Loss 1.290023                                        LR 0.000006    Time 0.083361    
2022-12-29 10:38:27,580 - Epoch: [320][   20/  113]    Overall Loss 1.248629    Objective Loss 1.248629                                        LR 0.000006    Time 0.073486    
2022-12-29 10:38:28,219 - Epoch: [320][   30/  113]    Overall Loss 1.222529    Objective Loss 1.222529                                        LR 0.000006    Time 0.070271    
2022-12-29 10:38:28,858 - Epoch: [320][   40/  113]    Overall Loss 1.242456    Objective Loss 1.242456                                        LR 0.000006    Time 0.068663    
2022-12-29 10:38:29,494 - Epoch: [320][   50/  113]    Overall Loss 1.244327    Objective Loss 1.244327                                        LR 0.000006    Time 0.067636    
2022-12-29 10:38:30,127 - Epoch: [320][   60/  113]    Overall Loss 1.248647    Objective Loss 1.248647                                        LR 0.000006    Time 0.066910    
2022-12-29 10:38:30,764 - Epoch: [320][   70/  113]    Overall Loss 1.256090    Objective Loss 1.256090                                        LR 0.000006    Time 0.066444    
2022-12-29 10:38:31,393 - Epoch: [320][   80/  113]    Overall Loss 1.260357    Objective Loss 1.260357                                        LR 0.000006    Time 0.065989    
2022-12-29 10:38:32,023 - Epoch: [320][   90/  113]    Overall Loss 1.255336    Objective Loss 1.255336                                        LR 0.000006    Time 0.065658    
2022-12-29 10:38:32,655 - Epoch: [320][  100/  113]    Overall Loss 1.262698    Objective Loss 1.262698                                        LR 0.000006    Time 0.065403    
2022-12-29 10:38:33,287 - Epoch: [320][  110/  113]    Overall Loss 1.262626    Objective Loss 1.262626                                        LR 0.000006    Time 0.065196    
2022-12-29 10:38:33,460 - Epoch: [320][  113/  113]    Overall Loss 1.265375    Objective Loss 1.265375    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.064997    
2022-12-29 10:38:33,510 - --- validate (epoch=320)-----------
2022-12-29 10:38:33,511 - 200 samples (16 per mini-batch)
2022-12-29 10:38:34,046 - Epoch: [320][   10/   13]    Loss 1.369284    Top1 53.750000    Top5 95.625000    
2022-12-29 10:38:34,132 - Epoch: [320][   13/   13]    Loss 1.413489    Top1 51.000000    Top5 95.500000    
2022-12-29 10:38:34,177 - ==> Top1: 51.000    Top5: 95.500    Loss: 1.413

2022-12-29 10:38:34,178 - ==> Confusion:
[[14  4  2 11  3  1]
 [ 3 27  7  4  7  0]
 [ 0  7 16  3  7  0]
 [ 2  6  1 24  2  0]
 [ 1  6  3  8 18  0]
 [ 0  2  2  2  4  3]]

2022-12-29 10:38:34,182 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:34,182 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:34,204 - 

2022-12-29 10:38:34,204 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:35,041 - Epoch: [321][   10/  113]    Overall Loss 1.311973    Objective Loss 1.311973                                        LR 0.000006    Time 0.083568    
2022-12-29 10:38:35,676 - Epoch: [321][   20/  113]    Overall Loss 1.318290    Objective Loss 1.318290                                        LR 0.000006    Time 0.073496    
2022-12-29 10:38:36,316 - Epoch: [321][   30/  113]    Overall Loss 1.267367    Objective Loss 1.267367                                        LR 0.000006    Time 0.070317    
2022-12-29 10:38:36,944 - Epoch: [321][   40/  113]    Overall Loss 1.278212    Objective Loss 1.278212                                        LR 0.000006    Time 0.068432    
2022-12-29 10:38:37,579 - Epoch: [321][   50/  113]    Overall Loss 1.280996    Objective Loss 1.280996                                        LR 0.000006    Time 0.067441    
2022-12-29 10:38:38,208 - Epoch: [321][   60/  113]    Overall Loss 1.281248    Objective Loss 1.281248                                        LR 0.000006    Time 0.066668    
2022-12-29 10:38:38,840 - Epoch: [321][   70/  113]    Overall Loss 1.279114    Objective Loss 1.279114                                        LR 0.000006    Time 0.066169    
2022-12-29 10:38:39,470 - Epoch: [321][   80/  113]    Overall Loss 1.251419    Objective Loss 1.251419                                        LR 0.000006    Time 0.065765    
2022-12-29 10:38:40,102 - Epoch: [321][   90/  113]    Overall Loss 1.251848    Objective Loss 1.251848                                        LR 0.000006    Time 0.065478    
2022-12-29 10:38:40,730 - Epoch: [321][  100/  113]    Overall Loss 1.250556    Objective Loss 1.250556                                        LR 0.000006    Time 0.065208    
2022-12-29 10:38:41,358 - Epoch: [321][  110/  113]    Overall Loss 1.251144    Objective Loss 1.251144                                        LR 0.000006    Time 0.064983    
2022-12-29 10:38:41,533 - Epoch: [321][  113/  113]    Overall Loss 1.254962    Objective Loss 1.254962    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064800    
2022-12-29 10:38:41,584 - --- validate (epoch=321)-----------
2022-12-29 10:38:41,585 - 200 samples (16 per mini-batch)
2022-12-29 10:38:42,124 - Epoch: [321][   10/   13]    Loss 1.377763    Top1 48.750000    Top5 97.500000    
2022-12-29 10:38:42,210 - Epoch: [321][   13/   13]    Loss 1.393992    Top1 49.500000    Top5 97.500000    
2022-12-29 10:38:42,258 - ==> Top1: 49.500    Top5: 97.500    Loss: 1.394

2022-12-29 10:38:42,258 - ==> Confusion:
[[29  1  2  7  2  0]
 [ 4 16  3  6  5  0]
 [ 2 10 12  5 12  0]
 [ 5  6  0 22  3  1]
 [ 4  8  2  5 18  0]
 [ 0  1  2  3  2  2]]

2022-12-29 10:38:42,260 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:42,260 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:42,279 - 

2022-12-29 10:38:42,280 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:43,133 - Epoch: [322][   10/  113]    Overall Loss 1.296755    Objective Loss 1.296755                                        LR 0.000006    Time 0.085210    
2022-12-29 10:38:43,769 - Epoch: [322][   20/  113]    Overall Loss 1.266386    Objective Loss 1.266386                                        LR 0.000006    Time 0.074349    
2022-12-29 10:38:44,401 - Epoch: [322][   30/  113]    Overall Loss 1.243300    Objective Loss 1.243300                                        LR 0.000006    Time 0.070625    
2022-12-29 10:38:45,035 - Epoch: [322][   40/  113]    Overall Loss 1.246234    Objective Loss 1.246234                                        LR 0.000006    Time 0.068804    
2022-12-29 10:38:45,668 - Epoch: [322][   50/  113]    Overall Loss 1.227115    Objective Loss 1.227115                                        LR 0.000006    Time 0.067688    
2022-12-29 10:38:46,301 - Epoch: [322][   60/  113]    Overall Loss 1.224782    Objective Loss 1.224782                                        LR 0.000006    Time 0.066952    
2022-12-29 10:38:46,937 - Epoch: [322][   70/  113]    Overall Loss 1.239265    Objective Loss 1.239265                                        LR 0.000006    Time 0.066470    
2022-12-29 10:38:47,572 - Epoch: [322][   80/  113]    Overall Loss 1.231337    Objective Loss 1.231337                                        LR 0.000006    Time 0.066089    
2022-12-29 10:38:48,204 - Epoch: [322][   90/  113]    Overall Loss 1.222338    Objective Loss 1.222338                                        LR 0.000006    Time 0.065771    
2022-12-29 10:38:48,839 - Epoch: [322][  100/  113]    Overall Loss 1.230872    Objective Loss 1.230872                                        LR 0.000006    Time 0.065535    
2022-12-29 10:38:49,467 - Epoch: [322][  110/  113]    Overall Loss 1.223944    Objective Loss 1.223944                                        LR 0.000006    Time 0.065282    
2022-12-29 10:38:49,646 - Epoch: [322][  113/  113]    Overall Loss 1.226649    Objective Loss 1.226649    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065126    
2022-12-29 10:38:49,696 - --- validate (epoch=322)-----------
2022-12-29 10:38:49,696 - 200 samples (16 per mini-batch)
2022-12-29 10:38:50,252 - Epoch: [322][   10/   13]    Loss 1.481870    Top1 36.250000    Top5 96.250000    
2022-12-29 10:38:50,336 - Epoch: [322][   13/   13]    Loss 1.404760    Top1 41.500000    Top5 96.500000    
2022-12-29 10:38:50,401 - ==> Top1: 41.500    Top5: 96.500    Loss: 1.405

2022-12-29 10:38:50,402 - ==> Confusion:
[[ 9  4  1  7  5  1]
 [ 3 19  6 10  7  1]
 [ 0 10  9  5  8  1]
 [ 4  2  0 34  2  0]
 [ 3 12  3 10 11  0]
 [ 0  4  1  1  6  1]]

2022-12-29 10:38:50,404 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:50,404 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:50,413 - 

2022-12-29 10:38:50,413 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:51,242 - Epoch: [323][   10/  113]    Overall Loss 1.395986    Objective Loss 1.395986                                        LR 0.000006    Time 0.082817    
2022-12-29 10:38:51,880 - Epoch: [323][   20/  113]    Overall Loss 1.308566    Objective Loss 1.308566                                        LR 0.000006    Time 0.073268    
2022-12-29 10:38:52,518 - Epoch: [323][   30/  113]    Overall Loss 1.280936    Objective Loss 1.280936                                        LR 0.000006    Time 0.070081    
2022-12-29 10:38:53,159 - Epoch: [323][   40/  113]    Overall Loss 1.266238    Objective Loss 1.266238                                        LR 0.000006    Time 0.068581    
2022-12-29 10:38:53,792 - Epoch: [323][   50/  113]    Overall Loss 1.278752    Objective Loss 1.278752                                        LR 0.000006    Time 0.067520    
2022-12-29 10:38:54,429 - Epoch: [323][   60/  113]    Overall Loss 1.267632    Objective Loss 1.267632                                        LR 0.000006    Time 0.066873    
2022-12-29 10:38:55,066 - Epoch: [323][   70/  113]    Overall Loss 1.252885    Objective Loss 1.252885                                        LR 0.000006    Time 0.066410    
2022-12-29 10:38:55,701 - Epoch: [323][   80/  113]    Overall Loss 1.235594    Objective Loss 1.235594                                        LR 0.000006    Time 0.066043    
2022-12-29 10:38:56,336 - Epoch: [323][   90/  113]    Overall Loss 1.243516    Objective Loss 1.243516                                        LR 0.000006    Time 0.065759    
2022-12-29 10:38:56,968 - Epoch: [323][  100/  113]    Overall Loss 1.239637    Objective Loss 1.239637                                        LR 0.000006    Time 0.065497    
2022-12-29 10:38:57,593 - Epoch: [323][  110/  113]    Overall Loss 1.240191    Objective Loss 1.240191                                        LR 0.000006    Time 0.065216    
2022-12-29 10:38:57,767 - Epoch: [323][  113/  113]    Overall Loss 1.242687    Objective Loss 1.242687    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.065018    
2022-12-29 10:38:57,830 - --- validate (epoch=323)-----------
2022-12-29 10:38:57,830 - 200 samples (16 per mini-batch)
2022-12-29 10:38:58,369 - Epoch: [323][   10/   13]    Loss 1.298677    Top1 43.750000    Top5 98.125000    
2022-12-29 10:38:58,456 - Epoch: [323][   13/   13]    Loss 1.311604    Top1 44.500000    Top5 97.500000    
2022-12-29 10:38:58,501 - ==> Top1: 44.500    Top5: 97.500    Loss: 1.312

2022-12-29 10:38:58,501 - ==> Confusion:
[[17  4  0  2  2  0]
 [ 3 22  2  6  4  2]
 [ 1 17  9  6  6  0]
 [ 3  4  3 25  5  1]
 [ 3 10  2  9 15  0]
 [ 4  5  1  2  4  1]]

2022-12-29 10:38:58,503 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:38:58,503 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:38:58,515 - 

2022-12-29 10:38:58,515 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:38:59,346 - Epoch: [324][   10/  113]    Overall Loss 1.279493    Objective Loss 1.279493                                        LR 0.000006    Time 0.082919    
2022-12-29 10:38:59,976 - Epoch: [324][   20/  113]    Overall Loss 1.258339    Objective Loss 1.258339                                        LR 0.000006    Time 0.072936    
2022-12-29 10:39:00,608 - Epoch: [324][   30/  113]    Overall Loss 1.251204    Objective Loss 1.251204                                        LR 0.000006    Time 0.069685    
2022-12-29 10:39:01,241 - Epoch: [324][   40/  113]    Overall Loss 1.229520    Objective Loss 1.229520                                        LR 0.000006    Time 0.068066    
2022-12-29 10:39:01,870 - Epoch: [324][   50/  113]    Overall Loss 1.222044    Objective Loss 1.222044                                        LR 0.000006    Time 0.067031    
2022-12-29 10:39:02,499 - Epoch: [324][   60/  113]    Overall Loss 1.233688    Objective Loss 1.233688                                        LR 0.000006    Time 0.066332    
2022-12-29 10:39:03,145 - Epoch: [324][   70/  113]    Overall Loss 1.245442    Objective Loss 1.245442                                        LR 0.000006    Time 0.066077    
2022-12-29 10:39:03,772 - Epoch: [324][   80/  113]    Overall Loss 1.234167    Objective Loss 1.234167                                        LR 0.000006    Time 0.065652    
2022-12-29 10:39:04,407 - Epoch: [324][   90/  113]    Overall Loss 1.231497    Objective Loss 1.231497                                        LR 0.000006    Time 0.065411    
2022-12-29 10:39:05,040 - Epoch: [324][  100/  113]    Overall Loss 1.229605    Objective Loss 1.229605                                        LR 0.000006    Time 0.065192    
2022-12-29 10:39:05,663 - Epoch: [324][  110/  113]    Overall Loss 1.242105    Objective Loss 1.242105                                        LR 0.000006    Time 0.064928    
2022-12-29 10:39:05,838 - Epoch: [324][  113/  113]    Overall Loss 1.245987    Objective Loss 1.245987    Top1 25.000000    Top5 91.666667    LR 0.000006    Time 0.064744    
2022-12-29 10:39:05,903 - --- validate (epoch=324)-----------
2022-12-29 10:39:05,903 - 200 samples (16 per mini-batch)
2022-12-29 10:39:06,462 - Epoch: [324][   10/   13]    Loss 1.217270    Top1 49.375000    Top5 97.500000    
2022-12-29 10:39:06,546 - Epoch: [324][   13/   13]    Loss 1.197318    Top1 50.000000    Top5 97.500000    
2022-12-29 10:39:06,595 - ==> Top1: 50.000    Top5: 97.500    Loss: 1.197

2022-12-29 10:39:06,596 - ==> Confusion:
[[21  5  0  6  0  0]
 [ 3 22  4  8  8  1]
 [ 0 10 16  8  3  0]
 [ 6  1  5 30  2  0]
 [ 5  7  6  3 11  0]
 [ 0  4  1  1  3  0]]

2022-12-29 10:39:06,601 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:06,601 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:06,622 - 

2022-12-29 10:39:06,622 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:07,458 - Epoch: [325][   10/  113]    Overall Loss 1.300013    Objective Loss 1.300013                                        LR 0.000006    Time 0.083492    
2022-12-29 10:39:08,092 - Epoch: [325][   20/  113]    Overall Loss 1.252933    Objective Loss 1.252933                                        LR 0.000006    Time 0.073404    
2022-12-29 10:39:08,722 - Epoch: [325][   30/  113]    Overall Loss 1.239022    Objective Loss 1.239022                                        LR 0.000006    Time 0.069937    
2022-12-29 10:39:09,359 - Epoch: [325][   40/  113]    Overall Loss 1.230690    Objective Loss 1.230690                                        LR 0.000006    Time 0.068354    
2022-12-29 10:39:09,994 - Epoch: [325][   50/  113]    Overall Loss 1.222349    Objective Loss 1.222349                                        LR 0.000006    Time 0.067374    
2022-12-29 10:39:10,627 - Epoch: [325][   60/  113]    Overall Loss 1.222819    Objective Loss 1.222819                                        LR 0.000006    Time 0.066695    
2022-12-29 10:39:11,265 - Epoch: [325][   70/  113]    Overall Loss 1.238460    Objective Loss 1.238460                                        LR 0.000006    Time 0.066268    
2022-12-29 10:39:11,895 - Epoch: [325][   80/  113]    Overall Loss 1.240018    Objective Loss 1.240018                                        LR 0.000006    Time 0.065863    
2022-12-29 10:39:12,531 - Epoch: [325][   90/  113]    Overall Loss 1.238361    Objective Loss 1.238361                                        LR 0.000006    Time 0.065600    
2022-12-29 10:39:13,168 - Epoch: [325][  100/  113]    Overall Loss 1.242460    Objective Loss 1.242460                                        LR 0.000006    Time 0.065410    
2022-12-29 10:39:13,797 - Epoch: [325][  110/  113]    Overall Loss 1.245188    Objective Loss 1.245188                                        LR 0.000006    Time 0.065174    
2022-12-29 10:39:13,974 - Epoch: [325][  113/  113]    Overall Loss 1.246725    Objective Loss 1.246725    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065002    
2022-12-29 10:39:14,015 - --- validate (epoch=325)-----------
2022-12-29 10:39:14,015 - 200 samples (16 per mini-batch)
2022-12-29 10:39:14,568 - Epoch: [325][   10/   13]    Loss 1.219952    Top1 51.250000    Top5 99.375000    
2022-12-29 10:39:14,653 - Epoch: [325][   13/   13]    Loss 1.304567    Top1 48.500000    Top5 99.500000    
2022-12-29 10:39:14,706 - ==> Top1: 48.500    Top5: 99.500    Loss: 1.305

2022-12-29 10:39:14,707 - ==> Confusion:
[[25  1  0 13  4  0]
 [ 3 17  3 12  7  0]
 [ 0  2  9 10  8  1]
 [ 4  2  2 29  2  0]
 [ 2  7  0  9 15  0]
 [ 0  2  1  4  4  2]]

2022-12-29 10:39:14,711 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:14,711 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:14,732 - 

2022-12-29 10:39:14,732 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:15,571 - Epoch: [326][   10/  113]    Overall Loss 1.181243    Objective Loss 1.181243                                        LR 0.000006    Time 0.083803    
2022-12-29 10:39:16,208 - Epoch: [326][   20/  113]    Overall Loss 1.211013    Objective Loss 1.211013                                        LR 0.000006    Time 0.073727    
2022-12-29 10:39:16,845 - Epoch: [326][   30/  113]    Overall Loss 1.214852    Objective Loss 1.214852                                        LR 0.000006    Time 0.070364    
2022-12-29 10:39:17,480 - Epoch: [326][   40/  113]    Overall Loss 1.223004    Objective Loss 1.223004                                        LR 0.000006    Time 0.068638    
2022-12-29 10:39:18,110 - Epoch: [326][   50/  113]    Overall Loss 1.244203    Objective Loss 1.244203                                        LR 0.000006    Time 0.067499    
2022-12-29 10:39:18,737 - Epoch: [326][   60/  113]    Overall Loss 1.235181    Objective Loss 1.235181                                        LR 0.000006    Time 0.066688    
2022-12-29 10:39:19,364 - Epoch: [326][   70/  113]    Overall Loss 1.250431    Objective Loss 1.250431                                        LR 0.000006    Time 0.066107    
2022-12-29 10:39:19,988 - Epoch: [326][   80/  113]    Overall Loss 1.246250    Objective Loss 1.246250                                        LR 0.000006    Time 0.065639    
2022-12-29 10:39:20,613 - Epoch: [326][   90/  113]    Overall Loss 1.238166    Objective Loss 1.238166                                        LR 0.000006    Time 0.065290    
2022-12-29 10:39:21,240 - Epoch: [326][  100/  113]    Overall Loss 1.240635    Objective Loss 1.240635                                        LR 0.000006    Time 0.065024    
2022-12-29 10:39:21,865 - Epoch: [326][  110/  113]    Overall Loss 1.239795    Objective Loss 1.239795                                        LR 0.000006    Time 0.064788    
2022-12-29 10:39:22,040 - Epoch: [326][  113/  113]    Overall Loss 1.239101    Objective Loss 1.239101    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.064618    
2022-12-29 10:39:22,103 - --- validate (epoch=326)-----------
2022-12-29 10:39:22,103 - 200 samples (16 per mini-batch)
2022-12-29 10:39:22,654 - Epoch: [326][   10/   13]    Loss 1.259013    Top1 51.250000    Top5 97.500000    
2022-12-29 10:39:22,739 - Epoch: [326][   13/   13]    Loss 1.287089    Top1 52.000000    Top5 97.000000    
2022-12-29 10:39:22,805 - ==> Top1: 52.000    Top5: 97.000    Loss: 1.287

2022-12-29 10:39:22,805 - ==> Confusion:
[[16  1  0  4  2  0]
 [ 2 29  3  7  1  0]
 [ 1  5 12  6  2  0]
 [ 8  6  2 32  5  0]
 [ 1 14  8  8 15  0]
 [ 3  2  1  2  2  0]]

2022-12-29 10:39:22,807 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:22,807 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:22,829 - 

2022-12-29 10:39:22,829 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:23,676 - Epoch: [327][   10/  113]    Overall Loss 1.275719    Objective Loss 1.275719                                        LR 0.000006    Time 0.084543    
2022-12-29 10:39:24,316 - Epoch: [327][   20/  113]    Overall Loss 1.209433    Objective Loss 1.209433                                        LR 0.000006    Time 0.074214    
2022-12-29 10:39:24,956 - Epoch: [327][   30/  113]    Overall Loss 1.235228    Objective Loss 1.235228                                        LR 0.000006    Time 0.070813    
2022-12-29 10:39:25,594 - Epoch: [327][   40/  113]    Overall Loss 1.244324    Objective Loss 1.244324                                        LR 0.000006    Time 0.069037    
2022-12-29 10:39:26,225 - Epoch: [327][   50/  113]    Overall Loss 1.244941    Objective Loss 1.244941                                        LR 0.000006    Time 0.067845    
2022-12-29 10:39:26,859 - Epoch: [327][   60/  113]    Overall Loss 1.248557    Objective Loss 1.248557                                        LR 0.000006    Time 0.067093    
2022-12-29 10:39:27,497 - Epoch: [327][   70/  113]    Overall Loss 1.249049    Objective Loss 1.249049                                        LR 0.000006    Time 0.066624    
2022-12-29 10:39:28,129 - Epoch: [327][   80/  113]    Overall Loss 1.235645    Objective Loss 1.235645                                        LR 0.000006    Time 0.066190    
2022-12-29 10:39:28,763 - Epoch: [327][   90/  113]    Overall Loss 1.245097    Objective Loss 1.245097                                        LR 0.000006    Time 0.065876    
2022-12-29 10:39:29,400 - Epoch: [327][  100/  113]    Overall Loss 1.260160    Objective Loss 1.260160                                        LR 0.000006    Time 0.065654    
2022-12-29 10:39:30,028 - Epoch: [327][  110/  113]    Overall Loss 1.259648    Objective Loss 1.259648                                        LR 0.000006    Time 0.065390    
2022-12-29 10:39:30,201 - Epoch: [327][  113/  113]    Overall Loss 1.267466    Objective Loss 1.267466    Top1 41.666667    Top5 91.666667    LR 0.000006    Time 0.065139    
2022-12-29 10:39:30,264 - --- validate (epoch=327)-----------
2022-12-29 10:39:30,265 - 200 samples (16 per mini-batch)
2022-12-29 10:39:30,826 - Epoch: [327][   10/   13]    Loss 1.199620    Top1 51.875000    Top5 98.750000    
2022-12-29 10:39:30,911 - Epoch: [327][   13/   13]    Loss 1.214345    Top1 50.000000    Top5 99.000000    
2022-12-29 10:39:30,957 - ==> Top1: 50.000    Top5: 99.000    Loss: 1.214

2022-12-29 10:39:30,957 - ==> Confusion:
[[14  4  0  2  1  0]
 [ 3 31  4  5  2  0]
 [ 1 17  8  5  5  0]
 [ 6  2  1 37 10  0]
 [ 1 12  2  9  9  0]
 [ 1  2  1  3  1  1]]

2022-12-29 10:39:30,959 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:30,959 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:30,979 - 

2022-12-29 10:39:30,979 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:31,812 - Epoch: [328][   10/  113]    Overall Loss 1.377392    Objective Loss 1.377392                                        LR 0.000006    Time 0.083150    
2022-12-29 10:39:32,445 - Epoch: [328][   20/  113]    Overall Loss 1.302840    Objective Loss 1.302840                                        LR 0.000006    Time 0.073235    
2022-12-29 10:39:33,085 - Epoch: [328][   30/  113]    Overall Loss 1.235147    Objective Loss 1.235147                                        LR 0.000006    Time 0.070124    
2022-12-29 10:39:33,717 - Epoch: [328][   40/  113]    Overall Loss 1.224204    Objective Loss 1.224204                                        LR 0.000006    Time 0.068379    
2022-12-29 10:39:34,348 - Epoch: [328][   50/  113]    Overall Loss 1.212089    Objective Loss 1.212089                                        LR 0.000006    Time 0.067324    
2022-12-29 10:39:34,980 - Epoch: [328][   60/  113]    Overall Loss 1.235277    Objective Loss 1.235277                                        LR 0.000006    Time 0.066629    
2022-12-29 10:39:35,614 - Epoch: [328][   70/  113]    Overall Loss 1.259679    Objective Loss 1.259679                                        LR 0.000006    Time 0.066164    
2022-12-29 10:39:36,244 - Epoch: [328][   80/  113]    Overall Loss 1.254972    Objective Loss 1.254972                                        LR 0.000006    Time 0.065761    
2022-12-29 10:39:36,879 - Epoch: [328][   90/  113]    Overall Loss 1.254931    Objective Loss 1.254931                                        LR 0.000006    Time 0.065502    
2022-12-29 10:39:37,512 - Epoch: [328][  100/  113]    Overall Loss 1.259682    Objective Loss 1.259682                                        LR 0.000006    Time 0.065278    
2022-12-29 10:39:38,138 - Epoch: [328][  110/  113]    Overall Loss 1.270963    Objective Loss 1.270963                                        LR 0.000006    Time 0.065029    
2022-12-29 10:39:38,311 - Epoch: [328][  113/  113]    Overall Loss 1.269433    Objective Loss 1.269433    Top1 66.666667    Top5 100.000000    LR 0.000006    Time 0.064829    
2022-12-29 10:39:38,369 - --- validate (epoch=328)-----------
2022-12-29 10:39:38,370 - 200 samples (16 per mini-batch)
2022-12-29 10:39:38,918 - Epoch: [328][   10/   13]    Loss 1.285101    Top1 47.500000    Top5 98.750000    
2022-12-29 10:39:39,003 - Epoch: [328][   13/   13]    Loss 1.234921    Top1 51.000000    Top5 99.000000    
2022-12-29 10:39:39,060 - ==> Top1: 51.000    Top5: 99.000    Loss: 1.235

2022-12-29 10:39:39,061 - ==> Confusion:
[[25  2  0  4  3  0]
 [ 2 22  1  3 15  1]
 [ 2  8  7  5  6  1]
 [ 6  3  0 35  4  1]
 [ 2  6  3  7 13  0]
 [ 1  6  2  0  4  0]]

2022-12-29 10:39:39,064 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:39,065 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:39,088 - 

2022-12-29 10:39:39,088 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:39,930 - Epoch: [329][   10/  113]    Overall Loss 1.244156    Objective Loss 1.244156                                        LR 0.000006    Time 0.084053    
2022-12-29 10:39:40,565 - Epoch: [329][   20/  113]    Overall Loss 1.217663    Objective Loss 1.217663                                        LR 0.000006    Time 0.073771    
2022-12-29 10:39:41,200 - Epoch: [329][   30/  113]    Overall Loss 1.261407    Objective Loss 1.261407                                        LR 0.000006    Time 0.070340    
2022-12-29 10:39:41,842 - Epoch: [329][   40/  113]    Overall Loss 1.268307    Objective Loss 1.268307                                        LR 0.000006    Time 0.068788    
2022-12-29 10:39:42,477 - Epoch: [329][   50/  113]    Overall Loss 1.263584    Objective Loss 1.263584                                        LR 0.000006    Time 0.067720    
2022-12-29 10:39:43,123 - Epoch: [329][   60/  113]    Overall Loss 1.268917    Objective Loss 1.268917                                        LR 0.000006    Time 0.067193    
2022-12-29 10:39:43,761 - Epoch: [329][   70/  113]    Overall Loss 1.257998    Objective Loss 1.257998                                        LR 0.000006    Time 0.066703    
2022-12-29 10:39:44,396 - Epoch: [329][   80/  113]    Overall Loss 1.258595    Objective Loss 1.258595                                        LR 0.000006    Time 0.066299    
2022-12-29 10:39:45,036 - Epoch: [329][   90/  113]    Overall Loss 1.254412    Objective Loss 1.254412                                        LR 0.000006    Time 0.066029    
2022-12-29 10:39:45,675 - Epoch: [329][  100/  113]    Overall Loss 1.259416    Objective Loss 1.259416                                        LR 0.000006    Time 0.065814    
2022-12-29 10:39:46,301 - Epoch: [329][  110/  113]    Overall Loss 1.278028    Objective Loss 1.278028                                        LR 0.000006    Time 0.065522    
2022-12-29 10:39:46,477 - Epoch: [329][  113/  113]    Overall Loss 1.279699    Objective Loss 1.279699    Top1 33.333333    Top5 100.000000    LR 0.000006    Time 0.065335    
2022-12-29 10:39:46,534 - --- validate (epoch=329)-----------
2022-12-29 10:39:46,535 - 200 samples (16 per mini-batch)
2022-12-29 10:39:47,082 - Epoch: [329][   10/   13]    Loss 1.229242    Top1 51.875000    Top5 99.375000    
2022-12-29 10:39:47,166 - Epoch: [329][   13/   13]    Loss 1.244499    Top1 51.000000    Top5 99.000000    
2022-12-29 10:39:47,224 - ==> Top1: 51.000    Top5: 99.000    Loss: 1.244

2022-12-29 10:39:47,224 - ==> Confusion:
[[20  4  2  4  2  0]
 [ 1 24  5  2  5  2]
 [ 2 13  8  2  3  0]
 [ 5  4  1 26  8  0]
 [ 1 12  2  6 20  2]
 [ 3  0  2  3  2  4]]

2022-12-29 10:39:47,226 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:47,227 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:47,249 - 

2022-12-29 10:39:47,249 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:48,073 - Epoch: [330][   10/  113]    Overall Loss 1.294062    Objective Loss 1.294062                                        LR 0.000006    Time 0.082368    
2022-12-29 10:39:48,712 - Epoch: [330][   20/  113]    Overall Loss 1.277186    Objective Loss 1.277186                                        LR 0.000006    Time 0.073095    
2022-12-29 10:39:49,347 - Epoch: [330][   30/  113]    Overall Loss 1.251112    Objective Loss 1.251112                                        LR 0.000006    Time 0.069885    
2022-12-29 10:39:49,983 - Epoch: [330][   40/  113]    Overall Loss 1.228002    Objective Loss 1.228002                                        LR 0.000006    Time 0.068304    
2022-12-29 10:39:50,616 - Epoch: [330][   50/  113]    Overall Loss 1.263991    Objective Loss 1.263991                                        LR 0.000006    Time 0.067285    
2022-12-29 10:39:51,249 - Epoch: [330][   60/  113]    Overall Loss 1.269565    Objective Loss 1.269565                                        LR 0.000006    Time 0.066608    
2022-12-29 10:39:51,883 - Epoch: [330][   70/  113]    Overall Loss 1.274378    Objective Loss 1.274378                                        LR 0.000006    Time 0.066149    
2022-12-29 10:39:52,517 - Epoch: [330][   80/  113]    Overall Loss 1.270317    Objective Loss 1.270317                                        LR 0.000006    Time 0.065804    
2022-12-29 10:39:53,151 - Epoch: [330][   90/  113]    Overall Loss 1.271683    Objective Loss 1.271683                                        LR 0.000006    Time 0.065524    
2022-12-29 10:39:53,785 - Epoch: [330][  100/  113]    Overall Loss 1.268784    Objective Loss 1.268784                                        LR 0.000006    Time 0.065313    
2022-12-29 10:39:54,406 - Epoch: [330][  110/  113]    Overall Loss 1.275059    Objective Loss 1.275059                                        LR 0.000006    Time 0.065018    
2022-12-29 10:39:54,580 - Epoch: [330][  113/  113]    Overall Loss 1.273583    Objective Loss 1.273583    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064821    
2022-12-29 10:39:54,638 - --- validate (epoch=330)-----------
2022-12-29 10:39:54,639 - 200 samples (16 per mini-batch)
2022-12-29 10:39:55,188 - Epoch: [330][   10/   13]    Loss 1.175422    Top1 53.750000    Top5 97.500000    
2022-12-29 10:39:55,273 - Epoch: [330][   13/   13]    Loss 1.193024    Top1 54.000000    Top5 98.000000    
2022-12-29 10:39:55,326 - ==> Top1: 54.000    Top5: 98.000    Loss: 1.193

2022-12-29 10:39:55,326 - ==> Confusion:
[[27  1  0  7  4  0]
 [ 2 15  2  4  8  1]
 [ 1  3 10  2 10  0]
 [ 6  3  3 35  3  0]
 [ 4  7  6  9 21  0]
 [ 2  1  0  1  2  0]]

2022-12-29 10:39:55,331 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:39:55,331 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:39:55,345 - 

2022-12-29 10:39:55,345 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:39:56,167 - Epoch: [331][   10/  113]    Overall Loss 1.244324    Objective Loss 1.244324                                        LR 0.000006    Time 0.082117    
2022-12-29 10:39:56,805 - Epoch: [331][   20/  113]    Overall Loss 1.203089    Objective Loss 1.203089                                        LR 0.000006    Time 0.072927    
2022-12-29 10:39:57,445 - Epoch: [331][   30/  113]    Overall Loss 1.208980    Objective Loss 1.208980                                        LR 0.000006    Time 0.069932    
2022-12-29 10:39:58,083 - Epoch: [331][   40/  113]    Overall Loss 1.222015    Objective Loss 1.222015                                        LR 0.000006    Time 0.068385    
2022-12-29 10:39:58,717 - Epoch: [331][   50/  113]    Overall Loss 1.221564    Objective Loss 1.221564                                        LR 0.000006    Time 0.067385    
2022-12-29 10:39:59,348 - Epoch: [331][   60/  113]    Overall Loss 1.210847    Objective Loss 1.210847                                        LR 0.000006    Time 0.066667    
2022-12-29 10:39:59,982 - Epoch: [331][   70/  113]    Overall Loss 1.194483    Objective Loss 1.194483                                        LR 0.000006    Time 0.066185    
2022-12-29 10:40:00,611 - Epoch: [331][   80/  113]    Overall Loss 1.190074    Objective Loss 1.190074                                        LR 0.000006    Time 0.065776    
2022-12-29 10:40:01,246 - Epoch: [331][   90/  113]    Overall Loss 1.210597    Objective Loss 1.210597                                        LR 0.000006    Time 0.065511    
2022-12-29 10:40:01,881 - Epoch: [331][  100/  113]    Overall Loss 1.215951    Objective Loss 1.215951                                        LR 0.000006    Time 0.065310    
2022-12-29 10:40:02,510 - Epoch: [331][  110/  113]    Overall Loss 1.225203    Objective Loss 1.225203                                        LR 0.000006    Time 0.065083    
2022-12-29 10:40:02,686 - Epoch: [331][  113/  113]    Overall Loss 1.225651    Objective Loss 1.225651    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064909    
2022-12-29 10:40:02,743 - --- validate (epoch=331)-----------
2022-12-29 10:40:02,743 - 200 samples (16 per mini-batch)
2022-12-29 10:40:03,303 - Epoch: [331][   10/   13]    Loss 1.176141    Top1 54.375000    Top5 97.500000    
2022-12-29 10:40:03,387 - Epoch: [331][   13/   13]    Loss 1.179787    Top1 53.500000    Top5 97.500000    
2022-12-29 10:40:03,430 - ==> Top1: 53.500    Top5: 97.500    Loss: 1.180

2022-12-29 10:40:03,431 - ==> Confusion:
[[17  2  0  1  1  0]
 [ 1 25  3  7  7  0]
 [ 1 14 13  2  8  1]
 [ 3  9  0 35 11  0]
 [ 0  6  3  4 17  0]
 [ 1  5  1  0  2  0]]

2022-12-29 10:40:03,433 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:03,434 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:03,457 - 

2022-12-29 10:40:03,457 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:04,292 - Epoch: [332][   10/  113]    Overall Loss 1.300135    Objective Loss 1.300135                                        LR 0.000006    Time 0.083398    
2022-12-29 10:40:04,931 - Epoch: [332][   20/  113]    Overall Loss 1.277261    Objective Loss 1.277261                                        LR 0.000006    Time 0.073585    
2022-12-29 10:40:05,569 - Epoch: [332][   30/  113]    Overall Loss 1.266600    Objective Loss 1.266600                                        LR 0.000006    Time 0.070325    
2022-12-29 10:40:06,207 - Epoch: [332][   40/  113]    Overall Loss 1.269222    Objective Loss 1.269222                                        LR 0.000006    Time 0.068678    
2022-12-29 10:40:06,841 - Epoch: [332][   50/  113]    Overall Loss 1.256905    Objective Loss 1.256905                                        LR 0.000006    Time 0.067620    
2022-12-29 10:40:07,475 - Epoch: [332][   60/  113]    Overall Loss 1.265012    Objective Loss 1.265012                                        LR 0.000006    Time 0.066910    
2022-12-29 10:40:08,109 - Epoch: [332][   70/  113]    Overall Loss 1.245376    Objective Loss 1.245376                                        LR 0.000006    Time 0.066402    
2022-12-29 10:40:08,742 - Epoch: [332][   80/  113]    Overall Loss 1.245007    Objective Loss 1.245007                                        LR 0.000006    Time 0.066003    
2022-12-29 10:40:09,375 - Epoch: [332][   90/  113]    Overall Loss 1.246070    Objective Loss 1.246070                                        LR 0.000006    Time 0.065700    
2022-12-29 10:40:10,008 - Epoch: [332][  100/  113]    Overall Loss 1.245914    Objective Loss 1.245914                                        LR 0.000006    Time 0.065453    
2022-12-29 10:40:10,636 - Epoch: [332][  110/  113]    Overall Loss 1.253487    Objective Loss 1.253487                                        LR 0.000006    Time 0.065208    
2022-12-29 10:40:10,812 - Epoch: [332][  113/  113]    Overall Loss 1.255007    Objective Loss 1.255007    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.065028    
2022-12-29 10:40:10,878 - --- validate (epoch=332)-----------
2022-12-29 10:40:10,878 - 200 samples (16 per mini-batch)
2022-12-29 10:40:11,438 - Epoch: [332][   10/   13]    Loss 1.251332    Top1 50.625000    Top5 98.125000    
2022-12-29 10:40:11,526 - Epoch: [332][   13/   13]    Loss 1.274524    Top1 50.500000    Top5 98.000000    
2022-12-29 10:40:11,583 - ==> Top1: 50.500    Top5: 98.000    Loss: 1.275

2022-12-29 10:40:11,584 - ==> Confusion:
[[19  2  1  7  2  1]
 [ 2 22  4  7  6  0]
 [ 0  4 12  6  9  0]
 [ 6  6  1 29  4  0]
 [ 4  7  3  6 19  1]
 [ 1  1  2  2  4  0]]

2022-12-29 10:40:11,588 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:11,588 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:11,601 - 

2022-12-29 10:40:11,601 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:12,430 - Epoch: [333][   10/  113]    Overall Loss 1.175419    Objective Loss 1.175419                                        LR 0.000006    Time 0.082836    
2022-12-29 10:40:13,070 - Epoch: [333][   20/  113]    Overall Loss 1.232422    Objective Loss 1.232422                                        LR 0.000006    Time 0.073377    
2022-12-29 10:40:13,705 - Epoch: [333][   30/  113]    Overall Loss 1.263116    Objective Loss 1.263116                                        LR 0.000006    Time 0.070059    
2022-12-29 10:40:14,332 - Epoch: [333][   40/  113]    Overall Loss 1.266270    Objective Loss 1.266270                                        LR 0.000006    Time 0.068214    
2022-12-29 10:40:14,964 - Epoch: [333][   50/  113]    Overall Loss 1.263465    Objective Loss 1.263465                                        LR 0.000006    Time 0.067205    
2022-12-29 10:40:15,594 - Epoch: [333][   60/  113]    Overall Loss 1.249401    Objective Loss 1.249401                                        LR 0.000006    Time 0.066491    
2022-12-29 10:40:16,228 - Epoch: [333][   70/  113]    Overall Loss 1.245754    Objective Loss 1.245754                                        LR 0.000006    Time 0.066048    
2022-12-29 10:40:16,858 - Epoch: [333][   80/  113]    Overall Loss 1.230545    Objective Loss 1.230545                                        LR 0.000006    Time 0.065661    
2022-12-29 10:40:17,493 - Epoch: [333][   90/  113]    Overall Loss 1.241429    Objective Loss 1.241429                                        LR 0.000006    Time 0.065417    
2022-12-29 10:40:18,129 - Epoch: [333][  100/  113]    Overall Loss 1.240806    Objective Loss 1.240806                                        LR 0.000006    Time 0.065228    
2022-12-29 10:40:18,758 - Epoch: [333][  110/  113]    Overall Loss 1.245509    Objective Loss 1.245509                                        LR 0.000006    Time 0.065006    
2022-12-29 10:40:18,938 - Epoch: [333][  113/  113]    Overall Loss 1.242181    Objective Loss 1.242181    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.064873    
2022-12-29 10:40:18,990 - --- validate (epoch=333)-----------
2022-12-29 10:40:18,990 - 200 samples (16 per mini-batch)
2022-12-29 10:40:19,546 - Epoch: [333][   10/   13]    Loss 1.227497    Top1 55.000000    Top5 96.250000    
2022-12-29 10:40:19,632 - Epoch: [333][   13/   13]    Loss 1.236124    Top1 54.000000    Top5 97.000000    
2022-12-29 10:40:19,702 - ==> Top1: 54.000    Top5: 97.000    Loss: 1.236

2022-12-29 10:40:19,702 - ==> Confusion:
[[20  2  0  9  2  0]
 [ 1 24  5  5  7  0]
 [ 0  4 16  5  8  0]
 [ 4  3  1 28  4  3]
 [ 3  5  7  4 18  1]
 [ 0  2  1  2  4  2]]

2022-12-29 10:40:19,704 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:19,705 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:19,727 - 

2022-12-29 10:40:19,727 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:20,554 - Epoch: [334][   10/  113]    Overall Loss 1.302332    Objective Loss 1.302332                                        LR 0.000006    Time 0.082590    
2022-12-29 10:40:21,191 - Epoch: [334][   20/  113]    Overall Loss 1.224715    Objective Loss 1.224715                                        LR 0.000006    Time 0.073116    
2022-12-29 10:40:21,832 - Epoch: [334][   30/  113]    Overall Loss 1.225269    Objective Loss 1.225269                                        LR 0.000006    Time 0.070078    
2022-12-29 10:40:22,470 - Epoch: [334][   40/  113]    Overall Loss 1.232184    Objective Loss 1.232184                                        LR 0.000006    Time 0.068503    
2022-12-29 10:40:23,107 - Epoch: [334][   50/  113]    Overall Loss 1.223394    Objective Loss 1.223394                                        LR 0.000006    Time 0.067531    
2022-12-29 10:40:23,745 - Epoch: [334][   60/  113]    Overall Loss 1.208587    Objective Loss 1.208587                                        LR 0.000006    Time 0.066914    
2022-12-29 10:40:24,381 - Epoch: [334][   70/  113]    Overall Loss 1.221456    Objective Loss 1.221456                                        LR 0.000006    Time 0.066429    
2022-12-29 10:40:25,011 - Epoch: [334][   80/  113]    Overall Loss 1.204180    Objective Loss 1.204180                                        LR 0.000006    Time 0.065990    
2022-12-29 10:40:25,639 - Epoch: [334][   90/  113]    Overall Loss 1.199024    Objective Loss 1.199024                                        LR 0.000006    Time 0.065631    
2022-12-29 10:40:26,269 - Epoch: [334][  100/  113]    Overall Loss 1.213618    Objective Loss 1.213618                                        LR 0.000006    Time 0.065365    
2022-12-29 10:40:26,896 - Epoch: [334][  110/  113]    Overall Loss 1.215308    Objective Loss 1.215308                                        LR 0.000006    Time 0.065115    
2022-12-29 10:40:27,068 - Epoch: [334][  113/  113]    Overall Loss 1.215410    Objective Loss 1.215410    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064912    
2022-12-29 10:40:27,124 - --- validate (epoch=334)-----------
2022-12-29 10:40:27,125 - 200 samples (16 per mini-batch)
2022-12-29 10:40:27,687 - Epoch: [334][   10/   13]    Loss 1.198685    Top1 54.375000    Top5 96.875000    
2022-12-29 10:40:27,773 - Epoch: [334][   13/   13]    Loss 1.200138    Top1 52.500000    Top5 97.500000    
2022-12-29 10:40:27,818 - ==> Top1: 52.500    Top5: 97.500    Loss: 1.200

2022-12-29 10:40:27,818 - ==> Confusion:
[[17  5  0  2  0  0]
 [ 2 28  5  8  4  0]
 [ 1 12 13  4  4  0]
 [ 5  6  1 32  4  0]
 [ 0 15  2  6 14  0]
 [ 2  4  1  1  1  1]]

2022-12-29 10:40:27,822 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:27,822 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:27,842 - 

2022-12-29 10:40:27,842 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:28,675 - Epoch: [335][   10/  113]    Overall Loss 1.258105    Objective Loss 1.258105                                        LR 0.000006    Time 0.083216    
2022-12-29 10:40:29,315 - Epoch: [335][   20/  113]    Overall Loss 1.260520    Objective Loss 1.260520                                        LR 0.000006    Time 0.073548    
2022-12-29 10:40:29,954 - Epoch: [335][   30/  113]    Overall Loss 1.215229    Objective Loss 1.215229                                        LR 0.000006    Time 0.070316    
2022-12-29 10:40:30,592 - Epoch: [335][   40/  113]    Overall Loss 1.214321    Objective Loss 1.214321                                        LR 0.000006    Time 0.068695    
2022-12-29 10:40:31,228 - Epoch: [335][   50/  113]    Overall Loss 1.232450    Objective Loss 1.232450                                        LR 0.000006    Time 0.067655    
2022-12-29 10:40:31,864 - Epoch: [335][   60/  113]    Overall Loss 1.228010    Objective Loss 1.228010                                        LR 0.000006    Time 0.066975    
2022-12-29 10:40:32,496 - Epoch: [335][   70/  113]    Overall Loss 1.233294    Objective Loss 1.233294                                        LR 0.000006    Time 0.066425    
2022-12-29 10:40:33,132 - Epoch: [335][   80/  113]    Overall Loss 1.223689    Objective Loss 1.223689                                        LR 0.000006    Time 0.066072    
2022-12-29 10:40:33,771 - Epoch: [335][   90/  113]    Overall Loss 1.236907    Objective Loss 1.236907                                        LR 0.000006    Time 0.065826    
2022-12-29 10:40:34,402 - Epoch: [335][  100/  113]    Overall Loss 1.254252    Objective Loss 1.254252                                        LR 0.000006    Time 0.065542    
2022-12-29 10:40:35,034 - Epoch: [335][  110/  113]    Overall Loss 1.252806    Objective Loss 1.252806                                        LR 0.000006    Time 0.065329    
2022-12-29 10:40:35,205 - Epoch: [335][  113/  113]    Overall Loss 1.257232    Objective Loss 1.257232    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.065104    
2022-12-29 10:40:35,266 - --- validate (epoch=335)-----------
2022-12-29 10:40:35,267 - 200 samples (16 per mini-batch)
2022-12-29 10:40:35,823 - Epoch: [335][   10/   13]    Loss 1.242629    Top1 46.250000    Top5 96.875000    
2022-12-29 10:40:35,908 - Epoch: [335][   13/   13]    Loss 1.229230    Top1 47.500000    Top5 97.500000    
2022-12-29 10:40:35,958 - ==> Top1: 47.500    Top5: 97.500    Loss: 1.229

2022-12-29 10:40:35,959 - ==> Confusion:
[[13  4  1  4  1  0]
 [ 1 21  2  9  8  1]
 [ 1  7 15  4  9  0]
 [ 6  4  2 33  4  1]
 [ 8 10  5  6 11  0]
 [ 1  1  1  2  2  2]]

2022-12-29 10:40:35,963 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:35,963 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:35,985 - 

2022-12-29 10:40:35,986 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:36,822 - Epoch: [336][   10/  113]    Overall Loss 1.338083    Objective Loss 1.338083                                        LR 0.000006    Time 0.083567    
2022-12-29 10:40:37,455 - Epoch: [336][   20/  113]    Overall Loss 1.300176    Objective Loss 1.300176                                        LR 0.000006    Time 0.073413    
2022-12-29 10:40:38,088 - Epoch: [336][   30/  113]    Overall Loss 1.300366    Objective Loss 1.300366                                        LR 0.000006    Time 0.070026    
2022-12-29 10:40:38,721 - Epoch: [336][   40/  113]    Overall Loss 1.281498    Objective Loss 1.281498                                        LR 0.000006    Time 0.068327    
2022-12-29 10:40:39,355 - Epoch: [336][   50/  113]    Overall Loss 1.286143    Objective Loss 1.286143                                        LR 0.000006    Time 0.067336    
2022-12-29 10:40:39,989 - Epoch: [336][   60/  113]    Overall Loss 1.269141    Objective Loss 1.269141                                        LR 0.000006    Time 0.066667    
2022-12-29 10:40:40,624 - Epoch: [336][   70/  113]    Overall Loss 1.284326    Objective Loss 1.284326                                        LR 0.000006    Time 0.066206    
2022-12-29 10:40:41,254 - Epoch: [336][   80/  113]    Overall Loss 1.268957    Objective Loss 1.268957                                        LR 0.000006    Time 0.065796    
2022-12-29 10:40:41,888 - Epoch: [336][   90/  113]    Overall Loss 1.250919    Objective Loss 1.250919                                        LR 0.000006    Time 0.065523    
2022-12-29 10:40:42,520 - Epoch: [336][  100/  113]    Overall Loss 1.252071    Objective Loss 1.252071                                        LR 0.000006    Time 0.065290    
2022-12-29 10:40:43,151 - Epoch: [336][  110/  113]    Overall Loss 1.258938    Objective Loss 1.258938                                        LR 0.000006    Time 0.065087    
2022-12-29 10:40:43,322 - Epoch: [336][  113/  113]    Overall Loss 1.257732    Objective Loss 1.257732    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.064870    
2022-12-29 10:40:43,367 - --- validate (epoch=336)-----------
2022-12-29 10:40:43,368 - 200 samples (16 per mini-batch)
2022-12-29 10:40:43,933 - Epoch: [336][   10/   13]    Loss 1.117440    Top1 55.625000    Top5 98.125000    
2022-12-29 10:40:44,019 - Epoch: [336][   13/   13]    Loss 1.144031    Top1 54.000000    Top5 98.000000    
2022-12-29 10:40:44,066 - ==> Top1: 54.000    Top5: 98.000    Loss: 1.144

2022-12-29 10:40:44,066 - ==> Confusion:
[[26  1  0  6  1  1]
 [ 2 20  1  4  7  1]
 [ 1 10 17  4  7  1]
 [ 5  4  2 33  6  0]
 [ 1  8  2 11 10  0]
 [ 0  1  1  1  3  2]]

2022-12-29 10:40:44,070 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:44,070 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:44,090 - 

2022-12-29 10:40:44,091 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:44,926 - Epoch: [337][   10/  113]    Overall Loss 1.271667    Objective Loss 1.271667                                        LR 0.000006    Time 0.083498    
2022-12-29 10:40:45,561 - Epoch: [337][   20/  113]    Overall Loss 1.202381    Objective Loss 1.202381                                        LR 0.000006    Time 0.073426    
2022-12-29 10:40:46,198 - Epoch: [337][   30/  113]    Overall Loss 1.220835    Objective Loss 1.220835                                        LR 0.000006    Time 0.070175    
2022-12-29 10:40:46,834 - Epoch: [337][   40/  113]    Overall Loss 1.212759    Objective Loss 1.212759                                        LR 0.000006    Time 0.068529    
2022-12-29 10:40:47,466 - Epoch: [337][   50/  113]    Overall Loss 1.214759    Objective Loss 1.214759                                        LR 0.000006    Time 0.067447    
2022-12-29 10:40:48,102 - Epoch: [337][   60/  113]    Overall Loss 1.213009    Objective Loss 1.213009                                        LR 0.000006    Time 0.066796    
2022-12-29 10:40:48,736 - Epoch: [337][   70/  113]    Overall Loss 1.217777    Objective Loss 1.217777                                        LR 0.000006    Time 0.066316    
2022-12-29 10:40:49,369 - Epoch: [337][   80/  113]    Overall Loss 1.225920    Objective Loss 1.225920                                        LR 0.000006    Time 0.065929    
2022-12-29 10:40:50,005 - Epoch: [337][   90/  113]    Overall Loss 1.238195    Objective Loss 1.238195                                        LR 0.000006    Time 0.065665    
2022-12-29 10:40:50,640 - Epoch: [337][  100/  113]    Overall Loss 1.238418    Objective Loss 1.238418                                        LR 0.000006    Time 0.065439    
2022-12-29 10:40:51,260 - Epoch: [337][  110/  113]    Overall Loss 1.231686    Objective Loss 1.231686                                        LR 0.000006    Time 0.065121    
2022-12-29 10:40:51,430 - Epoch: [337][  113/  113]    Overall Loss 1.231375    Objective Loss 1.231375    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.064899    
2022-12-29 10:40:51,489 - --- validate (epoch=337)-----------
2022-12-29 10:40:51,490 - 200 samples (16 per mini-batch)
2022-12-29 10:40:52,037 - Epoch: [337][   10/   13]    Loss 1.275208    Top1 51.875000    Top5 96.875000    
2022-12-29 10:40:52,122 - Epoch: [337][   13/   13]    Loss 1.259816    Top1 53.500000    Top5 97.000000    
2022-12-29 10:40:52,169 - ==> Top1: 53.500    Top5: 97.000    Loss: 1.260

2022-12-29 10:40:52,170 - ==> Confusion:
[[20  1  1  7  4  0]
 [ 4 22  3  7  2  0]
 [ 1  5 12  7  9  0]
 [ 2  1  3 34  7  0]
 [ 1  7  3  8 16  0]
 [ 3  1  1  2  3  3]]

2022-12-29 10:40:52,173 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:40:52,173 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:40:52,197 - 

2022-12-29 10:40:52,197 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:40:53,037 - Epoch: [338][   10/  113]    Overall Loss 1.279263    Objective Loss 1.279263                                        LR 0.000006    Time 0.083771    
2022-12-29 10:40:53,672 - Epoch: [338][   20/  113]    Overall Loss 1.264792    Objective Loss 1.264792                                        LR 0.000006    Time 0.073633    
2022-12-29 10:40:54,308 - Epoch: [338][   30/  113]    Overall Loss 1.261442    Objective Loss 1.261442                                        LR 0.000006    Time 0.070274    
2022-12-29 10:40:54,941 - Epoch: [338][   40/  113]    Overall Loss 1.226460    Objective Loss 1.226460                                        LR 0.000006    Time 0.068504    
2022-12-29 10:40:55,568 - Epoch: [338][   50/  113]    Overall Loss 1.250407    Objective Loss 1.250407                                        LR 0.000006    Time 0.067331    
2022-12-29 10:40:56,203 - Epoch: [338][   60/  113]    Overall Loss 1.253413    Objective Loss 1.253413                                        LR 0.000006    Time 0.066682    
2022-12-29 10:40:56,830 - Epoch: [338][   70/  113]    Overall Loss 1.270606    Objective Loss 1.270606                                        LR 0.000006    Time 0.066108    
2022-12-29 10:40:57,456 - Epoch: [338][   80/  113]    Overall Loss 1.267694    Objective Loss 1.267694                                        LR 0.000006    Time 0.065667    
2022-12-29 10:40:58,085 - Epoch: [338][   90/  113]    Overall Loss 1.267414    Objective Loss 1.267414                                        LR 0.000006    Time 0.065349    
2022-12-29 10:40:58,711 - Epoch: [338][  100/  113]    Overall Loss 1.257307    Objective Loss 1.257307                                        LR 0.000006    Time 0.065075    
2022-12-29 10:40:59,337 - Epoch: [338][  110/  113]    Overall Loss 1.256670    Objective Loss 1.256670                                        LR 0.000006    Time 0.064841    
2022-12-29 10:40:59,508 - Epoch: [338][  113/  113]    Overall Loss 1.260515    Objective Loss 1.260515    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064631    
2022-12-29 10:40:59,568 - --- validate (epoch=338)-----------
2022-12-29 10:40:59,569 - 200 samples (16 per mini-batch)
2022-12-29 10:41:00,128 - Epoch: [338][   10/   13]    Loss 1.183217    Top1 56.875000    Top5 97.500000    
2022-12-29 10:41:00,214 - Epoch: [338][   13/   13]    Loss 1.166555    Top1 57.500000    Top5 97.500000    
2022-12-29 10:41:00,272 - ==> Top1: 57.500    Top5: 97.500    Loss: 1.167

2022-12-29 10:41:00,272 - ==> Confusion:
[[23  2  1  6  1  1]
 [ 2 17  2  8  9  1]
 [ 0  3 12  5  9  0]
 [ 4  4  4 42  3  0]
 [ 2  5  1  5 20  0]
 [ 1  3  0  0  3  1]]

2022-12-29 10:41:00,276 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:00,276 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:00,298 - 

2022-12-29 10:41:00,299 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:01,126 - Epoch: [339][   10/  113]    Overall Loss 1.321805    Objective Loss 1.321805                                        LR 0.000006    Time 0.082508    
2022-12-29 10:41:01,767 - Epoch: [339][   20/  113]    Overall Loss 1.271426    Objective Loss 1.271426                                        LR 0.000006    Time 0.073323    
2022-12-29 10:41:02,407 - Epoch: [339][   30/  113]    Overall Loss 1.256129    Objective Loss 1.256129                                        LR 0.000006    Time 0.070190    
2022-12-29 10:41:03,049 - Epoch: [339][   40/  113]    Overall Loss 1.258697    Objective Loss 1.258697                                        LR 0.000006    Time 0.068675    
2022-12-29 10:41:03,683 - Epoch: [339][   50/  113]    Overall Loss 1.248978    Objective Loss 1.248978                                        LR 0.000006    Time 0.067612    
2022-12-29 10:41:04,320 - Epoch: [339][   60/  113]    Overall Loss 1.239302    Objective Loss 1.239302                                        LR 0.000006    Time 0.066953    
2022-12-29 10:41:04,954 - Epoch: [339][   70/  113]    Overall Loss 1.258078    Objective Loss 1.258078                                        LR 0.000006    Time 0.066439    
2022-12-29 10:41:05,579 - Epoch: [339][   80/  113]    Overall Loss 1.242355    Objective Loss 1.242355                                        LR 0.000006    Time 0.065941    
2022-12-29 10:41:06,213 - Epoch: [339][   90/  113]    Overall Loss 1.248990    Objective Loss 1.248990                                        LR 0.000006    Time 0.065652    
2022-12-29 10:41:06,842 - Epoch: [339][  100/  113]    Overall Loss 1.252261    Objective Loss 1.252261                                        LR 0.000006    Time 0.065372    
2022-12-29 10:41:07,472 - Epoch: [339][  110/  113]    Overall Loss 1.251986    Objective Loss 1.251986                                        LR 0.000006    Time 0.065150    
2022-12-29 10:41:07,641 - Epoch: [339][  113/  113]    Overall Loss 1.249326    Objective Loss 1.249326    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064916    
2022-12-29 10:41:07,699 - --- validate (epoch=339)-----------
2022-12-29 10:41:07,700 - 200 samples (16 per mini-batch)
2022-12-29 10:41:08,264 - Epoch: [339][   10/   13]    Loss 1.269775    Top1 46.875000    Top5 98.750000    
2022-12-29 10:41:08,351 - Epoch: [339][   13/   13]    Loss 1.274759    Top1 47.000000    Top5 98.000000    
2022-12-29 10:41:08,399 - ==> Top1: 47.000    Top5: 98.000    Loss: 1.275

2022-12-29 10:41:08,399 - ==> Confusion:
[[23  3  0  7  2  0]
 [ 2 14  2  4  8  1]
 [ 4 10 10  3  3  0]
 [ 5  6  2 29  6  0]
 [ 7 10  3  6 17  1]
 [ 1  5  1  2  2  1]]

2022-12-29 10:41:08,402 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:08,403 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:08,426 - 

2022-12-29 10:41:08,426 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:09,264 - Epoch: [340][   10/  113]    Overall Loss 1.241474    Objective Loss 1.241474                                        LR 0.000006    Time 0.083752    
2022-12-29 10:41:09,903 - Epoch: [340][   20/  113]    Overall Loss 1.213031    Objective Loss 1.213031                                        LR 0.000006    Time 0.073779    
2022-12-29 10:41:10,535 - Epoch: [340][   30/  113]    Overall Loss 1.208521    Objective Loss 1.208521                                        LR 0.000006    Time 0.070253    
2022-12-29 10:41:11,172 - Epoch: [340][   40/  113]    Overall Loss 1.212523    Objective Loss 1.212523                                        LR 0.000006    Time 0.068589    
2022-12-29 10:41:11,807 - Epoch: [340][   50/  113]    Overall Loss 1.216446    Objective Loss 1.216446                                        LR 0.000006    Time 0.067564    
2022-12-29 10:41:12,442 - Epoch: [340][   60/  113]    Overall Loss 1.221042    Objective Loss 1.221042                                        LR 0.000006    Time 0.066881    
2022-12-29 10:41:13,080 - Epoch: [340][   70/  113]    Overall Loss 1.234093    Objective Loss 1.234093                                        LR 0.000006    Time 0.066424    
2022-12-29 10:41:13,711 - Epoch: [340][   80/  113]    Overall Loss 1.239717    Objective Loss 1.239717                                        LR 0.000006    Time 0.066010    
2022-12-29 10:41:14,350 - Epoch: [340][   90/  113]    Overall Loss 1.228499    Objective Loss 1.228499                                        LR 0.000006    Time 0.065770    
2022-12-29 10:41:14,981 - Epoch: [340][  100/  113]    Overall Loss 1.231161    Objective Loss 1.231161                                        LR 0.000006    Time 0.065495    
2022-12-29 10:41:15,612 - Epoch: [340][  110/  113]    Overall Loss 1.229258    Objective Loss 1.229258                                        LR 0.000006    Time 0.065272    
2022-12-29 10:41:15,782 - Epoch: [340][  113/  113]    Overall Loss 1.237234    Objective Loss 1.237234    Top1 41.666667    Top5 87.500000    LR 0.000006    Time 0.065047    
2022-12-29 10:41:15,839 - --- validate (epoch=340)-----------
2022-12-29 10:41:15,839 - 200 samples (16 per mini-batch)
2022-12-29 10:41:16,391 - Epoch: [340][   10/   13]    Loss 1.218944    Top1 57.500000    Top5 95.625000    
2022-12-29 10:41:16,476 - Epoch: [340][   13/   13]    Loss 1.213682    Top1 54.000000    Top5 96.000000    
2022-12-29 10:41:16,529 - ==> Top1: 54.000    Top5: 96.000    Loss: 1.214

2022-12-29 10:41:16,529 - ==> Confusion:
[[23  3  2  8  0  0]
 [ 1 24  2  4  7  0]
 [ 0  9 12  4  6  0]
 [ 4  2  2 33  6  0]
 [ 3  8  2 12 15  0]
 [ 0  2  1  1  3  1]]

2022-12-29 10:41:16,533 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:16,534 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:16,561 - 

2022-12-29 10:41:16,561 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:17,401 - Epoch: [341][   10/  113]    Overall Loss 1.218616    Objective Loss 1.218616                                        LR 0.000006    Time 0.083912    
2022-12-29 10:41:18,037 - Epoch: [341][   20/  113]    Overall Loss 1.219900    Objective Loss 1.219900                                        LR 0.000006    Time 0.073703    
2022-12-29 10:41:18,674 - Epoch: [341][   30/  113]    Overall Loss 1.233529    Objective Loss 1.233529                                        LR 0.000006    Time 0.070346    
2022-12-29 10:41:19,309 - Epoch: [341][   40/  113]    Overall Loss 1.232617    Objective Loss 1.232617                                        LR 0.000006    Time 0.068623    
2022-12-29 10:41:19,947 - Epoch: [341][   50/  113]    Overall Loss 1.248056    Objective Loss 1.248056                                        LR 0.000006    Time 0.067648    
2022-12-29 10:41:20,582 - Epoch: [341][   60/  113]    Overall Loss 1.217773    Objective Loss 1.217773                                        LR 0.000006    Time 0.066950    
2022-12-29 10:41:21,217 - Epoch: [341][   70/  113]    Overall Loss 1.211568    Objective Loss 1.211568                                        LR 0.000006    Time 0.066446    
2022-12-29 10:41:21,850 - Epoch: [341][   80/  113]    Overall Loss 1.205938    Objective Loss 1.205938                                        LR 0.000006    Time 0.066054    
2022-12-29 10:41:22,489 - Epoch: [341][   90/  113]    Overall Loss 1.220390    Objective Loss 1.220390                                        LR 0.000006    Time 0.065808    
2022-12-29 10:41:23,128 - Epoch: [341][  100/  113]    Overall Loss 1.224435    Objective Loss 1.224435                                        LR 0.000006    Time 0.065612    
2022-12-29 10:41:23,759 - Epoch: [341][  110/  113]    Overall Loss 1.220433    Objective Loss 1.220433                                        LR 0.000006    Time 0.065381    
2022-12-29 10:41:23,934 - Epoch: [341][  113/  113]    Overall Loss 1.220705    Objective Loss 1.220705    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.065191    
2022-12-29 10:41:23,998 - --- validate (epoch=341)-----------
2022-12-29 10:41:23,998 - 200 samples (16 per mini-batch)
2022-12-29 10:41:24,546 - Epoch: [341][   10/   13]    Loss 1.460572    Top1 44.375000    Top5 94.375000    
2022-12-29 10:41:24,631 - Epoch: [341][   13/   13]    Loss 1.371832    Top1 47.000000    Top5 95.000000    
2022-12-29 10:41:24,691 - ==> Top1: 47.000    Top5: 95.000    Loss: 1.372

2022-12-29 10:41:24,692 - ==> Confusion:
[[17  5  0  5  0  0]
 [ 3 24  4  3  8  2]
 [ 1  9 10  4  4  1]
 [ 4 11  0 25  7  0]
 [ 0 13  5  6 18  0]
 [ 2  4  0  2  3  0]]

2022-12-29 10:41:24,694 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:24,694 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:24,707 - 

2022-12-29 10:41:24,707 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:25,529 - Epoch: [342][   10/  113]    Overall Loss 1.281717    Objective Loss 1.281717                                        LR 0.000006    Time 0.082083    
2022-12-29 10:41:26,167 - Epoch: [342][   20/  113]    Overall Loss 1.294618    Objective Loss 1.294618                                        LR 0.000006    Time 0.072914    
2022-12-29 10:41:26,803 - Epoch: [342][   30/  113]    Overall Loss 1.287251    Objective Loss 1.287251                                        LR 0.000006    Time 0.069799    
2022-12-29 10:41:27,439 - Epoch: [342][   40/  113]    Overall Loss 1.271391    Objective Loss 1.271391                                        LR 0.000006    Time 0.068242    
2022-12-29 10:41:28,070 - Epoch: [342][   50/  113]    Overall Loss 1.259053    Objective Loss 1.259053                                        LR 0.000006    Time 0.067216    
2022-12-29 10:41:28,701 - Epoch: [342][   60/  113]    Overall Loss 1.261886    Objective Loss 1.261886                                        LR 0.000006    Time 0.066517    
2022-12-29 10:41:29,331 - Epoch: [342][   70/  113]    Overall Loss 1.266555    Objective Loss 1.266555                                        LR 0.000006    Time 0.066004    
2022-12-29 10:41:29,960 - Epoch: [342][   80/  113]    Overall Loss 1.262614    Objective Loss 1.262614                                        LR 0.000006    Time 0.065608    
2022-12-29 10:41:30,595 - Epoch: [342][   90/  113]    Overall Loss 1.267777    Objective Loss 1.267777                                        LR 0.000006    Time 0.065378    
2022-12-29 10:41:31,226 - Epoch: [342][  100/  113]    Overall Loss 1.282080    Objective Loss 1.282080                                        LR 0.000006    Time 0.065140    
2022-12-29 10:41:31,855 - Epoch: [342][  110/  113]    Overall Loss 1.279870    Objective Loss 1.279870                                        LR 0.000006    Time 0.064932    
2022-12-29 10:41:32,029 - Epoch: [342][  113/  113]    Overall Loss 1.281001    Objective Loss 1.281001    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064748    
2022-12-29 10:41:32,084 - --- validate (epoch=342)-----------
2022-12-29 10:41:32,084 - 200 samples (16 per mini-batch)
2022-12-29 10:41:32,644 - Epoch: [342][   10/   13]    Loss 1.221061    Top1 47.500000    Top5 100.000000    
2022-12-29 10:41:32,729 - Epoch: [342][   13/   13]    Loss 1.188610    Top1 48.500000    Top5 100.000000    
2022-12-29 10:41:32,778 - ==> Top1: 48.500    Top5: 100.000    Loss: 1.189

2022-12-29 10:41:32,778 - ==> Confusion:
[[20  3  1  7  2  0]
 [ 1 19  3  5  6  1]
 [ 0 18 13  4  8  0]
 [ 2 11  0 30  6  1]
 [ 2  5  2  5 14  2]
 [ 0  0  2  3  3  1]]

2022-12-29 10:41:32,782 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:32,782 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:32,795 - 

2022-12-29 10:41:32,795 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:33,630 - Epoch: [343][   10/  113]    Overall Loss 1.251722    Objective Loss 1.251722                                        LR 0.000006    Time 0.083326    
2022-12-29 10:41:34,266 - Epoch: [343][   20/  113]    Overall Loss 1.288687    Objective Loss 1.288687                                        LR 0.000006    Time 0.073441    
2022-12-29 10:41:34,901 - Epoch: [343][   30/  113]    Overall Loss 1.290369    Objective Loss 1.290369                                        LR 0.000006    Time 0.070138    
2022-12-29 10:41:35,533 - Epoch: [343][   40/  113]    Overall Loss 1.290597    Objective Loss 1.290597                                        LR 0.000006    Time 0.068380    
2022-12-29 10:41:36,162 - Epoch: [343][   50/  113]    Overall Loss 1.278045    Objective Loss 1.278045                                        LR 0.000006    Time 0.067274    
2022-12-29 10:41:36,789 - Epoch: [343][   60/  113]    Overall Loss 1.260619    Objective Loss 1.260619                                        LR 0.000006    Time 0.066506    
2022-12-29 10:41:37,417 - Epoch: [343][   70/  113]    Overall Loss 1.259663    Objective Loss 1.259663                                        LR 0.000006    Time 0.065971    
2022-12-29 10:41:38,047 - Epoch: [343][   80/  113]    Overall Loss 1.258327    Objective Loss 1.258327                                        LR 0.000006    Time 0.065584    
2022-12-29 10:41:38,681 - Epoch: [343][   90/  113]    Overall Loss 1.248373    Objective Loss 1.248373                                        LR 0.000006    Time 0.065337    
2022-12-29 10:41:39,313 - Epoch: [343][  100/  113]    Overall Loss 1.262191    Objective Loss 1.262191                                        LR 0.000006    Time 0.065123    
2022-12-29 10:41:39,941 - Epoch: [343][  110/  113]    Overall Loss 1.258647    Objective Loss 1.258647                                        LR 0.000006    Time 0.064905    
2022-12-29 10:41:40,113 - Epoch: [343][  113/  113]    Overall Loss 1.261118    Objective Loss 1.261118    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064704    
2022-12-29 10:41:40,181 - --- validate (epoch=343)-----------
2022-12-29 10:41:40,181 - 200 samples (16 per mini-batch)
2022-12-29 10:41:40,739 - Epoch: [343][   10/   13]    Loss 1.287883    Top1 47.500000    Top5 96.250000    
2022-12-29 10:41:40,824 - Epoch: [343][   13/   13]    Loss 1.275263    Top1 49.000000    Top5 97.000000    
2022-12-29 10:41:40,873 - ==> Top1: 49.000    Top5: 97.000    Loss: 1.275

2022-12-29 10:41:40,873 - ==> Confusion:
[[15  2  1  8  4  0]
 [ 4 27  0 10  5  1]
 [ 1 11 10 13  6  1]
 [ 3  4  0 31  0  1]
 [ 0 10  2  7 13  1]
 [ 2  2  1  2  0  2]]

2022-12-29 10:41:40,877 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:40,877 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:40,899 - 

2022-12-29 10:41:40,900 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:41,743 - Epoch: [344][   10/  113]    Overall Loss 1.311331    Objective Loss 1.311331                                        LR 0.000006    Time 0.084187    
2022-12-29 10:41:42,380 - Epoch: [344][   20/  113]    Overall Loss 1.224999    Objective Loss 1.224999                                        LR 0.000006    Time 0.073910    
2022-12-29 10:41:43,021 - Epoch: [344][   30/  113]    Overall Loss 1.244145    Objective Loss 1.244145                                        LR 0.000006    Time 0.070617    
2022-12-29 10:41:43,655 - Epoch: [344][   40/  113]    Overall Loss 1.235425    Objective Loss 1.235425                                        LR 0.000006    Time 0.068806    
2022-12-29 10:41:44,296 - Epoch: [344][   50/  113]    Overall Loss 1.241242    Objective Loss 1.241242                                        LR 0.000006    Time 0.067841    
2022-12-29 10:41:44,931 - Epoch: [344][   60/  113]    Overall Loss 1.256058    Objective Loss 1.256058                                        LR 0.000006    Time 0.067117    
2022-12-29 10:41:45,565 - Epoch: [344][   70/  113]    Overall Loss 1.257339    Objective Loss 1.257339                                        LR 0.000006    Time 0.066575    
2022-12-29 10:41:46,198 - Epoch: [344][   80/  113]    Overall Loss 1.259252    Objective Loss 1.259252                                        LR 0.000006    Time 0.066157    
2022-12-29 10:41:46,829 - Epoch: [344][   90/  113]    Overall Loss 1.259407    Objective Loss 1.259407                                        LR 0.000006    Time 0.065818    
2022-12-29 10:41:47,458 - Epoch: [344][  100/  113]    Overall Loss 1.255463    Objective Loss 1.255463                                        LR 0.000006    Time 0.065524    
2022-12-29 10:41:48,080 - Epoch: [344][  110/  113]    Overall Loss 1.253705    Objective Loss 1.253705                                        LR 0.000006    Time 0.065217    
2022-12-29 10:41:48,263 - Epoch: [344][  113/  113]    Overall Loss 1.255073    Objective Loss 1.255073    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065104    
2022-12-29 10:41:48,316 - --- validate (epoch=344)-----------
2022-12-29 10:41:48,316 - 200 samples (16 per mini-batch)
2022-12-29 10:41:48,854 - Epoch: [344][   10/   13]    Loss 1.281603    Top1 49.375000    Top5 99.375000    
2022-12-29 10:41:48,939 - Epoch: [344][   13/   13]    Loss 1.240326    Top1 52.000000    Top5 99.000000    
2022-12-29 10:41:48,997 - ==> Top1: 52.000    Top5: 99.000    Loss: 1.240

2022-12-29 10:41:48,998 - ==> Confusion:
[[21  1  0  9  2  0]
 [ 1 24  3  7  4  0]
 [ 1  9 11  7  4  0]
 [ 7  6  6 34  6  0]
 [ 2  5  2  7 14  0]
 [ 0  3  0  1  3  0]]

2022-12-29 10:41:49,000 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:49,000 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:49,012 - 

2022-12-29 10:41:49,012 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:49,841 - Epoch: [345][   10/  113]    Overall Loss 1.222630    Objective Loss 1.222630                                        LR 0.000006    Time 0.082792    
2022-12-29 10:41:50,479 - Epoch: [345][   20/  113]    Overall Loss 1.238954    Objective Loss 1.238954                                        LR 0.000006    Time 0.073246    
2022-12-29 10:41:51,115 - Epoch: [345][   30/  113]    Overall Loss 1.237836    Objective Loss 1.237836                                        LR 0.000006    Time 0.070018    
2022-12-29 10:41:51,749 - Epoch: [345][   40/  113]    Overall Loss 1.227264    Objective Loss 1.227264                                        LR 0.000006    Time 0.068352    
2022-12-29 10:41:52,385 - Epoch: [345][   50/  113]    Overall Loss 1.243485    Objective Loss 1.243485                                        LR 0.000006    Time 0.067395    
2022-12-29 10:41:53,017 - Epoch: [345][   60/  113]    Overall Loss 1.253662    Objective Loss 1.253662                                        LR 0.000006    Time 0.066696    
2022-12-29 10:41:53,653 - Epoch: [345][   70/  113]    Overall Loss 1.250138    Objective Loss 1.250138                                        LR 0.000006    Time 0.066242    
2022-12-29 10:41:54,284 - Epoch: [345][   80/  113]    Overall Loss 1.235526    Objective Loss 1.235526                                        LR 0.000006    Time 0.065841    
2022-12-29 10:41:54,912 - Epoch: [345][   90/  113]    Overall Loss 1.249723    Objective Loss 1.249723                                        LR 0.000006    Time 0.065498    
2022-12-29 10:41:55,539 - Epoch: [345][  100/  113]    Overall Loss 1.244582    Objective Loss 1.244582                                        LR 0.000006    Time 0.065214    
2022-12-29 10:41:56,158 - Epoch: [345][  110/  113]    Overall Loss 1.245366    Objective Loss 1.245366                                        LR 0.000006    Time 0.064901    
2022-12-29 10:41:56,332 - Epoch: [345][  113/  113]    Overall Loss 1.241893    Objective Loss 1.241893    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064719    
2022-12-29 10:41:56,390 - --- validate (epoch=345)-----------
2022-12-29 10:41:56,391 - 200 samples (16 per mini-batch)
2022-12-29 10:41:56,938 - Epoch: [345][   10/   13]    Loss 1.258003    Top1 44.375000    Top5 98.750000    
2022-12-29 10:41:57,023 - Epoch: [345][   13/   13]    Loss 1.223844    Top1 46.500000    Top5 99.000000    
2022-12-29 10:41:57,071 - ==> Top1: 46.500    Top5: 99.000    Loss: 1.224

2022-12-29 10:41:57,071 - ==> Confusion:
[[19  5  0  5  3  0]
 [ 2 25  5  6  7  0]
 [ 0  8  8  6  8  1]
 [ 7  4  1 30  2  2]
 [ 1 10  4 12  8  0]
 [ 2  4  0  0  2  3]]

2022-12-29 10:41:57,074 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:41:57,074 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:41:57,093 - 

2022-12-29 10:41:57,094 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:41:57,930 - Epoch: [346][   10/  113]    Overall Loss 1.219388    Objective Loss 1.219388                                        LR 0.000006    Time 0.083547    
2022-12-29 10:41:58,566 - Epoch: [346][   20/  113]    Overall Loss 1.222993    Objective Loss 1.222993                                        LR 0.000006    Time 0.073527    
2022-12-29 10:41:59,203 - Epoch: [346][   30/  113]    Overall Loss 1.201761    Objective Loss 1.201761                                        LR 0.000006    Time 0.070254    
2022-12-29 10:41:59,838 - Epoch: [346][   40/  113]    Overall Loss 1.198607    Objective Loss 1.198607                                        LR 0.000006    Time 0.068540    
2022-12-29 10:42:00,478 - Epoch: [346][   50/  113]    Overall Loss 1.219337    Objective Loss 1.219337                                        LR 0.000006    Time 0.067626    
2022-12-29 10:42:01,109 - Epoch: [346][   60/  113]    Overall Loss 1.235866    Objective Loss 1.235866                                        LR 0.000006    Time 0.066864    
2022-12-29 10:42:01,740 - Epoch: [346][   70/  113]    Overall Loss 1.224700    Objective Loss 1.224700                                        LR 0.000006    Time 0.066317    
2022-12-29 10:42:02,375 - Epoch: [346][   80/  113]    Overall Loss 1.223168    Objective Loss 1.223168                                        LR 0.000006    Time 0.065963    
2022-12-29 10:42:03,005 - Epoch: [346][   90/  113]    Overall Loss 1.236777    Objective Loss 1.236777                                        LR 0.000006    Time 0.065630    
2022-12-29 10:42:03,634 - Epoch: [346][  100/  113]    Overall Loss 1.235959    Objective Loss 1.235959                                        LR 0.000006    Time 0.065344    
2022-12-29 10:42:04,255 - Epoch: [346][  110/  113]    Overall Loss 1.242542    Objective Loss 1.242542                                        LR 0.000006    Time 0.065047    
2022-12-29 10:42:04,433 - Epoch: [346][  113/  113]    Overall Loss 1.244094    Objective Loss 1.244094    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.064898    
2022-12-29 10:42:04,482 - --- validate (epoch=346)-----------
2022-12-29 10:42:04,483 - 200 samples (16 per mini-batch)
2022-12-29 10:42:05,043 - Epoch: [346][   10/   13]    Loss 1.223507    Top1 52.500000    Top5 96.250000    
2022-12-29 10:42:05,128 - Epoch: [346][   13/   13]    Loss 1.227847    Top1 51.500000    Top5 96.500000    
2022-12-29 10:42:05,177 - ==> Top1: 51.500    Top5: 96.500    Loss: 1.228

2022-12-29 10:42:05,178 - ==> Confusion:
[[17  1  2  5  1  0]
 [ 1 20  4  9 12  0]
 [ 2  8 13  7  3  0]
 [ 4  1  3 35  4  0]
 [ 3  6  6  7 17  1]
 [ 0  1  0  3  3  1]]

2022-12-29 10:42:05,180 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:05,180 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:05,195 - 

2022-12-29 10:42:05,195 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:06,034 - Epoch: [347][   10/  113]    Overall Loss 1.297271    Objective Loss 1.297271                                        LR 0.000006    Time 0.083737    
2022-12-29 10:42:06,667 - Epoch: [347][   20/  113]    Overall Loss 1.238591    Objective Loss 1.238591                                        LR 0.000006    Time 0.073496    
2022-12-29 10:42:07,305 - Epoch: [347][   30/  113]    Overall Loss 1.250227    Objective Loss 1.250227                                        LR 0.000006    Time 0.070241    
2022-12-29 10:42:07,939 - Epoch: [347][   40/  113]    Overall Loss 1.237823    Objective Loss 1.237823                                        LR 0.000006    Time 0.068535    
2022-12-29 10:42:08,572 - Epoch: [347][   50/  113]    Overall Loss 1.243031    Objective Loss 1.243031                                        LR 0.000006    Time 0.067474    
2022-12-29 10:42:09,204 - Epoch: [347][   60/  113]    Overall Loss 1.227760    Objective Loss 1.227760                                        LR 0.000006    Time 0.066746    
2022-12-29 10:42:09,839 - Epoch: [347][   70/  113]    Overall Loss 1.234439    Objective Loss 1.234439                                        LR 0.000006    Time 0.066276    
2022-12-29 10:42:10,474 - Epoch: [347][   80/  113]    Overall Loss 1.234738    Objective Loss 1.234738                                        LR 0.000006    Time 0.065929    
2022-12-29 10:42:11,110 - Epoch: [347][   90/  113]    Overall Loss 1.232992    Objective Loss 1.232992                                        LR 0.000006    Time 0.065666    
2022-12-29 10:42:11,749 - Epoch: [347][  100/  113]    Overall Loss 1.219799    Objective Loss 1.219799                                        LR 0.000006    Time 0.065485    
2022-12-29 10:42:12,378 - Epoch: [347][  110/  113]    Overall Loss 1.223470    Objective Loss 1.223470                                        LR 0.000006    Time 0.065242    
2022-12-29 10:42:12,558 - Epoch: [347][  113/  113]    Overall Loss 1.220971    Objective Loss 1.220971    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.065105    
2022-12-29 10:42:12,603 - --- validate (epoch=347)-----------
2022-12-29 10:42:12,603 - 200 samples (16 per mini-batch)
2022-12-29 10:42:13,167 - Epoch: [347][   10/   13]    Loss 1.350041    Top1 51.875000    Top5 94.375000    
2022-12-29 10:42:13,254 - Epoch: [347][   13/   13]    Loss 1.335711    Top1 51.500000    Top5 95.000000    
2022-12-29 10:42:13,307 - ==> Top1: 51.500    Top5: 95.000    Loss: 1.336

2022-12-29 10:42:13,307 - ==> Confusion:
[[25  1  0  6  4  0]
 [ 2 23  2  3  5  1]
 [ 1  5 13  5  7  0]
 [ 3  8  2 28  5  0]
 [ 0  7  5 10 13  0]
 [ 0  3  1  5  6  1]]

2022-12-29 10:42:13,309 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:13,310 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:13,320 - 

2022-12-29 10:42:13,320 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:14,149 - Epoch: [348][   10/  113]    Overall Loss 1.203963    Objective Loss 1.203963                                        LR 0.000006    Time 0.082785    
2022-12-29 10:42:14,781 - Epoch: [348][   20/  113]    Overall Loss 1.174774    Objective Loss 1.174774                                        LR 0.000006    Time 0.072965    
2022-12-29 10:42:15,412 - Epoch: [348][   30/  113]    Overall Loss 1.184383    Objective Loss 1.184383                                        LR 0.000006    Time 0.069660    
2022-12-29 10:42:16,044 - Epoch: [348][   40/  113]    Overall Loss 1.153760    Objective Loss 1.153760                                        LR 0.000006    Time 0.068029    
2022-12-29 10:42:16,677 - Epoch: [348][   50/  113]    Overall Loss 1.178756    Objective Loss 1.178756                                        LR 0.000006    Time 0.067078    
2022-12-29 10:42:17,310 - Epoch: [348][   60/  113]    Overall Loss 1.175447    Objective Loss 1.175447                                        LR 0.000006    Time 0.066440    
2022-12-29 10:42:17,945 - Epoch: [348][   70/  113]    Overall Loss 1.190905    Objective Loss 1.190905                                        LR 0.000006    Time 0.066006    
2022-12-29 10:42:18,577 - Epoch: [348][   80/  113]    Overall Loss 1.184138    Objective Loss 1.184138                                        LR 0.000006    Time 0.065649    
2022-12-29 10:42:19,211 - Epoch: [348][   90/  113]    Overall Loss 1.190882    Objective Loss 1.190882                                        LR 0.000006    Time 0.065395    
2022-12-29 10:42:19,846 - Epoch: [348][  100/  113]    Overall Loss 1.204706    Objective Loss 1.204706                                        LR 0.000006    Time 0.065203    
2022-12-29 10:42:20,468 - Epoch: [348][  110/  113]    Overall Loss 1.205127    Objective Loss 1.205127                                        LR 0.000006    Time 0.064927    
2022-12-29 10:42:20,644 - Epoch: [348][  113/  113]    Overall Loss 1.206450    Objective Loss 1.206450    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.064760    
2022-12-29 10:42:20,705 - --- validate (epoch=348)-----------
2022-12-29 10:42:20,706 - 200 samples (16 per mini-batch)
2022-12-29 10:42:21,263 - Epoch: [348][   10/   13]    Loss 1.243763    Top1 50.625000    Top5 98.125000    
2022-12-29 10:42:21,349 - Epoch: [348][   13/   13]    Loss 1.209055    Top1 51.000000    Top5 98.500000    
2022-12-29 10:42:21,404 - ==> Top1: 51.000    Top5: 98.500    Loss: 1.209

2022-12-29 10:42:21,405 - ==> Confusion:
[[17  4  1  4  2  0]
 [ 2 20  3 10  6  0]
 [ 0  9 14  4  8  0]
 [ 2  3  1 36  2  1]
 [ 1 10  6 13 13  0]
 [ 1  1  0  1  3  2]]

2022-12-29 10:42:21,408 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:21,409 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:21,429 - 

2022-12-29 10:42:21,430 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:22,261 - Epoch: [349][   10/  113]    Overall Loss 1.322587    Objective Loss 1.322587                                        LR 0.000006    Time 0.083007    
2022-12-29 10:42:22,905 - Epoch: [349][   20/  113]    Overall Loss 1.282483    Objective Loss 1.282483                                        LR 0.000006    Time 0.073671    
2022-12-29 10:42:23,551 - Epoch: [349][   30/  113]    Overall Loss 1.294767    Objective Loss 1.294767                                        LR 0.000006    Time 0.070661    
2022-12-29 10:42:24,189 - Epoch: [349][   40/  113]    Overall Loss 1.308664    Objective Loss 1.308664                                        LR 0.000006    Time 0.068919    
2022-12-29 10:42:24,826 - Epoch: [349][   50/  113]    Overall Loss 1.289319    Objective Loss 1.289319                                        LR 0.000006    Time 0.067865    
2022-12-29 10:42:25,460 - Epoch: [349][   60/  113]    Overall Loss 1.264204    Objective Loss 1.264204                                        LR 0.000006    Time 0.067120    
2022-12-29 10:42:26,099 - Epoch: [349][   70/  113]    Overall Loss 1.268697    Objective Loss 1.268697                                        LR 0.000006    Time 0.066658    
2022-12-29 10:42:26,744 - Epoch: [349][   80/  113]    Overall Loss 1.270746    Objective Loss 1.270746                                        LR 0.000006    Time 0.066376    
2022-12-29 10:42:27,381 - Epoch: [349][   90/  113]    Overall Loss 1.268704    Objective Loss 1.268704                                        LR 0.000006    Time 0.066075    
2022-12-29 10:42:28,017 - Epoch: [349][  100/  113]    Overall Loss 1.263751    Objective Loss 1.263751                                        LR 0.000006    Time 0.065822    
2022-12-29 10:42:28,651 - Epoch: [349][  110/  113]    Overall Loss 1.260617    Objective Loss 1.260617                                        LR 0.000006    Time 0.065595    
2022-12-29 10:42:28,820 - Epoch: [349][  113/  113]    Overall Loss 1.261062    Objective Loss 1.261062    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.065349    
2022-12-29 10:42:28,881 - --- validate (epoch=349)-----------
2022-12-29 10:42:28,881 - 200 samples (16 per mini-batch)
2022-12-29 10:42:29,440 - Epoch: [349][   10/   13]    Loss 1.360979    Top1 46.250000    Top5 97.500000    
2022-12-29 10:42:29,526 - Epoch: [349][   13/   13]    Loss 1.397739    Top1 45.500000    Top5 97.500000    
2022-12-29 10:42:29,585 - ==> Top1: 45.500    Top5: 97.500    Loss: 1.398

2022-12-29 10:42:29,585 - ==> Confusion:
[[13  6  0  5  5  1]
 [ 4 28  4 11  3  0]
 [ 0  8  8  5  3  1]
 [ 6  2  2 26  5  0]
 [ 2 15  4  9 14  0]
 [ 0  5  0  1  2  2]]

2022-12-29 10:42:29,589 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:29,590 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:29,601 - 

2022-12-29 10:42:29,602 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:30,440 - Epoch: [350][   10/  113]    Overall Loss 1.163434    Objective Loss 1.163434                                        LR 0.000006    Time 0.083728    
2022-12-29 10:42:31,073 - Epoch: [350][   20/  113]    Overall Loss 1.109830    Objective Loss 1.109830                                        LR 0.000006    Time 0.073500    
2022-12-29 10:42:31,703 - Epoch: [350][   30/  113]    Overall Loss 1.171458    Objective Loss 1.171458                                        LR 0.000006    Time 0.069972    
2022-12-29 10:42:32,345 - Epoch: [350][   40/  113]    Overall Loss 1.212162    Objective Loss 1.212162                                        LR 0.000006    Time 0.068522    
2022-12-29 10:42:32,987 - Epoch: [350][   50/  113]    Overall Loss 1.238565    Objective Loss 1.238565                                        LR 0.000006    Time 0.067645    
2022-12-29 10:42:33,623 - Epoch: [350][   60/  113]    Overall Loss 1.251284    Objective Loss 1.251284                                        LR 0.000006    Time 0.066960    
2022-12-29 10:42:34,259 - Epoch: [350][   70/  113]    Overall Loss 1.264156    Objective Loss 1.264156                                        LR 0.000006    Time 0.066487    
2022-12-29 10:42:34,897 - Epoch: [350][   80/  113]    Overall Loss 1.275044    Objective Loss 1.275044                                        LR 0.000006    Time 0.066128    
2022-12-29 10:42:35,536 - Epoch: [350][   90/  113]    Overall Loss 1.288874    Objective Loss 1.288874                                        LR 0.000006    Time 0.065873    
2022-12-29 10:42:36,173 - Epoch: [350][  100/  113]    Overall Loss 1.290681    Objective Loss 1.290681                                        LR 0.000006    Time 0.065651    
2022-12-29 10:42:36,803 - Epoch: [350][  110/  113]    Overall Loss 1.295468    Objective Loss 1.295468                                        LR 0.000006    Time 0.065401    
2022-12-29 10:42:36,977 - Epoch: [350][  113/  113]    Overall Loss 1.299219    Objective Loss 1.299219    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.065202    
2022-12-29 10:42:37,019 - --- validate (epoch=350)-----------
2022-12-29 10:42:37,019 - 200 samples (16 per mini-batch)
2022-12-29 10:42:37,585 - Epoch: [350][   10/   13]    Loss 1.467296    Top1 38.125000    Top5 96.250000    
2022-12-29 10:42:37,671 - Epoch: [350][   13/   13]    Loss 1.437461    Top1 41.000000    Top5 95.500000    
2022-12-29 10:42:37,725 - ==> Top1: 41.000    Top5: 95.500    Loss: 1.437

2022-12-29 10:42:37,726 - ==> Confusion:
[[21  7  2  8  1  0]
 [ 2 21  6  6  1  1]
 [ 1 11 11 10  3  0]
 [ 5  7  3 24  2  0]
 [ 3  9  6  6  5  0]
 [ 8  5  0  2  3  0]]

2022-12-29 10:42:37,729 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:37,729 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:37,752 - 

2022-12-29 10:42:37,753 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:38,593 - Epoch: [351][   10/  113]    Overall Loss 1.390460    Objective Loss 1.390460                                        LR 0.000006    Time 0.083937    
2022-12-29 10:42:39,228 - Epoch: [351][   20/  113]    Overall Loss 1.350895    Objective Loss 1.350895                                        LR 0.000006    Time 0.073716    
2022-12-29 10:42:39,868 - Epoch: [351][   30/  113]    Overall Loss 1.346959    Objective Loss 1.346959                                        LR 0.000006    Time 0.070448    
2022-12-29 10:42:40,500 - Epoch: [351][   40/  113]    Overall Loss 1.341256    Objective Loss 1.341256                                        LR 0.000006    Time 0.068615    
2022-12-29 10:42:41,137 - Epoch: [351][   50/  113]    Overall Loss 1.343783    Objective Loss 1.343783                                        LR 0.000006    Time 0.067620    
2022-12-29 10:42:41,768 - Epoch: [351][   60/  113]    Overall Loss 1.334123    Objective Loss 1.334123                                        LR 0.000006    Time 0.066873    
2022-12-29 10:42:42,404 - Epoch: [351][   70/  113]    Overall Loss 1.341884    Objective Loss 1.341884                                        LR 0.000006    Time 0.066398    
2022-12-29 10:42:43,036 - Epoch: [351][   80/  113]    Overall Loss 1.336847    Objective Loss 1.336847                                        LR 0.000006    Time 0.065985    
2022-12-29 10:42:43,675 - Epoch: [351][   90/  113]    Overall Loss 1.335312    Objective Loss 1.335312                                        LR 0.000006    Time 0.065754    
2022-12-29 10:42:44,308 - Epoch: [351][  100/  113]    Overall Loss 1.336878    Objective Loss 1.336878                                        LR 0.000006    Time 0.065506    
2022-12-29 10:42:44,938 - Epoch: [351][  110/  113]    Overall Loss 1.334120    Objective Loss 1.334120                                        LR 0.000006    Time 0.065274    
2022-12-29 10:42:45,112 - Epoch: [351][  113/  113]    Overall Loss 1.330929    Objective Loss 1.330929    Top1 66.666667    Top5 100.000000    LR 0.000006    Time 0.065078    
2022-12-29 10:42:45,167 - --- validate (epoch=351)-----------
2022-12-29 10:42:45,167 - 200 samples (16 per mini-batch)
2022-12-29 10:42:45,715 - Epoch: [351][   10/   13]    Loss 1.309859    Top1 52.500000    Top5 96.875000    
2022-12-29 10:42:45,800 - Epoch: [351][   13/   13]    Loss 1.306723    Top1 52.000000    Top5 97.000000    
2022-12-29 10:42:45,862 - ==> Top1: 52.000    Top5: 97.000    Loss: 1.307

2022-12-29 10:42:45,863 - ==> Confusion:
[[22  2  1 10  1  0]
 [ 1 28  1  6  2  1]
 [ 0  6 10  4  5  0]
 [ 6  7  3 32  2  0]
 [ 0 16  7  6  9  0]
 [ 1  4  2  2  0  3]]

2022-12-29 10:42:45,867 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:45,868 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:45,889 - 

2022-12-29 10:42:45,889 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:46,723 - Epoch: [352][   10/  113]    Overall Loss 1.333634    Objective Loss 1.333634                                        LR 0.000006    Time 0.083317    
2022-12-29 10:42:47,352 - Epoch: [352][   20/  113]    Overall Loss 1.344682    Objective Loss 1.344682                                        LR 0.000006    Time 0.073063    
2022-12-29 10:42:47,980 - Epoch: [352][   30/  113]    Overall Loss 1.332570    Objective Loss 1.332570                                        LR 0.000006    Time 0.069638    
2022-12-29 10:42:48,613 - Epoch: [352][   40/  113]    Overall Loss 1.332351    Objective Loss 1.332351                                        LR 0.000006    Time 0.068041    
2022-12-29 10:42:49,242 - Epoch: [352][   50/  113]    Overall Loss 1.313466    Objective Loss 1.313466                                        LR 0.000006    Time 0.067001    
2022-12-29 10:42:49,869 - Epoch: [352][   60/  113]    Overall Loss 1.319322    Objective Loss 1.319322                                        LR 0.000006    Time 0.066274    
2022-12-29 10:42:50,496 - Epoch: [352][   70/  113]    Overall Loss 1.316190    Objective Loss 1.316190                                        LR 0.000006    Time 0.065761    
2022-12-29 10:42:51,131 - Epoch: [352][   80/  113]    Overall Loss 1.316075    Objective Loss 1.316075                                        LR 0.000006    Time 0.065463    
2022-12-29 10:42:51,770 - Epoch: [352][   90/  113]    Overall Loss 1.318450    Objective Loss 1.318450                                        LR 0.000006    Time 0.065285    
2022-12-29 10:42:52,409 - Epoch: [352][  100/  113]    Overall Loss 1.325354    Objective Loss 1.325354                                        LR 0.000006    Time 0.065148    
2022-12-29 10:42:53,039 - Epoch: [352][  110/  113]    Overall Loss 1.319588    Objective Loss 1.319588                                        LR 0.000006    Time 0.064943    
2022-12-29 10:42:53,217 - Epoch: [352][  113/  113]    Overall Loss 1.316760    Objective Loss 1.316760    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064789    
2022-12-29 10:42:53,269 - --- validate (epoch=352)-----------
2022-12-29 10:42:53,270 - 200 samples (16 per mini-batch)
2022-12-29 10:42:53,824 - Epoch: [352][   10/   13]    Loss 1.374441    Top1 45.000000    Top5 96.250000    
2022-12-29 10:42:53,909 - Epoch: [352][   13/   13]    Loss 1.347532    Top1 44.500000    Top5 96.500000    
2022-12-29 10:42:53,951 - ==> Top1: 44.500    Top5: 96.500    Loss: 1.348

2022-12-29 10:42:53,952 - ==> Confusion:
[[17  1  1 14  0  0]
 [ 2 19  2  7  2  1]
 [ 1 14 13  3  3  0]
 [ 3  5  2 30  3  0]
 [ 2 13  6 13 10  0]
 [ 2  3  0  7  1  0]]

2022-12-29 10:42:53,957 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:42:53,957 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:42:53,968 - 

2022-12-29 10:42:53,968 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:42:54,807 - Epoch: [353][   10/  113]    Overall Loss 1.350429    Objective Loss 1.350429                                        LR 0.000006    Time 0.083798    
2022-12-29 10:42:55,443 - Epoch: [353][   20/  113]    Overall Loss 1.323493    Objective Loss 1.323493                                        LR 0.000006    Time 0.073649    
2022-12-29 10:42:56,079 - Epoch: [353][   30/  113]    Overall Loss 1.324531    Objective Loss 1.324531                                        LR 0.000006    Time 0.070294    
2022-12-29 10:42:56,715 - Epoch: [353][   40/  113]    Overall Loss 1.338322    Objective Loss 1.338322                                        LR 0.000006    Time 0.068609    
2022-12-29 10:42:57,346 - Epoch: [353][   50/  113]    Overall Loss 1.331299    Objective Loss 1.331299                                        LR 0.000006    Time 0.067502    
2022-12-29 10:42:57,974 - Epoch: [353][   60/  113]    Overall Loss 1.319784    Objective Loss 1.319784                                        LR 0.000006    Time 0.066701    
2022-12-29 10:42:58,605 - Epoch: [353][   70/  113]    Overall Loss 1.319445    Objective Loss 1.319445                                        LR 0.000006    Time 0.066183    
2022-12-29 10:42:59,234 - Epoch: [353][   80/  113]    Overall Loss 1.312199    Objective Loss 1.312199                                        LR 0.000006    Time 0.065767    
2022-12-29 10:42:59,866 - Epoch: [353][   90/  113]    Overall Loss 1.309275    Objective Loss 1.309275                                        LR 0.000006    Time 0.065482    
2022-12-29 10:43:00,497 - Epoch: [353][  100/  113]    Overall Loss 1.311744    Objective Loss 1.311744                                        LR 0.000006    Time 0.065238    
2022-12-29 10:43:01,128 - Epoch: [353][  110/  113]    Overall Loss 1.307337    Objective Loss 1.307337                                        LR 0.000006    Time 0.065036    
2022-12-29 10:43:01,300 - Epoch: [353][  113/  113]    Overall Loss 1.311139    Objective Loss 1.311139    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064826    
2022-12-29 10:43:01,359 - --- validate (epoch=353)-----------
2022-12-29 10:43:01,360 - 200 samples (16 per mini-batch)
2022-12-29 10:43:01,919 - Epoch: [353][   10/   13]    Loss 1.294896    Top1 52.500000    Top5 98.750000    
2022-12-29 10:43:02,004 - Epoch: [353][   13/   13]    Loss 1.325022    Top1 49.000000    Top5 98.000000    
2022-12-29 10:43:02,072 - ==> Top1: 49.000    Top5: 98.000    Loss: 1.325

2022-12-29 10:43:02,073 - ==> Confusion:
[[18  0  0  9  1  0]
 [ 3 17  5  8  4  0]
 [ 0  6  9 11  4  0]
 [ 6  2  1 46  5  0]
 [ 2 12  4  7  8  0]
 [ 1  3  0  6  2  0]]

2022-12-29 10:43:02,075 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:02,076 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:02,095 - 

2022-12-29 10:43:02,095 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:02,928 - Epoch: [354][   10/  113]    Overall Loss 1.309823    Objective Loss 1.309823                                        LR 0.000006    Time 0.083086    
2022-12-29 10:43:03,571 - Epoch: [354][   20/  113]    Overall Loss 1.270730    Objective Loss 1.270730                                        LR 0.000006    Time 0.073674    
2022-12-29 10:43:04,211 - Epoch: [354][   30/  113]    Overall Loss 1.271737    Objective Loss 1.271737                                        LR 0.000006    Time 0.070420    
2022-12-29 10:43:04,846 - Epoch: [354][   40/  113]    Overall Loss 1.285969    Objective Loss 1.285969                                        LR 0.000006    Time 0.068683    
2022-12-29 10:43:05,480 - Epoch: [354][   50/  113]    Overall Loss 1.262270    Objective Loss 1.262270                                        LR 0.000006    Time 0.067614    
2022-12-29 10:43:06,114 - Epoch: [354][   60/  113]    Overall Loss 1.269182    Objective Loss 1.269182                                        LR 0.000006    Time 0.066913    
2022-12-29 10:43:06,746 - Epoch: [354][   70/  113]    Overall Loss 1.275531    Objective Loss 1.275531                                        LR 0.000006    Time 0.066369    
2022-12-29 10:43:07,381 - Epoch: [354][   80/  113]    Overall Loss 1.271611    Objective Loss 1.271611                                        LR 0.000006    Time 0.066007    
2022-12-29 10:43:08,014 - Epoch: [354][   90/  113]    Overall Loss 1.277534    Objective Loss 1.277534                                        LR 0.000006    Time 0.065701    
2022-12-29 10:43:08,650 - Epoch: [354][  100/  113]    Overall Loss 1.284937    Objective Loss 1.284937                                        LR 0.000006    Time 0.065486    
2022-12-29 10:43:09,286 - Epoch: [354][  110/  113]    Overall Loss 1.289966    Objective Loss 1.289966                                        LR 0.000006    Time 0.065310    
2022-12-29 10:43:09,460 - Epoch: [354][  113/  113]    Overall Loss 1.289321    Objective Loss 1.289321    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.065117    
2022-12-29 10:43:09,515 - --- validate (epoch=354)-----------
2022-12-29 10:43:09,515 - 200 samples (16 per mini-batch)
2022-12-29 10:43:10,056 - Epoch: [354][   10/   13]    Loss 1.206659    Top1 55.625000    Top5 98.750000    
2022-12-29 10:43:10,140 - Epoch: [354][   13/   13]    Loss 1.243134    Top1 52.000000    Top5 99.000000    
2022-12-29 10:43:10,187 - ==> Top1: 52.000    Top5: 99.000    Loss: 1.243

2022-12-29 10:43:10,187 - ==> Confusion:
[[23  3  0  6  0  0]
 [ 2 26  4  9  0  0]
 [ 0  9 11  2  9  0]
 [ 6  3  1 34  3  0]
 [ 2 14  5 10 10  0]
 [ 2  4  1  1  0  0]]

2022-12-29 10:43:10,190 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:10,190 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:10,203 - 

2022-12-29 10:43:10,203 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:11,042 - Epoch: [355][   10/  113]    Overall Loss 1.321107    Objective Loss 1.321107                                        LR 0.000006    Time 0.083784    
2022-12-29 10:43:11,680 - Epoch: [355][   20/  113]    Overall Loss 1.294231    Objective Loss 1.294231                                        LR 0.000006    Time 0.073775    
2022-12-29 10:43:12,317 - Epoch: [355][   30/  113]    Overall Loss 1.288352    Objective Loss 1.288352                                        LR 0.000006    Time 0.070378    
2022-12-29 10:43:12,954 - Epoch: [355][   40/  113]    Overall Loss 1.292888    Objective Loss 1.292888                                        LR 0.000006    Time 0.068698    
2022-12-29 10:43:13,592 - Epoch: [355][   50/  113]    Overall Loss 1.280949    Objective Loss 1.280949                                        LR 0.000006    Time 0.067720    
2022-12-29 10:43:14,220 - Epoch: [355][   60/  113]    Overall Loss 1.296677    Objective Loss 1.296677                                        LR 0.000006    Time 0.066883    
2022-12-29 10:43:14,850 - Epoch: [355][   70/  113]    Overall Loss 1.289566    Objective Loss 1.289566                                        LR 0.000006    Time 0.066330    
2022-12-29 10:43:15,478 - Epoch: [355][   80/  113]    Overall Loss 1.283785    Objective Loss 1.283785                                        LR 0.000006    Time 0.065881    
2022-12-29 10:43:16,109 - Epoch: [355][   90/  113]    Overall Loss 1.292468    Objective Loss 1.292468                                        LR 0.000006    Time 0.065565    
2022-12-29 10:43:16,734 - Epoch: [355][  100/  113]    Overall Loss 1.289480    Objective Loss 1.289480                                        LR 0.000006    Time 0.065254    
2022-12-29 10:43:17,359 - Epoch: [355][  110/  113]    Overall Loss 1.290106    Objective Loss 1.290106                                        LR 0.000006    Time 0.064995    
2022-12-29 10:43:17,534 - Epoch: [355][  113/  113]    Overall Loss 1.289138    Objective Loss 1.289138    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064821    
2022-12-29 10:43:17,596 - --- validate (epoch=355)-----------
2022-12-29 10:43:17,597 - 200 samples (16 per mini-batch)
2022-12-29 10:43:18,156 - Epoch: [355][   10/   13]    Loss 1.354514    Top1 46.250000    Top5 97.500000    
2022-12-29 10:43:18,241 - Epoch: [355][   13/   13]    Loss 1.348890    Top1 45.000000    Top5 97.500000    
2022-12-29 10:43:18,294 - ==> Top1: 45.000    Top5: 97.500    Loss: 1.349

2022-12-29 10:43:18,295 - ==> Confusion:
[[11  5  0 11  0  0]
 [ 4 24  2 10  3  0]
 [ 1 13 13  5  3  0]
 [ 3  4  4 34  6  0]
 [ 1 16  4  7  8  0]
 [ 0  4  0  3  1  0]]

2022-12-29 10:43:18,299 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:18,300 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:18,319 - 

2022-12-29 10:43:18,320 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:19,138 - Epoch: [356][   10/  113]    Overall Loss 1.274967    Objective Loss 1.274967                                        LR 0.000006    Time 0.081744    
2022-12-29 10:43:19,779 - Epoch: [356][   20/  113]    Overall Loss 1.328816    Objective Loss 1.328816                                        LR 0.000006    Time 0.072877    
2022-12-29 10:43:20,420 - Epoch: [356][   30/  113]    Overall Loss 1.303546    Objective Loss 1.303546                                        LR 0.000006    Time 0.069931    
2022-12-29 10:43:21,057 - Epoch: [356][   40/  113]    Overall Loss 1.303847    Objective Loss 1.303847                                        LR 0.000006    Time 0.068367    
2022-12-29 10:43:21,691 - Epoch: [356][   50/  113]    Overall Loss 1.304121    Objective Loss 1.304121                                        LR 0.000006    Time 0.067372    
2022-12-29 10:43:22,324 - Epoch: [356][   60/  113]    Overall Loss 1.304421    Objective Loss 1.304421                                        LR 0.000006    Time 0.066684    
2022-12-29 10:43:22,961 - Epoch: [356][   70/  113]    Overall Loss 1.306213    Objective Loss 1.306213                                        LR 0.000006    Time 0.066249    
2022-12-29 10:43:23,604 - Epoch: [356][   80/  113]    Overall Loss 1.312684    Objective Loss 1.312684                                        LR 0.000006    Time 0.065999    
2022-12-29 10:43:24,238 - Epoch: [356][   90/  113]    Overall Loss 1.304339    Objective Loss 1.304339                                        LR 0.000006    Time 0.065709    
2022-12-29 10:43:24,868 - Epoch: [356][  100/  113]    Overall Loss 1.303403    Objective Loss 1.303403                                        LR 0.000006    Time 0.065427    
2022-12-29 10:43:25,486 - Epoch: [356][  110/  113]    Overall Loss 1.305709    Objective Loss 1.305709                                        LR 0.000006    Time 0.065099    
2022-12-29 10:43:25,663 - Epoch: [356][  113/  113]    Overall Loss 1.310586    Objective Loss 1.310586    Top1 29.166667    Top5 95.833333    LR 0.000006    Time 0.064934    
2022-12-29 10:43:25,715 - --- validate (epoch=356)-----------
2022-12-29 10:43:25,715 - 200 samples (16 per mini-batch)
2022-12-29 10:43:26,273 - Epoch: [356][   10/   13]    Loss 1.288040    Top1 45.625000    Top5 98.125000    
2022-12-29 10:43:26,359 - Epoch: [356][   13/   13]    Loss 1.288996    Top1 44.500000    Top5 98.500000    
2022-12-29 10:43:26,403 - ==> Top1: 44.500    Top5: 98.500    Loss: 1.289

2022-12-29 10:43:26,403 - ==> Confusion:
[[22  3  0  6  1  0]
 [ 2 20  1  7  4  0]
 [ 2 13 15 10  5  0]
 [ 4  6  1 24  7  0]
 [ 0 14  4 13  8  0]
 [ 1  4  0  1  2  0]]

2022-12-29 10:43:26,406 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:26,406 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:26,420 - 

2022-12-29 10:43:26,420 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:27,255 - Epoch: [357][   10/  113]    Overall Loss 1.345922    Objective Loss 1.345922                                        LR 0.000006    Time 0.083421    
2022-12-29 10:43:27,890 - Epoch: [357][   20/  113]    Overall Loss 1.310597    Objective Loss 1.310597                                        LR 0.000006    Time 0.073399    
2022-12-29 10:43:28,532 - Epoch: [357][   30/  113]    Overall Loss 1.326668    Objective Loss 1.326668                                        LR 0.000006    Time 0.070326    
2022-12-29 10:43:29,172 - Epoch: [357][   40/  113]    Overall Loss 1.321419    Objective Loss 1.321419                                        LR 0.000006    Time 0.068750    
2022-12-29 10:43:29,807 - Epoch: [357][   50/  113]    Overall Loss 1.317431    Objective Loss 1.317431                                        LR 0.000006    Time 0.067682    
2022-12-29 10:43:30,439 - Epoch: [357][   60/  113]    Overall Loss 1.318107    Objective Loss 1.318107                                        LR 0.000006    Time 0.066934    
2022-12-29 10:43:31,076 - Epoch: [357][   70/  113]    Overall Loss 1.314309    Objective Loss 1.314309                                        LR 0.000006    Time 0.066457    
2022-12-29 10:43:31,705 - Epoch: [357][   80/  113]    Overall Loss 1.294892    Objective Loss 1.294892                                        LR 0.000006    Time 0.066011    
2022-12-29 10:43:32,335 - Epoch: [357][   90/  113]    Overall Loss 1.299402    Objective Loss 1.299402                                        LR 0.000006    Time 0.065667    
2022-12-29 10:43:32,965 - Epoch: [357][  100/  113]    Overall Loss 1.304738    Objective Loss 1.304738                                        LR 0.000006    Time 0.065401    
2022-12-29 10:43:33,593 - Epoch: [357][  110/  113]    Overall Loss 1.311170    Objective Loss 1.311170                                        LR 0.000006    Time 0.065153    
2022-12-29 10:43:33,768 - Epoch: [357][  113/  113]    Overall Loss 1.308651    Objective Loss 1.308651    Top1 66.666667    Top5 95.833333    LR 0.000006    Time 0.064974    
2022-12-29 10:43:33,823 - --- validate (epoch=357)-----------
2022-12-29 10:43:33,823 - 200 samples (16 per mini-batch)
2022-12-29 10:43:34,375 - Epoch: [357][   10/   13]    Loss 1.321322    Top1 49.375000    Top5 97.500000    
2022-12-29 10:43:34,458 - Epoch: [357][   13/   13]    Loss 1.314524    Top1 48.500000    Top5 97.500000    
2022-12-29 10:43:34,514 - ==> Top1: 48.500    Top5: 97.500    Loss: 1.315

2022-12-29 10:43:34,514 - ==> Confusion:
[[18  2  0  6  2  0]
 [ 5 29  4  5  3  0]
 [ 0  8 12  5  6  0]
 [ 5  6  2 29  2  0]
 [ 0 13  9 10  8  0]
 [ 0  3  1  4  2  1]]

2022-12-29 10:43:34,518 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:34,518 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:34,532 - 

2022-12-29 10:43:34,532 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:35,380 - Epoch: [358][   10/  113]    Overall Loss 1.410809    Objective Loss 1.410809                                        LR 0.000006    Time 0.084694    
2022-12-29 10:43:36,017 - Epoch: [358][   20/  113]    Overall Loss 1.337081    Objective Loss 1.337081                                        LR 0.000006    Time 0.074149    
2022-12-29 10:43:36,653 - Epoch: [358][   30/  113]    Overall Loss 1.312408    Objective Loss 1.312408                                        LR 0.000006    Time 0.070630    
2022-12-29 10:43:37,289 - Epoch: [358][   40/  113]    Overall Loss 1.325742    Objective Loss 1.325742                                        LR 0.000006    Time 0.068848    
2022-12-29 10:43:37,923 - Epoch: [358][   50/  113]    Overall Loss 1.325391    Objective Loss 1.325391                                        LR 0.000006    Time 0.067741    
2022-12-29 10:43:38,563 - Epoch: [358][   60/  113]    Overall Loss 1.317763    Objective Loss 1.317763                                        LR 0.000006    Time 0.067122    
2022-12-29 10:43:39,209 - Epoch: [358][   70/  113]    Overall Loss 1.316968    Objective Loss 1.316968                                        LR 0.000006    Time 0.066750    
2022-12-29 10:43:39,839 - Epoch: [358][   80/  113]    Overall Loss 1.302418    Objective Loss 1.302418                                        LR 0.000006    Time 0.066280    
2022-12-29 10:43:40,475 - Epoch: [358][   90/  113]    Overall Loss 1.305543    Objective Loss 1.305543                                        LR 0.000006    Time 0.065978    
2022-12-29 10:43:41,104 - Epoch: [358][  100/  113]    Overall Loss 1.294827    Objective Loss 1.294827                                        LR 0.000006    Time 0.065665    
2022-12-29 10:43:41,731 - Epoch: [358][  110/  113]    Overall Loss 1.288415    Objective Loss 1.288415                                        LR 0.000006    Time 0.065387    
2022-12-29 10:43:41,905 - Epoch: [358][  113/  113]    Overall Loss 1.288404    Objective Loss 1.288404    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065193    
2022-12-29 10:43:41,968 - --- validate (epoch=358)-----------
2022-12-29 10:43:41,968 - 200 samples (16 per mini-batch)
2022-12-29 10:43:42,520 - Epoch: [358][   10/   13]    Loss 1.359076    Top1 45.000000    Top5 98.750000    
2022-12-29 10:43:42,605 - Epoch: [358][   13/   13]    Loss 1.343160    Top1 45.000000    Top5 98.500000    
2022-12-29 10:43:42,649 - ==> Top1: 45.000    Top5: 98.500    Loss: 1.343

2022-12-29 10:43:42,650 - ==> Confusion:
[[18  4  0  6  0  0]
 [ 5 24  2  9  1  0]
 [ 1 19 12  9  1  0]
 [ 4 10  0 28  4  0]
 [ 4 11  0  5  8  0]
 [ 3  7  0  3  2  0]]

2022-12-29 10:43:42,652 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:42,653 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:42,676 - 

2022-12-29 10:43:42,676 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:43,501 - Epoch: [359][   10/  113]    Overall Loss 1.287834    Objective Loss 1.287834                                        LR 0.000006    Time 0.082376    
2022-12-29 10:43:44,137 - Epoch: [359][   20/  113]    Overall Loss 1.297054    Objective Loss 1.297054                                        LR 0.000006    Time 0.072975    
2022-12-29 10:43:44,775 - Epoch: [359][   30/  113]    Overall Loss 1.283823    Objective Loss 1.283823                                        LR 0.000006    Time 0.069907    
2022-12-29 10:43:45,416 - Epoch: [359][   40/  113]    Overall Loss 1.276094    Objective Loss 1.276094                                        LR 0.000006    Time 0.068419    
2022-12-29 10:43:46,052 - Epoch: [359][   50/  113]    Overall Loss 1.263654    Objective Loss 1.263654                                        LR 0.000006    Time 0.067460    
2022-12-29 10:43:46,683 - Epoch: [359][   60/  113]    Overall Loss 1.263268    Objective Loss 1.263268                                        LR 0.000006    Time 0.066727    
2022-12-29 10:43:47,322 - Epoch: [359][   70/  113]    Overall Loss 1.267002    Objective Loss 1.267002                                        LR 0.000006    Time 0.066304    
2022-12-29 10:43:47,953 - Epoch: [359][   80/  113]    Overall Loss 1.275154    Objective Loss 1.275154                                        LR 0.000006    Time 0.065901    
2022-12-29 10:43:48,592 - Epoch: [359][   90/  113]    Overall Loss 1.277936    Objective Loss 1.277936                                        LR 0.000006    Time 0.065670    
2022-12-29 10:43:49,229 - Epoch: [359][  100/  113]    Overall Loss 1.276453    Objective Loss 1.276453                                        LR 0.000006    Time 0.065468    
2022-12-29 10:43:49,858 - Epoch: [359][  110/  113]    Overall Loss 1.280864    Objective Loss 1.280864                                        LR 0.000006    Time 0.065229    
2022-12-29 10:43:50,027 - Epoch: [359][  113/  113]    Overall Loss 1.285157    Objective Loss 1.285157    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064999    
2022-12-29 10:43:50,082 - --- validate (epoch=359)-----------
2022-12-29 10:43:50,082 - 200 samples (16 per mini-batch)
2022-12-29 10:43:50,629 - Epoch: [359][   10/   13]    Loss 1.342316    Top1 46.250000    Top5 97.500000    
2022-12-29 10:43:50,715 - Epoch: [359][   13/   13]    Loss 1.319443    Top1 46.500000    Top5 97.500000    
2022-12-29 10:43:50,759 - ==> Top1: 46.500    Top5: 97.500    Loss: 1.319

2022-12-29 10:43:50,760 - ==> Confusion:
[[16  5  0  7  0  0]
 [ 2 27  3  9  1  0]
 [ 1 11 11  4  3  0]
 [ 1  5  5 28  2  0]
 [ 3 18  6  9 11  0]
 [ 1  4  1  1  5  0]]

2022-12-29 10:43:50,763 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:50,764 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:50,781 - 

2022-12-29 10:43:50,781 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:51,618 - Epoch: [360][   10/  113]    Overall Loss 1.192574    Objective Loss 1.192574                                        LR 0.000006    Time 0.083531    
2022-12-29 10:43:52,249 - Epoch: [360][   20/  113]    Overall Loss 1.283946    Objective Loss 1.283946                                        LR 0.000006    Time 0.073294    
2022-12-29 10:43:52,874 - Epoch: [360][   30/  113]    Overall Loss 1.302231    Objective Loss 1.302231                                        LR 0.000006    Time 0.069684    
2022-12-29 10:43:53,506 - Epoch: [360][   40/  113]    Overall Loss 1.301452    Objective Loss 1.301452                                        LR 0.000006    Time 0.068061    
2022-12-29 10:43:54,131 - Epoch: [360][   50/  113]    Overall Loss 1.293522    Objective Loss 1.293522                                        LR 0.000006    Time 0.066948    
2022-12-29 10:43:54,759 - Epoch: [360][   60/  113]    Overall Loss 1.286428    Objective Loss 1.286428                                        LR 0.000006    Time 0.066248    
2022-12-29 10:43:55,394 - Epoch: [360][   70/  113]    Overall Loss 1.286452    Objective Loss 1.286452                                        LR 0.000006    Time 0.065837    
2022-12-29 10:43:56,022 - Epoch: [360][   80/  113]    Overall Loss 1.282448    Objective Loss 1.282448                                        LR 0.000006    Time 0.065453    
2022-12-29 10:43:56,657 - Epoch: [360][   90/  113]    Overall Loss 1.277831    Objective Loss 1.277831                                        LR 0.000006    Time 0.065238    
2022-12-29 10:43:57,290 - Epoch: [360][  100/  113]    Overall Loss 1.277027    Objective Loss 1.277027                                        LR 0.000006    Time 0.065040    
2022-12-29 10:43:57,919 - Epoch: [360][  110/  113]    Overall Loss 1.287962    Objective Loss 1.287962                                        LR 0.000006    Time 0.064839    
2022-12-29 10:43:58,094 - Epoch: [360][  113/  113]    Overall Loss 1.286896    Objective Loss 1.286896    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064660    
2022-12-29 10:43:58,155 - --- validate (epoch=360)-----------
2022-12-29 10:43:58,156 - 200 samples (16 per mini-batch)
2022-12-29 10:43:58,704 - Epoch: [360][   10/   13]    Loss 1.332147    Top1 45.625000    Top5 98.750000    
2022-12-29 10:43:58,789 - Epoch: [360][   13/   13]    Loss 1.312291    Top1 46.000000    Top5 98.500000    
2022-12-29 10:43:58,840 - ==> Top1: 46.000    Top5: 98.500    Loss: 1.312

2022-12-29 10:43:58,841 - ==> Confusion:
[[18  2  1  5  3  0]
 [ 1 22  4  7  4  0]
 [ 0 14 12  4  3  0]
 [ 4  5  5 23 10  0]
 [ 2 10  9  3 17  0]
 [ 3  5  0  2  2  0]]

2022-12-29 10:43:58,844 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:43:58,844 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:43:58,868 - 

2022-12-29 10:43:58,869 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:43:59,697 - Epoch: [361][   10/  113]    Overall Loss 1.250146    Objective Loss 1.250146                                        LR 0.000006    Time 0.082705    
2022-12-29 10:44:00,330 - Epoch: [361][   20/  113]    Overall Loss 1.251484    Objective Loss 1.251484                                        LR 0.000006    Time 0.073003    
2022-12-29 10:44:00,962 - Epoch: [361][   30/  113]    Overall Loss 1.227522    Objective Loss 1.227522                                        LR 0.000006    Time 0.069719    
2022-12-29 10:44:01,591 - Epoch: [361][   40/  113]    Overall Loss 1.241194    Objective Loss 1.241194                                        LR 0.000006    Time 0.068002    
2022-12-29 10:44:02,223 - Epoch: [361][   50/  113]    Overall Loss 1.224999    Objective Loss 1.224999                                        LR 0.000006    Time 0.067028    
2022-12-29 10:44:02,854 - Epoch: [361][   60/  113]    Overall Loss 1.257851    Objective Loss 1.257851                                        LR 0.000006    Time 0.066364    
2022-12-29 10:44:03,491 - Epoch: [361][   70/  113]    Overall Loss 1.271569    Objective Loss 1.271569                                        LR 0.000006    Time 0.065983    
2022-12-29 10:44:04,121 - Epoch: [361][   80/  113]    Overall Loss 1.285855    Objective Loss 1.285855                                        LR 0.000006    Time 0.065595    
2022-12-29 10:44:04,753 - Epoch: [361][   90/  113]    Overall Loss 1.287222    Objective Loss 1.287222                                        LR 0.000006    Time 0.065329    
2022-12-29 10:44:05,384 - Epoch: [361][  100/  113]    Overall Loss 1.291095    Objective Loss 1.291095                                        LR 0.000006    Time 0.065100    
2022-12-29 10:44:06,015 - Epoch: [361][  110/  113]    Overall Loss 1.294180    Objective Loss 1.294180                                        LR 0.000006    Time 0.064914    
2022-12-29 10:44:06,184 - Epoch: [361][  113/  113]    Overall Loss 1.296156    Objective Loss 1.296156    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.064678    
2022-12-29 10:44:06,247 - --- validate (epoch=361)-----------
2022-12-29 10:44:06,248 - 200 samples (16 per mini-batch)
2022-12-29 10:44:06,797 - Epoch: [361][   10/   13]    Loss 1.319770    Top1 46.875000    Top5 98.750000    
2022-12-29 10:44:06,881 - Epoch: [361][   13/   13]    Loss 1.332844    Top1 47.000000    Top5 96.000000    
2022-12-29 10:44:06,936 - ==> Top1: 47.000    Top5: 96.000    Loss: 1.333

2022-12-29 10:44:06,937 - ==> Confusion:
[[14  7  2  3  3  0]
 [ 2 30  1  6  2  0]
 [ 0  9  5  6  3  0]
 [ 8 10  1 38  3  0]
 [ 5 13  3 10  6  0]
 [ 2  5  0  1  1  1]]

2022-12-29 10:44:06,940 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:06,940 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:06,955 - 

2022-12-29 10:44:06,956 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:07,785 - Epoch: [362][   10/  113]    Overall Loss 1.245718    Objective Loss 1.245718                                        LR 0.000006    Time 0.082833    
2022-12-29 10:44:08,418 - Epoch: [362][   20/  113]    Overall Loss 1.242201    Objective Loss 1.242201                                        LR 0.000006    Time 0.073041    
2022-12-29 10:44:09,051 - Epoch: [362][   30/  113]    Overall Loss 1.259453    Objective Loss 1.259453                                        LR 0.000006    Time 0.069763    
2022-12-29 10:44:09,683 - Epoch: [362][   40/  113]    Overall Loss 1.252427    Objective Loss 1.252427                                        LR 0.000006    Time 0.068109    
2022-12-29 10:44:10,313 - Epoch: [362][   50/  113]    Overall Loss 1.270123    Objective Loss 1.270123                                        LR 0.000006    Time 0.067085    
2022-12-29 10:44:10,943 - Epoch: [362][   60/  113]    Overall Loss 1.264800    Objective Loss 1.264800                                        LR 0.000006    Time 0.066399    
2022-12-29 10:44:11,575 - Epoch: [362][   70/  113]    Overall Loss 1.269264    Objective Loss 1.269264                                        LR 0.000006    Time 0.065930    
2022-12-29 10:44:12,199 - Epoch: [362][   80/  113]    Overall Loss 1.274316    Objective Loss 1.274316                                        LR 0.000006    Time 0.065487    
2022-12-29 10:44:12,834 - Epoch: [362][   90/  113]    Overall Loss 1.283798    Objective Loss 1.283798                                        LR 0.000006    Time 0.065256    
2022-12-29 10:44:13,475 - Epoch: [362][  100/  113]    Overall Loss 1.288686    Objective Loss 1.288686                                        LR 0.000006    Time 0.065142    
2022-12-29 10:44:14,101 - Epoch: [362][  110/  113]    Overall Loss 1.289368    Objective Loss 1.289368                                        LR 0.000006    Time 0.064904    
2022-12-29 10:44:14,277 - Epoch: [362][  113/  113]    Overall Loss 1.287459    Objective Loss 1.287459    Top1 58.333333    Top5 100.000000    LR 0.000006    Time 0.064737    
2022-12-29 10:44:14,330 - --- validate (epoch=362)-----------
2022-12-29 10:44:14,331 - 200 samples (16 per mini-batch)
2022-12-29 10:44:14,882 - Epoch: [362][   10/   13]    Loss 1.279114    Top1 51.250000    Top5 99.375000    
2022-12-29 10:44:14,967 - Epoch: [362][   13/   13]    Loss 1.299405    Top1 49.000000    Top5 99.500000    
2022-12-29 10:44:15,021 - ==> Top1: 49.000    Top5: 99.500    Loss: 1.299

2022-12-29 10:44:15,022 - ==> Confusion:
[[15  2  0  8  0  0]
 [ 3 30  2  8  7  0]
 [ 0  6 12  5  6  0]
 [ 3  5  2 29  4  0]
 [ 4 13  5  9 12  0]
 [ 1  3  1  1  4  0]]

2022-12-29 10:44:15,026 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:15,026 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:15,038 - 

2022-12-29 10:44:15,038 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:15,890 - Epoch: [363][   10/  113]    Overall Loss 1.276860    Objective Loss 1.276860                                        LR 0.000006    Time 0.085133    
2022-12-29 10:44:16,530 - Epoch: [363][   20/  113]    Overall Loss 1.295389    Objective Loss 1.295389                                        LR 0.000006    Time 0.074519    
2022-12-29 10:44:17,170 - Epoch: [363][   30/  113]    Overall Loss 1.291035    Objective Loss 1.291035                                        LR 0.000006    Time 0.071010    
2022-12-29 10:44:17,811 - Epoch: [363][   40/  113]    Overall Loss 1.322377    Objective Loss 1.322377                                        LR 0.000006    Time 0.069270    
2022-12-29 10:44:18,448 - Epoch: [363][   50/  113]    Overall Loss 1.310027    Objective Loss 1.310027                                        LR 0.000006    Time 0.068137    
2022-12-29 10:44:19,074 - Epoch: [363][   60/  113]    Overall Loss 1.295949    Objective Loss 1.295949                                        LR 0.000006    Time 0.067219    
2022-12-29 10:44:19,708 - Epoch: [363][   70/  113]    Overall Loss 1.291634    Objective Loss 1.291634                                        LR 0.000006    Time 0.066659    
2022-12-29 10:44:20,333 - Epoch: [363][   80/  113]    Overall Loss 1.277112    Objective Loss 1.277112                                        LR 0.000006    Time 0.066127    
2022-12-29 10:44:20,960 - Epoch: [363][   90/  113]    Overall Loss 1.281601    Objective Loss 1.281601                                        LR 0.000006    Time 0.065747    
2022-12-29 10:44:21,592 - Epoch: [363][  100/  113]    Overall Loss 1.278431    Objective Loss 1.278431                                        LR 0.000006    Time 0.065487    
2022-12-29 10:44:22,209 - Epoch: [363][  110/  113]    Overall Loss 1.284551    Objective Loss 1.284551                                        LR 0.000006    Time 0.065141    
2022-12-29 10:44:22,384 - Epoch: [363][  113/  113]    Overall Loss 1.284447    Objective Loss 1.284447    Top1 58.333333    Top5 87.500000    LR 0.000006    Time 0.064958    
2022-12-29 10:44:22,444 - --- validate (epoch=363)-----------
2022-12-29 10:44:22,444 - 200 samples (16 per mini-batch)
2022-12-29 10:44:22,990 - Epoch: [363][   10/   13]    Loss 1.252904    Top1 55.000000    Top5 97.500000    
2022-12-29 10:44:23,075 - Epoch: [363][   13/   13]    Loss 1.284642    Top1 53.500000    Top5 98.000000    
2022-12-29 10:44:23,118 - ==> Top1: 53.500    Top5: 98.000    Loss: 1.285

2022-12-29 10:44:23,119 - ==> Confusion:
[[19  6  1  5  0  0]
 [ 3 20  1  7  3  0]
 [ 3  8 16  2  5  0]
 [ 4  4  2 38  6  0]
 [ 3  8  1  9 13  0]
 [ 1  4  3  2  2  1]]

2022-12-29 10:44:23,122 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:23,122 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:23,145 - 

2022-12-29 10:44:23,145 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:23,996 - Epoch: [364][   10/  113]    Overall Loss 1.304443    Objective Loss 1.304443                                        LR 0.000006    Time 0.084966    
2022-12-29 10:44:24,637 - Epoch: [364][   20/  113]    Overall Loss 1.313077    Objective Loss 1.313077                                        LR 0.000006    Time 0.074507    
2022-12-29 10:44:25,276 - Epoch: [364][   30/  113]    Overall Loss 1.283532    Objective Loss 1.283532                                        LR 0.000006    Time 0.070944    
2022-12-29 10:44:25,916 - Epoch: [364][   40/  113]    Overall Loss 1.296654    Objective Loss 1.296654                                        LR 0.000006    Time 0.069202    
2022-12-29 10:44:26,546 - Epoch: [364][   50/  113]    Overall Loss 1.271389    Objective Loss 1.271389                                        LR 0.000006    Time 0.067962    
2022-12-29 10:44:27,177 - Epoch: [364][   60/  113]    Overall Loss 1.260947    Objective Loss 1.260947                                        LR 0.000006    Time 0.067131    
2022-12-29 10:44:27,809 - Epoch: [364][   70/  113]    Overall Loss 1.281730    Objective Loss 1.281730                                        LR 0.000006    Time 0.066448    
2022-12-29 10:44:28,433 - Epoch: [364][   80/  113]    Overall Loss 1.265832    Objective Loss 1.265832                                        LR 0.000006    Time 0.065944    
2022-12-29 10:44:29,063 - Epoch: [364][   90/  113]    Overall Loss 1.266558    Objective Loss 1.266558                                        LR 0.000006    Time 0.065614    
2022-12-29 10:44:29,694 - Epoch: [364][  100/  113]    Overall Loss 1.272548    Objective Loss 1.272548                                        LR 0.000006    Time 0.065355    
2022-12-29 10:44:30,316 - Epoch: [364][  110/  113]    Overall Loss 1.270926    Objective Loss 1.270926                                        LR 0.000006    Time 0.065062    
2022-12-29 10:44:30,487 - Epoch: [364][  113/  113]    Overall Loss 1.270106    Objective Loss 1.270106    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.064847    
2022-12-29 10:44:30,540 - --- validate (epoch=364)-----------
2022-12-29 10:44:30,540 - 200 samples (16 per mini-batch)
2022-12-29 10:44:31,087 - Epoch: [364][   10/   13]    Loss 1.347599    Top1 44.375000    Top5 98.125000    
2022-12-29 10:44:31,171 - Epoch: [364][   13/   13]    Loss 1.357005    Top1 44.500000    Top5 98.000000    
2022-12-29 10:44:31,221 - ==> Top1: 44.500    Top5: 98.000    Loss: 1.357

2022-12-29 10:44:31,222 - ==> Confusion:
[[14  2  0  8  4  0]
 [ 3 20  2 10  7  0]
 [ 2  9 12 10  6  0]
 [ 7  3  3 33  1  0]
 [ 7  4  3 13 10  0]
 [ 1  1  0  2  3  0]]

2022-12-29 10:44:31,225 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:31,226 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:31,239 - 

2022-12-29 10:44:31,239 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:32,073 - Epoch: [365][   10/  113]    Overall Loss 1.283369    Objective Loss 1.283369                                        LR 0.000006    Time 0.083226    
2022-12-29 10:44:32,710 - Epoch: [365][   20/  113]    Overall Loss 1.265270    Objective Loss 1.265270                                        LR 0.000006    Time 0.073480    
2022-12-29 10:44:33,350 - Epoch: [365][   30/  113]    Overall Loss 1.288999    Objective Loss 1.288999                                        LR 0.000006    Time 0.070279    
2022-12-29 10:44:33,986 - Epoch: [365][   40/  113]    Overall Loss 1.303650    Objective Loss 1.303650                                        LR 0.000006    Time 0.068597    
2022-12-29 10:44:34,617 - Epoch: [365][   50/  113]    Overall Loss 1.284047    Objective Loss 1.284047                                        LR 0.000006    Time 0.067495    
2022-12-29 10:44:35,243 - Epoch: [365][   60/  113]    Overall Loss 1.283296    Objective Loss 1.283296                                        LR 0.000006    Time 0.066669    
2022-12-29 10:44:35,874 - Epoch: [365][   70/  113]    Overall Loss 1.275510    Objective Loss 1.275510                                        LR 0.000006    Time 0.066159    
2022-12-29 10:44:36,502 - Epoch: [365][   80/  113]    Overall Loss 1.286527    Objective Loss 1.286527                                        LR 0.000006    Time 0.065729    
2022-12-29 10:44:37,132 - Epoch: [365][   90/  113]    Overall Loss 1.304355    Objective Loss 1.304355                                        LR 0.000006    Time 0.065424    
2022-12-29 10:44:37,758 - Epoch: [365][  100/  113]    Overall Loss 1.295370    Objective Loss 1.295370                                        LR 0.000006    Time 0.065131    
2022-12-29 10:44:38,380 - Epoch: [365][  110/  113]    Overall Loss 1.299376    Objective Loss 1.299376                                        LR 0.000006    Time 0.064860    
2022-12-29 10:44:38,551 - Epoch: [365][  113/  113]    Overall Loss 1.300003    Objective Loss 1.300003    Top1 45.833333    Top5 100.000000    LR 0.000006    Time 0.064653    
2022-12-29 10:44:38,603 - --- validate (epoch=365)-----------
2022-12-29 10:44:38,604 - 200 samples (16 per mini-batch)
2022-12-29 10:44:39,139 - Epoch: [365][   10/   13]    Loss 1.290034    Top1 51.250000    Top5 96.875000    
2022-12-29 10:44:39,224 - Epoch: [365][   13/   13]    Loss 1.291846    Top1 50.500000    Top5 96.500000    
2022-12-29 10:44:39,271 - ==> Top1: 50.500    Top5: 96.500    Loss: 1.292

2022-12-29 10:44:39,272 - ==> Confusion:
[[16  5  2  3  1  1]
 [ 1 32  2  6  8  0]
 [ 0 11  6  6  8  0]
 [ 3  6  1 36  6  0]
 [ 0 11  3  5 11  0]
 [ 1  5  1  2  1  0]]

2022-12-29 10:44:39,275 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:39,275 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:39,297 - 

2022-12-29 10:44:39,298 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:40,133 - Epoch: [366][   10/  113]    Overall Loss 1.299976    Objective Loss 1.299976                                        LR 0.000006    Time 0.083437    
2022-12-29 10:44:40,770 - Epoch: [366][   20/  113]    Overall Loss 1.252780    Objective Loss 1.252780                                        LR 0.000006    Time 0.073522    
2022-12-29 10:44:41,401 - Epoch: [366][   30/  113]    Overall Loss 1.262395    Objective Loss 1.262395                                        LR 0.000006    Time 0.070032    
2022-12-29 10:44:42,030 - Epoch: [366][   40/  113]    Overall Loss 1.257153    Objective Loss 1.257153                                        LR 0.000006    Time 0.068233    
2022-12-29 10:44:42,657 - Epoch: [366][   50/  113]    Overall Loss 1.259037    Objective Loss 1.259037                                        LR 0.000006    Time 0.067126    
2022-12-29 10:44:43,285 - Epoch: [366][   60/  113]    Overall Loss 1.255504    Objective Loss 1.255504                                        LR 0.000006    Time 0.066406    
2022-12-29 10:44:43,915 - Epoch: [366][   70/  113]    Overall Loss 1.254473    Objective Loss 1.254473                                        LR 0.000006    Time 0.065912    
2022-12-29 10:44:44,540 - Epoch: [366][   80/  113]    Overall Loss 1.248084    Objective Loss 1.248084                                        LR 0.000006    Time 0.065480    
2022-12-29 10:44:45,170 - Epoch: [366][   90/  113]    Overall Loss 1.252517    Objective Loss 1.252517                                        LR 0.000006    Time 0.065192    
2022-12-29 10:44:45,802 - Epoch: [366][  100/  113]    Overall Loss 1.262784    Objective Loss 1.262784                                        LR 0.000006    Time 0.064990    
2022-12-29 10:44:46,426 - Epoch: [366][  110/  113]    Overall Loss 1.265243    Objective Loss 1.265243                                        LR 0.000006    Time 0.064754    
2022-12-29 10:44:46,603 - Epoch: [366][  113/  113]    Overall Loss 1.261663    Objective Loss 1.261663    Top1 62.500000    Top5 95.833333    LR 0.000006    Time 0.064594    
2022-12-29 10:44:46,658 - --- validate (epoch=366)-----------
2022-12-29 10:44:46,659 - 200 samples (16 per mini-batch)
2022-12-29 10:44:47,218 - Epoch: [366][   10/   13]    Loss 1.342785    Top1 43.125000    Top5 98.750000    
2022-12-29 10:44:47,302 - Epoch: [366][   13/   13]    Loss 1.318692    Top1 43.000000    Top5 98.500000    
2022-12-29 10:44:47,347 - ==> Top1: 43.000    Top5: 98.500    Loss: 1.319

2022-12-29 10:44:47,347 - ==> Confusion:
[[25  7  0  4  1  0]
 [ 0 14  1  6  5  1]
 [ 1 11 11  8  9  0]
 [ 3  3  3 26  2  0]
 [ 3 17  3 12 10  0]
 [ 2  4  1  4  3  0]]

2022-12-29 10:44:47,350 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:47,350 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:47,373 - 

2022-12-29 10:44:47,374 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:48,215 - Epoch: [367][   10/  113]    Overall Loss 1.244646    Objective Loss 1.244646                                        LR 0.000006    Time 0.083981    
2022-12-29 10:44:48,854 - Epoch: [367][   20/  113]    Overall Loss 1.299658    Objective Loss 1.299658                                        LR 0.000006    Time 0.073905    
2022-12-29 10:44:49,491 - Epoch: [367][   30/  113]    Overall Loss 1.293107    Objective Loss 1.293107                                        LR 0.000006    Time 0.070502    
2022-12-29 10:44:50,133 - Epoch: [367][   40/  113]    Overall Loss 1.299168    Objective Loss 1.299168                                        LR 0.000006    Time 0.068908    
2022-12-29 10:44:50,767 - Epoch: [367][   50/  113]    Overall Loss 1.295997    Objective Loss 1.295997                                        LR 0.000006    Time 0.067798    
2022-12-29 10:44:51,398 - Epoch: [367][   60/  113]    Overall Loss 1.302441    Objective Loss 1.302441                                        LR 0.000006    Time 0.067004    
2022-12-29 10:44:52,034 - Epoch: [367][   70/  113]    Overall Loss 1.304040    Objective Loss 1.304040                                        LR 0.000006    Time 0.066502    
2022-12-29 10:44:52,669 - Epoch: [367][   80/  113]    Overall Loss 1.299101    Objective Loss 1.299101                                        LR 0.000006    Time 0.066120    
2022-12-29 10:44:53,303 - Epoch: [367][   90/  113]    Overall Loss 1.311168    Objective Loss 1.311168                                        LR 0.000006    Time 0.065819    
2022-12-29 10:44:53,935 - Epoch: [367][  100/  113]    Overall Loss 1.308779    Objective Loss 1.308779                                        LR 0.000006    Time 0.065545    
2022-12-29 10:44:54,563 - Epoch: [367][  110/  113]    Overall Loss 1.306355    Objective Loss 1.306355                                        LR 0.000006    Time 0.065299    
2022-12-29 10:44:54,740 - Epoch: [367][  113/  113]    Overall Loss 1.307206    Objective Loss 1.307206    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.065129    
2022-12-29 10:44:54,806 - --- validate (epoch=367)-----------
2022-12-29 10:44:54,807 - 200 samples (16 per mini-batch)
2022-12-29 10:44:55,360 - Epoch: [367][   10/   13]    Loss 1.223165    Top1 50.000000    Top5 100.000000    
2022-12-29 10:44:55,443 - Epoch: [367][   13/   13]    Loss 1.255655    Top1 49.000000    Top5 99.000000    
2022-12-29 10:44:55,491 - ==> Top1: 49.000    Top5: 99.000    Loss: 1.256

2022-12-29 10:44:55,491 - ==> Confusion:
[[22  2  1  6  0  0]
 [ 1 28  1  8  6  0]
 [ 0 11  8  5 11  0]
 [ 5  7  2 27  0  0]
 [ 7  9  4  7 13  0]
 [ 2  1  0  4  2  0]]

2022-12-29 10:44:55,494 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:44:55,494 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:44:55,519 - 

2022-12-29 10:44:55,519 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:44:56,347 - Epoch: [368][   10/  113]    Overall Loss 1.218429    Objective Loss 1.218429                                        LR 0.000006    Time 0.082685    
2022-12-29 10:44:56,985 - Epoch: [368][   20/  113]    Overall Loss 1.271132    Objective Loss 1.271132                                        LR 0.000006    Time 0.073192    
2022-12-29 10:44:57,622 - Epoch: [368][   30/  113]    Overall Loss 1.277206    Objective Loss 1.277206                                        LR 0.000006    Time 0.070031    
2022-12-29 10:44:58,258 - Epoch: [368][   40/  113]    Overall Loss 1.270585    Objective Loss 1.270585                                        LR 0.000006    Time 0.068402    
2022-12-29 10:44:58,893 - Epoch: [368][   50/  113]    Overall Loss 1.279802    Objective Loss 1.279802                                        LR 0.000006    Time 0.067413    
2022-12-29 10:44:59,525 - Epoch: [368][   60/  113]    Overall Loss 1.272982    Objective Loss 1.272982                                        LR 0.000006    Time 0.066698    
2022-12-29 10:45:00,162 - Epoch: [368][   70/  113]    Overall Loss 1.271090    Objective Loss 1.271090                                        LR 0.000006    Time 0.066265    
2022-12-29 10:45:00,802 - Epoch: [368][   80/  113]    Overall Loss 1.273698    Objective Loss 1.273698                                        LR 0.000006    Time 0.065981    
2022-12-29 10:45:01,437 - Epoch: [368][   90/  113]    Overall Loss 1.275328    Objective Loss 1.275328                                        LR 0.000006    Time 0.065704    
2022-12-29 10:45:02,074 - Epoch: [368][  100/  113]    Overall Loss 1.277317    Objective Loss 1.277317                                        LR 0.000006    Time 0.065489    
2022-12-29 10:45:02,698 - Epoch: [368][  110/  113]    Overall Loss 1.282710    Objective Loss 1.282710                                        LR 0.000006    Time 0.065209    
2022-12-29 10:45:02,873 - Epoch: [368][  113/  113]    Overall Loss 1.286577    Objective Loss 1.286577    Top1 45.833333    Top5 87.500000    LR 0.000006    Time 0.065022    
2022-12-29 10:45:02,932 - --- validate (epoch=368)-----------
2022-12-29 10:45:02,933 - 200 samples (16 per mini-batch)
2022-12-29 10:45:03,499 - Epoch: [368][   10/   13]    Loss 1.334218    Top1 45.000000    Top5 99.375000    
2022-12-29 10:45:03,585 - Epoch: [368][   13/   13]    Loss 1.299507    Top1 46.000000    Top5 99.000000    
2022-12-29 10:45:03,651 - ==> Top1: 46.000    Top5: 99.000    Loss: 1.300

2022-12-29 10:45:03,652 - ==> Confusion:
[[15  2  1  5  0  0]
 [ 3 19  8 13  7  0]
 [ 2  5 15  5 10  0]
 [ 1  1  6 35  5  0]
 [ 5  4  4  9  8  0]
 [ 1  3  3  2  3  0]]

2022-12-29 10:45:03,655 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:03,656 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:03,671 - 

2022-12-29 10:45:03,671 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:04,497 - Epoch: [369][   10/  113]    Overall Loss 1.223276    Objective Loss 1.223276                                        LR 0.000006    Time 0.082541    
2022-12-29 10:45:05,134 - Epoch: [369][   20/  113]    Overall Loss 1.293375    Objective Loss 1.293375                                        LR 0.000006    Time 0.073096    
2022-12-29 10:45:05,765 - Epoch: [369][   30/  113]    Overall Loss 1.313247    Objective Loss 1.313247                                        LR 0.000006    Time 0.069731    
2022-12-29 10:45:06,396 - Epoch: [369][   40/  113]    Overall Loss 1.301403    Objective Loss 1.301403                                        LR 0.000006    Time 0.068050    
2022-12-29 10:45:07,030 - Epoch: [369][   50/  113]    Overall Loss 1.294507    Objective Loss 1.294507                                        LR 0.000006    Time 0.067118    
2022-12-29 10:45:07,662 - Epoch: [369][   60/  113]    Overall Loss 1.296399    Objective Loss 1.296399                                        LR 0.000006    Time 0.066464    
2022-12-29 10:45:08,300 - Epoch: [369][   70/  113]    Overall Loss 1.300592    Objective Loss 1.300592                                        LR 0.000006    Time 0.066068    
2022-12-29 10:45:08,929 - Epoch: [369][   80/  113]    Overall Loss 1.290445    Objective Loss 1.290445                                        LR 0.000006    Time 0.065665    
2022-12-29 10:45:09,568 - Epoch: [369][   90/  113]    Overall Loss 1.296484    Objective Loss 1.296484                                        LR 0.000006    Time 0.065464    
2022-12-29 10:45:10,200 - Epoch: [369][  100/  113]    Overall Loss 1.297187    Objective Loss 1.297187                                        LR 0.000006    Time 0.065236    
2022-12-29 10:45:10,823 - Epoch: [369][  110/  113]    Overall Loss 1.311018    Objective Loss 1.311018                                        LR 0.000006    Time 0.064962    
2022-12-29 10:45:10,993 - Epoch: [369][  113/  113]    Overall Loss 1.306957    Objective Loss 1.306957    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064739    
2022-12-29 10:45:11,041 - --- validate (epoch=369)-----------
2022-12-29 10:45:11,041 - 200 samples (16 per mini-batch)
2022-12-29 10:45:11,585 - Epoch: [369][   10/   13]    Loss 1.254869    Top1 53.750000    Top5 97.500000    
2022-12-29 10:45:11,670 - Epoch: [369][   13/   13]    Loss 1.337183    Top1 49.000000    Top5 96.500000    
2022-12-29 10:45:11,715 - ==> Top1: 49.000    Top5: 96.500    Loss: 1.337

2022-12-29 10:45:11,716 - ==> Confusion:
[[18  8  0  9  1  0]
 [ 5 23  2  1  3  0]
 [ 2 12  8  4  5  0]
 [ 2  6  0 35  6  0]
 [ 1 15  4  6 14  0]
 [ 1  3  1  3  2  0]]

2022-12-29 10:45:11,718 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:11,718 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:11,736 - 

2022-12-29 10:45:11,737 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:12,592 - Epoch: [370][   10/  113]    Overall Loss 1.290622    Objective Loss 1.290622                                        LR 0.000006    Time 0.085301    
2022-12-29 10:45:13,228 - Epoch: [370][   20/  113]    Overall Loss 1.311712    Objective Loss 1.311712                                        LR 0.000006    Time 0.074409    
2022-12-29 10:45:13,871 - Epoch: [370][   30/  113]    Overall Loss 1.292039    Objective Loss 1.292039                                        LR 0.000006    Time 0.071019    
2022-12-29 10:45:14,510 - Epoch: [370][   40/  113]    Overall Loss 1.308803    Objective Loss 1.308803                                        LR 0.000006    Time 0.069230    
2022-12-29 10:45:15,141 - Epoch: [370][   50/  113]    Overall Loss 1.300384    Objective Loss 1.300384                                        LR 0.000006    Time 0.067990    
2022-12-29 10:45:15,773 - Epoch: [370][   60/  113]    Overall Loss 1.304977    Objective Loss 1.304977                                        LR 0.000006    Time 0.067188    
2022-12-29 10:45:16,408 - Epoch: [370][   70/  113]    Overall Loss 1.296097    Objective Loss 1.296097                                        LR 0.000006    Time 0.066651    
2022-12-29 10:45:17,035 - Epoch: [370][   80/  113]    Overall Loss 1.286338    Objective Loss 1.286338                                        LR 0.000006    Time 0.066151    
2022-12-29 10:45:17,669 - Epoch: [370][   90/  113]    Overall Loss 1.284069    Objective Loss 1.284069                                        LR 0.000006    Time 0.065841    
2022-12-29 10:45:18,304 - Epoch: [370][  100/  113]    Overall Loss 1.287982    Objective Loss 1.287982                                        LR 0.000006    Time 0.065598    
2022-12-29 10:45:18,934 - Epoch: [370][  110/  113]    Overall Loss 1.286892    Objective Loss 1.286892                                        LR 0.000006    Time 0.065361    
2022-12-29 10:45:19,105 - Epoch: [370][  113/  113]    Overall Loss 1.287338    Objective Loss 1.287338    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.065138    
2022-12-29 10:45:19,157 - --- validate (epoch=370)-----------
2022-12-29 10:45:19,157 - 200 samples (16 per mini-batch)
2022-12-29 10:45:19,712 - Epoch: [370][   10/   13]    Loss 1.276204    Top1 49.375000    Top5 98.125000    
2022-12-29 10:45:19,798 - Epoch: [370][   13/   13]    Loss 1.269137    Top1 48.500000    Top5 98.500000    
2022-12-29 10:45:19,860 - ==> Top1: 48.500    Top5: 98.500    Loss: 1.269

2022-12-29 10:45:19,860 - ==> Confusion:
[[19  7  1  6  1  0]
 [ 4 24  1  7  5  0]
 [ 0 14 10  9  7  0]
 [ 3  7  0 33  2  0]
 [ 3 10  0  8 10  0]
 [ 1  4  1  1  1  1]]

2022-12-29 10:45:19,863 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:19,864 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:19,887 - 

2022-12-29 10:45:19,888 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:20,734 - Epoch: [371][   10/  113]    Overall Loss 1.307320    Objective Loss 1.307320                                        LR 0.000006    Time 0.084528    
2022-12-29 10:45:21,374 - Epoch: [371][   20/  113]    Overall Loss 1.267943    Objective Loss 1.267943                                        LR 0.000006    Time 0.074251    
2022-12-29 10:45:22,020 - Epoch: [371][   30/  113]    Overall Loss 1.273482    Objective Loss 1.273482                                        LR 0.000006    Time 0.071002    
2022-12-29 10:45:22,661 - Epoch: [371][   40/  113]    Overall Loss 1.285189    Objective Loss 1.285189                                        LR 0.000006    Time 0.069260    
2022-12-29 10:45:23,296 - Epoch: [371][   50/  113]    Overall Loss 1.274985    Objective Loss 1.274985                                        LR 0.000006    Time 0.068119    
2022-12-29 10:45:23,928 - Epoch: [371][   60/  113]    Overall Loss 1.280959    Objective Loss 1.280959                                        LR 0.000006    Time 0.067280    
2022-12-29 10:45:24,564 - Epoch: [371][   70/  113]    Overall Loss 1.278551    Objective Loss 1.278551                                        LR 0.000006    Time 0.066744    
2022-12-29 10:45:25,194 - Epoch: [371][   80/  113]    Overall Loss 1.284261    Objective Loss 1.284261                                        LR 0.000006    Time 0.066277    
2022-12-29 10:45:25,824 - Epoch: [371][   90/  113]    Overall Loss 1.280637    Objective Loss 1.280637                                        LR 0.000006    Time 0.065905    
2022-12-29 10:45:26,456 - Epoch: [371][  100/  113]    Overall Loss 1.277563    Objective Loss 1.277563                                        LR 0.000006    Time 0.065625    
2022-12-29 10:45:27,082 - Epoch: [371][  110/  113]    Overall Loss 1.270224    Objective Loss 1.270224                                        LR 0.000006    Time 0.065352    
2022-12-29 10:45:27,257 - Epoch: [371][  113/  113]    Overall Loss 1.270367    Objective Loss 1.270367    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.065158    
2022-12-29 10:45:27,309 - --- validate (epoch=371)-----------
2022-12-29 10:45:27,309 - 200 samples (16 per mini-batch)
2022-12-29 10:45:27,859 - Epoch: [371][   10/   13]    Loss 1.281868    Top1 46.875000    Top5 98.750000    
2022-12-29 10:45:27,947 - Epoch: [371][   13/   13]    Loss 1.285727    Top1 48.500000    Top5 98.500000    
2022-12-29 10:45:28,007 - ==> Top1: 48.500    Top5: 98.500    Loss: 1.286

2022-12-29 10:45:28,007 - ==> Confusion:
[[19  4  0  7  0  0]
 [ 5 18  4  5  7  0]
 [ 0  8 15  7  3  0]
 [ 7  7  2 28  2  0]
 [ 4 11  2 10 17  1]
 [ 1  3  1  1  1  0]]

2022-12-29 10:45:28,011 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:28,012 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:28,034 - 

2022-12-29 10:45:28,035 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:28,865 - Epoch: [372][   10/  113]    Overall Loss 1.305098    Objective Loss 1.305098                                        LR 0.000006    Time 0.082760    
2022-12-29 10:45:29,500 - Epoch: [372][   20/  113]    Overall Loss 1.245625    Objective Loss 1.245625                                        LR 0.000006    Time 0.073111    
2022-12-29 10:45:30,146 - Epoch: [372][   30/  113]    Overall Loss 1.284794    Objective Loss 1.284794                                        LR 0.000006    Time 0.070255    
2022-12-29 10:45:30,780 - Epoch: [372][   40/  113]    Overall Loss 1.286347    Objective Loss 1.286347                                        LR 0.000006    Time 0.068530    
2022-12-29 10:45:31,413 - Epoch: [372][   50/  113]    Overall Loss 1.272296    Objective Loss 1.272296                                        LR 0.000006    Time 0.067476    
2022-12-29 10:45:32,048 - Epoch: [372][   60/  113]    Overall Loss 1.280562    Objective Loss 1.280562                                        LR 0.000006    Time 0.066803    
2022-12-29 10:45:32,676 - Epoch: [372][   70/  113]    Overall Loss 1.287311    Objective Loss 1.287311                                        LR 0.000006    Time 0.066234    
2022-12-29 10:45:33,309 - Epoch: [372][   80/  113]    Overall Loss 1.271469    Objective Loss 1.271469                                        LR 0.000006    Time 0.065853    
2022-12-29 10:45:33,941 - Epoch: [372][   90/  113]    Overall Loss 1.275233    Objective Loss 1.275233                                        LR 0.000006    Time 0.065556    
2022-12-29 10:45:34,572 - Epoch: [372][  100/  113]    Overall Loss 1.273358    Objective Loss 1.273358                                        LR 0.000006    Time 0.065309    
2022-12-29 10:45:35,192 - Epoch: [372][  110/  113]    Overall Loss 1.272340    Objective Loss 1.272340                                        LR 0.000006    Time 0.065003    
2022-12-29 10:45:35,366 - Epoch: [372][  113/  113]    Overall Loss 1.274283    Objective Loss 1.274283    Top1 37.500000    Top5 95.833333    LR 0.000006    Time 0.064815    
2022-12-29 10:45:35,422 - --- validate (epoch=372)-----------
2022-12-29 10:45:35,423 - 200 samples (16 per mini-batch)
2022-12-29 10:45:35,968 - Epoch: [372][   10/   13]    Loss 1.278199    Top1 51.250000    Top5 95.000000    
2022-12-29 10:45:36,055 - Epoch: [372][   13/   13]    Loss 1.283540    Top1 51.000000    Top5 96.000000    
2022-12-29 10:45:36,103 - ==> Top1: 51.000    Top5: 96.000    Loss: 1.284

2022-12-29 10:45:36,104 - ==> Confusion:
[[18  8  2  4  2  0]
 [ 2 27  3  5  3  0]
 [ 1  9 12  7  2  0]
 [ 4  8  2 31  4  0]
 [ 2  9  5  7 14  0]
 [ 1  3  2  2  1  0]]

2022-12-29 10:45:36,109 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:36,109 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:36,126 - 

2022-12-29 10:45:36,127 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:36,973 - Epoch: [373][   10/  113]    Overall Loss 1.216009    Objective Loss 1.216009                                        LR 0.000006    Time 0.084498    
2022-12-29 10:45:37,611 - Epoch: [373][   20/  113]    Overall Loss 1.246053    Objective Loss 1.246053                                        LR 0.000006    Time 0.074119    
2022-12-29 10:45:38,252 - Epoch: [373][   30/  113]    Overall Loss 1.244417    Objective Loss 1.244417                                        LR 0.000006    Time 0.070761    
2022-12-29 10:45:38,885 - Epoch: [373][   40/  113]    Overall Loss 1.229620    Objective Loss 1.229620                                        LR 0.000006    Time 0.068884    
2022-12-29 10:45:39,514 - Epoch: [373][   50/  113]    Overall Loss 1.236995    Objective Loss 1.236995                                        LR 0.000006    Time 0.067674    
2022-12-29 10:45:40,149 - Epoch: [373][   60/  113]    Overall Loss 1.252643    Objective Loss 1.252643                                        LR 0.000006    Time 0.066973    
2022-12-29 10:45:40,784 - Epoch: [373][   70/  113]    Overall Loss 1.249066    Objective Loss 1.249066                                        LR 0.000006    Time 0.066470    
2022-12-29 10:45:41,415 - Epoch: [373][   80/  113]    Overall Loss 1.246389    Objective Loss 1.246389                                        LR 0.000006    Time 0.066039    
2022-12-29 10:45:42,051 - Epoch: [373][   90/  113]    Overall Loss 1.247513    Objective Loss 1.247513                                        LR 0.000006    Time 0.065767    
2022-12-29 10:45:42,684 - Epoch: [373][  100/  113]    Overall Loss 1.249338    Objective Loss 1.249338                                        LR 0.000006    Time 0.065513    
2022-12-29 10:45:43,317 - Epoch: [373][  110/  113]    Overall Loss 1.251674    Objective Loss 1.251674                                        LR 0.000006    Time 0.065307    
2022-12-29 10:45:43,497 - Epoch: [373][  113/  113]    Overall Loss 1.252702    Objective Loss 1.252702    Top1 62.500000    Top5 95.833333    LR 0.000006    Time 0.065163    
2022-12-29 10:45:43,556 - --- validate (epoch=373)-----------
2022-12-29 10:45:43,557 - 200 samples (16 per mini-batch)
2022-12-29 10:45:44,103 - Epoch: [373][   10/   13]    Loss 1.350491    Top1 46.250000    Top5 94.375000    
2022-12-29 10:45:44,191 - Epoch: [373][   13/   13]    Loss 1.361013    Top1 46.000000    Top5 94.000000    
2022-12-29 10:45:44,253 - ==> Top1: 46.000    Top5: 94.000    Loss: 1.361

2022-12-29 10:45:44,253 - ==> Confusion:
[[10  1  0  6  1  0]
 [ 3 21  2  8  4  0]
 [ 4  5  6 12  4  0]
 [ 4  4  3 42  6  0]
 [ 3  9  1 10 12  1]
 [ 2  6  1  4  4  1]]

2022-12-29 10:45:44,258 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:44,258 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:44,275 - 

2022-12-29 10:45:44,275 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:45,117 - Epoch: [374][   10/  113]    Overall Loss 1.415443    Objective Loss 1.415443                                        LR 0.000006    Time 0.084042    
2022-12-29 10:45:45,753 - Epoch: [374][   20/  113]    Overall Loss 1.340637    Objective Loss 1.340637                                        LR 0.000006    Time 0.073805    
2022-12-29 10:45:46,396 - Epoch: [374][   30/  113]    Overall Loss 1.340753    Objective Loss 1.340753                                        LR 0.000006    Time 0.070615    
2022-12-29 10:45:47,028 - Epoch: [374][   40/  113]    Overall Loss 1.312432    Objective Loss 1.312432                                        LR 0.000006    Time 0.068758    
2022-12-29 10:45:47,663 - Epoch: [374][   50/  113]    Overall Loss 1.311688    Objective Loss 1.311688                                        LR 0.000006    Time 0.067693    
2022-12-29 10:45:48,297 - Epoch: [374][   60/  113]    Overall Loss 1.295114    Objective Loss 1.295114                                        LR 0.000006    Time 0.066977    
2022-12-29 10:45:48,927 - Epoch: [374][   70/  113]    Overall Loss 1.283693    Objective Loss 1.283693                                        LR 0.000006    Time 0.066406    
2022-12-29 10:45:49,559 - Epoch: [374][   80/  113]    Overall Loss 1.270520    Objective Loss 1.270520                                        LR 0.000006    Time 0.065992    
2022-12-29 10:45:50,191 - Epoch: [374][   90/  113]    Overall Loss 1.270813    Objective Loss 1.270813                                        LR 0.000006    Time 0.065683    
2022-12-29 10:45:50,822 - Epoch: [374][  100/  113]    Overall Loss 1.275800    Objective Loss 1.275800                                        LR 0.000006    Time 0.065421    
2022-12-29 10:45:51,448 - Epoch: [374][  110/  113]    Overall Loss 1.272419    Objective Loss 1.272419                                        LR 0.000006    Time 0.065152    
2022-12-29 10:45:51,621 - Epoch: [374][  113/  113]    Overall Loss 1.268905    Objective Loss 1.268905    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064955    
2022-12-29 10:45:51,669 - --- validate (epoch=374)-----------
2022-12-29 10:45:51,669 - 200 samples (16 per mini-batch)
2022-12-29 10:45:52,214 - Epoch: [374][   10/   13]    Loss 1.313052    Top1 48.125000    Top5 98.750000    
2022-12-29 10:45:52,301 - Epoch: [374][   13/   13]    Loss 1.300070    Top1 48.000000    Top5 99.000000    
2022-12-29 10:45:52,346 - ==> Top1: 48.000    Top5: 99.000    Loss: 1.300

2022-12-29 10:45:52,346 - ==> Confusion:
[[13  3  1  1  3  0]
 [ 2 27  3  9  3  0]
 [ 0 13 10  2 10  0]
 [ 6  8  3 35  2  0]
 [ 3 10  5 11 11  0]
 [ 0  3  0  1  2  0]]

2022-12-29 10:45:52,350 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:45:52,351 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:45:52,371 - 

2022-12-29 10:45:52,371 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:45:53,214 - Epoch: [375][   10/  113]    Overall Loss 1.283050    Objective Loss 1.283050                                        LR 0.000006    Time 0.084077    
2022-12-29 10:45:53,852 - Epoch: [375][   20/  113]    Overall Loss 1.279917    Objective Loss 1.279917                                        LR 0.000006    Time 0.073903    
2022-12-29 10:45:54,490 - Epoch: [375][   30/  113]    Overall Loss 1.271672    Objective Loss 1.271672                                        LR 0.000006    Time 0.070528    
2022-12-29 10:45:55,127 - Epoch: [375][   40/  113]    Overall Loss 1.259122    Objective Loss 1.259122                                        LR 0.000006    Time 0.068818    
2022-12-29 10:45:55,761 - Epoch: [375][   50/  113]    Overall Loss 1.272139    Objective Loss 1.272139                                        LR 0.000006    Time 0.067726    
2022-12-29 10:45:56,398 - Epoch: [375][   60/  113]    Overall Loss 1.272426    Objective Loss 1.272426                                        LR 0.000006    Time 0.067034    
2022-12-29 10:45:57,031 - Epoch: [375][   70/  113]    Overall Loss 1.263946    Objective Loss 1.263946                                        LR 0.000006    Time 0.066506    
2022-12-29 10:45:57,662 - Epoch: [375][   80/  113]    Overall Loss 1.268527    Objective Loss 1.268527                                        LR 0.000006    Time 0.066070    
2022-12-29 10:45:58,298 - Epoch: [375][   90/  113]    Overall Loss 1.265467    Objective Loss 1.265467                                        LR 0.000006    Time 0.065789    
2022-12-29 10:45:58,931 - Epoch: [375][  100/  113]    Overall Loss 1.274837    Objective Loss 1.274837                                        LR 0.000006    Time 0.065538    
2022-12-29 10:45:59,562 - Epoch: [375][  110/  113]    Overall Loss 1.286929    Objective Loss 1.286929                                        LR 0.000006    Time 0.065311    
2022-12-29 10:45:59,732 - Epoch: [375][  113/  113]    Overall Loss 1.287908    Objective Loss 1.287908    Top1 58.333333    Top5 95.833333    LR 0.000006    Time 0.065078    
2022-12-29 10:45:59,792 - --- validate (epoch=375)-----------
2022-12-29 10:45:59,793 - 200 samples (16 per mini-batch)
2022-12-29 10:46:00,355 - Epoch: [375][   10/   13]    Loss 1.180332    Top1 55.000000    Top5 98.750000    
2022-12-29 10:46:00,439 - Epoch: [375][   13/   13]    Loss 1.182043    Top1 52.500000    Top5 98.500000    
2022-12-29 10:46:00,488 - ==> Top1: 52.500    Top5: 98.500    Loss: 1.182

2022-12-29 10:46:00,488 - ==> Confusion:
[[20  7  1  6  0  0]
 [ 1 24  4  8  4  0]
 [ 1  9  6  5  4  0]
 [ 4  3  1 37  4  0]
 [ 2 12  4  7 16  0]
 [ 1  1  2  0  4  2]]

2022-12-29 10:46:00,492 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:00,492 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:00,504 - 

2022-12-29 10:46:00,504 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:01,344 - Epoch: [376][   10/  113]    Overall Loss 1.224670    Objective Loss 1.224670                                        LR 0.000006    Time 0.083856    
2022-12-29 10:46:01,982 - Epoch: [376][   20/  113]    Overall Loss 1.226054    Objective Loss 1.226054                                        LR 0.000006    Time 0.073797    
2022-12-29 10:46:02,621 - Epoch: [376][   30/  113]    Overall Loss 1.220009    Objective Loss 1.220009                                        LR 0.000006    Time 0.070498    
2022-12-29 10:46:03,257 - Epoch: [376][   40/  113]    Overall Loss 1.229424    Objective Loss 1.229424                                        LR 0.000006    Time 0.068766    
2022-12-29 10:46:03,893 - Epoch: [376][   50/  113]    Overall Loss 1.238279    Objective Loss 1.238279                                        LR 0.000006    Time 0.067722    
2022-12-29 10:46:04,523 - Epoch: [376][   60/  113]    Overall Loss 1.241813    Objective Loss 1.241813                                        LR 0.000006    Time 0.066921    
2022-12-29 10:46:05,154 - Epoch: [376][   70/  113]    Overall Loss 1.248242    Objective Loss 1.248242                                        LR 0.000006    Time 0.066369    
2022-12-29 10:46:05,787 - Epoch: [376][   80/  113]    Overall Loss 1.247616    Objective Loss 1.247616                                        LR 0.000006    Time 0.065984    
2022-12-29 10:46:06,422 - Epoch: [376][   90/  113]    Overall Loss 1.255563    Objective Loss 1.255563                                        LR 0.000006    Time 0.065700    
2022-12-29 10:46:07,054 - Epoch: [376][  100/  113]    Overall Loss 1.255286    Objective Loss 1.255286                                        LR 0.000006    Time 0.065444    
2022-12-29 10:46:07,678 - Epoch: [376][  110/  113]    Overall Loss 1.268004    Objective Loss 1.268004                                        LR 0.000006    Time 0.065166    
2022-12-29 10:46:07,851 - Epoch: [376][  113/  113]    Overall Loss 1.266773    Objective Loss 1.266773    Top1 58.333333    Top5 91.666667    LR 0.000006    Time 0.064963    
2022-12-29 10:46:07,900 - --- validate (epoch=376)-----------
2022-12-29 10:46:07,900 - 200 samples (16 per mini-batch)
2022-12-29 10:46:08,446 - Epoch: [376][   10/   13]    Loss 1.307289    Top1 46.250000    Top5 96.250000    
2022-12-29 10:46:08,529 - Epoch: [376][   13/   13]    Loss 1.283981    Top1 47.500000    Top5 95.000000    
2022-12-29 10:46:08,573 - ==> Top1: 47.500    Top5: 95.000    Loss: 1.284

2022-12-29 10:46:08,574 - ==> Confusion:
[[28  4  0  3  1  0]
 [ 1 14  3  1  5  0]
 [ 4  7 12  4  5  0]
 [ 5  5  2 31  6  0]
 [ 4  7  9 10 10  0]
 [ 1  9  2  3  4  0]]

2022-12-29 10:46:08,576 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:08,577 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:08,594 - 

2022-12-29 10:46:08,594 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:09,429 - Epoch: [377][   10/  113]    Overall Loss 1.250550    Objective Loss 1.250550                                        LR 0.000006    Time 0.083389    
2022-12-29 10:46:10,061 - Epoch: [377][   20/  113]    Overall Loss 1.241213    Objective Loss 1.241213                                        LR 0.000006    Time 0.073279    
2022-12-29 10:46:10,692 - Epoch: [377][   30/  113]    Overall Loss 1.271405    Objective Loss 1.271405                                        LR 0.000006    Time 0.069866    
2022-12-29 10:46:11,321 - Epoch: [377][   40/  113]    Overall Loss 1.259161    Objective Loss 1.259161                                        LR 0.000006    Time 0.068105    
2022-12-29 10:46:11,947 - Epoch: [377][   50/  113]    Overall Loss 1.258355    Objective Loss 1.258355                                        LR 0.000006    Time 0.066990    
2022-12-29 10:46:12,577 - Epoch: [377][   60/  113]    Overall Loss 1.252070    Objective Loss 1.252070                                        LR 0.000006    Time 0.066318    
2022-12-29 10:46:13,206 - Epoch: [377][   70/  113]    Overall Loss 1.263584    Objective Loss 1.263584                                        LR 0.000006    Time 0.065827    
2022-12-29 10:46:13,837 - Epoch: [377][   80/  113]    Overall Loss 1.271825    Objective Loss 1.271825                                        LR 0.000006    Time 0.065477    
2022-12-29 10:46:14,465 - Epoch: [377][   90/  113]    Overall Loss 1.269196    Objective Loss 1.269196                                        LR 0.000006    Time 0.065174    
2022-12-29 10:46:15,095 - Epoch: [377][  100/  113]    Overall Loss 1.263231    Objective Loss 1.263231                                        LR 0.000006    Time 0.064959    
2022-12-29 10:46:15,724 - Epoch: [377][  110/  113]    Overall Loss 1.256133    Objective Loss 1.256133                                        LR 0.000006    Time 0.064759    
2022-12-29 10:46:15,895 - Epoch: [377][  113/  113]    Overall Loss 1.254592    Objective Loss 1.254592    Top1 45.833333    Top5 91.666667    LR 0.000006    Time 0.064558    
2022-12-29 10:46:15,954 - --- validate (epoch=377)-----------
2022-12-29 10:46:15,954 - 200 samples (16 per mini-batch)
2022-12-29 10:46:16,515 - Epoch: [377][   10/   13]    Loss 1.329449    Top1 46.250000    Top5 98.125000    
2022-12-29 10:46:16,600 - Epoch: [377][   13/   13]    Loss 1.305650    Top1 46.000000    Top5 98.000000    
2022-12-29 10:46:16,650 - ==> Top1: 46.000    Top5: 98.000    Loss: 1.306

2022-12-29 10:46:16,650 - ==> Confusion:
[[24  0  1  3  0  0]
 [ 6 18  2 10  5  1]
 [ 1  8  6  6 12  0]
 [ 6  5  1 33  7  0]
 [ 0  8 10  8  9  0]
 [ 2  1  2  1  2  2]]

2022-12-29 10:46:16,653 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:16,654 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:16,675 - 

2022-12-29 10:46:16,675 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:17,505 - Epoch: [378][   10/  113]    Overall Loss 1.226156    Objective Loss 1.226156                                        LR 0.000006    Time 0.082725    
2022-12-29 10:46:18,142 - Epoch: [378][   20/  113]    Overall Loss 1.202261    Objective Loss 1.202261                                        LR 0.000006    Time 0.073228    
2022-12-29 10:46:18,779 - Epoch: [378][   30/  113]    Overall Loss 1.210622    Objective Loss 1.210622                                        LR 0.000006    Time 0.070027    
2022-12-29 10:46:19,408 - Epoch: [378][   40/  113]    Overall Loss 1.233222    Objective Loss 1.233222                                        LR 0.000006    Time 0.068234    
2022-12-29 10:46:20,033 - Epoch: [378][   50/  113]    Overall Loss 1.240870    Objective Loss 1.240870                                        LR 0.000006    Time 0.067084    
2022-12-29 10:46:20,666 - Epoch: [378][   60/  113]    Overall Loss 1.246508    Objective Loss 1.246508                                        LR 0.000006    Time 0.066433    
2022-12-29 10:46:21,296 - Epoch: [378][   70/  113]    Overall Loss 1.238349    Objective Loss 1.238349                                        LR 0.000006    Time 0.065947    
2022-12-29 10:46:21,931 - Epoch: [378][   80/  113]    Overall Loss 1.242906    Objective Loss 1.242906                                        LR 0.000006    Time 0.065629    
2022-12-29 10:46:22,565 - Epoch: [378][   90/  113]    Overall Loss 1.253465    Objective Loss 1.253465                                        LR 0.000006    Time 0.065377    
2022-12-29 10:46:23,193 - Epoch: [378][  100/  113]    Overall Loss 1.253230    Objective Loss 1.253230                                        LR 0.000006    Time 0.065114    
2022-12-29 10:46:23,827 - Epoch: [378][  110/  113]    Overall Loss 1.263277    Objective Loss 1.263277                                        LR 0.000006    Time 0.064950    
2022-12-29 10:46:23,997 - Epoch: [378][  113/  113]    Overall Loss 1.265341    Objective Loss 1.265341    Top1 50.000000    Top5 95.833333    LR 0.000006    Time 0.064729    
2022-12-29 10:46:24,054 - --- validate (epoch=378)-----------
2022-12-29 10:46:24,054 - 200 samples (16 per mini-batch)
2022-12-29 10:46:24,600 - Epoch: [378][   10/   13]    Loss 1.263606    Top1 48.750000    Top5 96.250000    
2022-12-29 10:46:24,686 - Epoch: [378][   13/   13]    Loss 1.253209    Top1 48.000000    Top5 97.000000    
2022-12-29 10:46:24,735 - ==> Top1: 48.000    Top5: 97.000    Loss: 1.253

2022-12-29 10:46:24,736 - ==> Confusion:
[[26  1  1  4  3  0]
 [ 0 18  7  3  6  0]
 [ 4 14  9  2  8  0]
 [ 7  3  2 25  4  1]
 [ 3 14  6  4 17  0]
 [ 1  3  1  1  1  1]]

2022-12-29 10:46:24,739 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:24,739 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:24,759 - 

2022-12-29 10:46:24,759 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:25,600 - Epoch: [379][   10/  113]    Overall Loss 1.291606    Objective Loss 1.291606                                        LR 0.000006    Time 0.083913    
2022-12-29 10:46:26,237 - Epoch: [379][   20/  113]    Overall Loss 1.298309    Objective Loss 1.298309                                        LR 0.000006    Time 0.073757    
2022-12-29 10:46:26,875 - Epoch: [379][   30/  113]    Overall Loss 1.263806    Objective Loss 1.263806                                        LR 0.000006    Time 0.070423    
2022-12-29 10:46:27,509 - Epoch: [379][   40/  113]    Overall Loss 1.257972    Objective Loss 1.257972                                        LR 0.000006    Time 0.068671    
2022-12-29 10:46:28,139 - Epoch: [379][   50/  113]    Overall Loss 1.260195    Objective Loss 1.260195                                        LR 0.000006    Time 0.067528    
2022-12-29 10:46:28,774 - Epoch: [379][   60/  113]    Overall Loss 1.271428    Objective Loss 1.271428                                        LR 0.000006    Time 0.066840    
2022-12-29 10:46:29,410 - Epoch: [379][   70/  113]    Overall Loss 1.285703    Objective Loss 1.285703                                        LR 0.000006    Time 0.066367    
2022-12-29 10:46:30,042 - Epoch: [379][   80/  113]    Overall Loss 1.278932    Objective Loss 1.278932                                        LR 0.000006    Time 0.065971    
2022-12-29 10:46:30,680 - Epoch: [379][   90/  113]    Overall Loss 1.283702    Objective Loss 1.283702                                        LR 0.000006    Time 0.065725    
2022-12-29 10:46:31,309 - Epoch: [379][  100/  113]    Overall Loss 1.274958    Objective Loss 1.274958                                        LR 0.000006    Time 0.065435    
2022-12-29 10:46:31,942 - Epoch: [379][  110/  113]    Overall Loss 1.290608    Objective Loss 1.290608                                        LR 0.000006    Time 0.065238    
2022-12-29 10:46:32,114 - Epoch: [379][  113/  113]    Overall Loss 1.288511    Objective Loss 1.288511    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.065022    
2022-12-29 10:46:32,158 - --- validate (epoch=379)-----------
2022-12-29 10:46:32,158 - 200 samples (16 per mini-batch)
2022-12-29 10:46:32,704 - Epoch: [379][   10/   13]    Loss 1.230388    Top1 55.000000    Top5 96.875000    
2022-12-29 10:46:32,789 - Epoch: [379][   13/   13]    Loss 1.235135    Top1 54.500000    Top5 97.500000    
2022-12-29 10:46:32,835 - ==> Top1: 54.500    Top5: 97.500    Loss: 1.235

2022-12-29 10:46:32,835 - ==> Confusion:
[[23  3  0  5  0  1]
 [ 3 27  2  3  7  0]
 [ 1  6  8  3  6  0]
 [ 3  2  5 35  5  0]
 [ 2  9  6  9 15  0]
 [ 0  3  0  3  4  1]]

2022-12-29 10:46:32,838 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:32,838 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:32,856 - 

2022-12-29 10:46:32,856 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:33,685 - Epoch: [380][   10/  113]    Overall Loss 1.305005    Objective Loss 1.305005                                        LR 0.000006    Time 0.082813    
2022-12-29 10:46:34,320 - Epoch: [380][   20/  113]    Overall Loss 1.299122    Objective Loss 1.299122                                        LR 0.000006    Time 0.073126    
2022-12-29 10:46:34,959 - Epoch: [380][   30/  113]    Overall Loss 1.271487    Objective Loss 1.271487                                        LR 0.000006    Time 0.070042    
2022-12-29 10:46:35,593 - Epoch: [380][   40/  113]    Overall Loss 1.244416    Objective Loss 1.244416                                        LR 0.000006    Time 0.068371    
2022-12-29 10:46:36,233 - Epoch: [380][   50/  113]    Overall Loss 1.252542    Objective Loss 1.252542                                        LR 0.000006    Time 0.067484    
2022-12-29 10:46:36,866 - Epoch: [380][   60/  113]    Overall Loss 1.272396    Objective Loss 1.272396                                        LR 0.000006    Time 0.066782    
2022-12-29 10:46:37,499 - Epoch: [380][   70/  113]    Overall Loss 1.271568    Objective Loss 1.271568                                        LR 0.000006    Time 0.066270    
2022-12-29 10:46:38,127 - Epoch: [380][   80/  113]    Overall Loss 1.262111    Objective Loss 1.262111                                        LR 0.000006    Time 0.065833    
2022-12-29 10:46:38,767 - Epoch: [380][   90/  113]    Overall Loss 1.265186    Objective Loss 1.265186                                        LR 0.000006    Time 0.065621    
2022-12-29 10:46:39,401 - Epoch: [380][  100/  113]    Overall Loss 1.267645    Objective Loss 1.267645                                        LR 0.000006    Time 0.065401    
2022-12-29 10:46:40,030 - Epoch: [380][  110/  113]    Overall Loss 1.277877    Objective Loss 1.277877                                        LR 0.000006    Time 0.065168    
2022-12-29 10:46:40,203 - Epoch: [380][  113/  113]    Overall Loss 1.277209    Objective Loss 1.277209    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064967    
2022-12-29 10:46:40,256 - --- validate (epoch=380)-----------
2022-12-29 10:46:40,256 - 200 samples (16 per mini-batch)
2022-12-29 10:46:40,806 - Epoch: [380][   10/   13]    Loss 1.278274    Top1 53.750000    Top5 96.250000    
2022-12-29 10:46:40,892 - Epoch: [380][   13/   13]    Loss 1.259961    Top1 53.000000    Top5 97.000000    
2022-12-29 10:46:40,950 - ==> Top1: 53.000    Top5: 97.000    Loss: 1.260

2022-12-29 10:46:40,951 - ==> Confusion:
[[20  7  0  7  3  1]
 [ 1 24  5  2  6  0]
 [ 0  9 13 11  7  0]
 [ 2  4  1 34  2  0]
 [ 1  5  3  4 15  0]
 [ 1  3  1  3  5  0]]

2022-12-29 10:46:40,954 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:40,954 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:40,974 - 

2022-12-29 10:46:40,974 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:41,793 - Epoch: [381][   10/  113]    Overall Loss 1.322454    Objective Loss 1.322454                                        LR 0.000006    Time 0.081775    
2022-12-29 10:46:42,418 - Epoch: [381][   20/  113]    Overall Loss 1.315446    Objective Loss 1.315446                                        LR 0.000006    Time 0.072143    
2022-12-29 10:46:43,048 - Epoch: [381][   30/  113]    Overall Loss 1.320925    Objective Loss 1.320925                                        LR 0.000006    Time 0.069058    
2022-12-29 10:46:43,681 - Epoch: [381][   40/  113]    Overall Loss 1.305455    Objective Loss 1.305455                                        LR 0.000006    Time 0.067598    
2022-12-29 10:46:44,314 - Epoch: [381][   50/  113]    Overall Loss 1.294517    Objective Loss 1.294517                                        LR 0.000006    Time 0.066736    
2022-12-29 10:46:44,949 - Epoch: [381][   60/  113]    Overall Loss 1.295257    Objective Loss 1.295257                                        LR 0.000006    Time 0.066187    
2022-12-29 10:46:45,581 - Epoch: [381][   70/  113]    Overall Loss 1.303323    Objective Loss 1.303323                                        LR 0.000006    Time 0.065760    
2022-12-29 10:46:46,217 - Epoch: [381][   80/  113]    Overall Loss 1.296095    Objective Loss 1.296095                                        LR 0.000006    Time 0.065474    
2022-12-29 10:46:46,853 - Epoch: [381][   90/  113]    Overall Loss 1.280481    Objective Loss 1.280481                                        LR 0.000006    Time 0.065261    
2022-12-29 10:46:47,485 - Epoch: [381][  100/  113]    Overall Loss 1.273687    Objective Loss 1.273687                                        LR 0.000006    Time 0.065054    
2022-12-29 10:46:48,115 - Epoch: [381][  110/  113]    Overall Loss 1.275051    Objective Loss 1.275051                                        LR 0.000006    Time 0.064857    
2022-12-29 10:46:48,287 - Epoch: [381][  113/  113]    Overall Loss 1.278872    Objective Loss 1.278872    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064658    
2022-12-29 10:46:48,348 - --- validate (epoch=381)-----------
2022-12-29 10:46:48,348 - 200 samples (16 per mini-batch)
2022-12-29 10:46:48,911 - Epoch: [381][   10/   13]    Loss 1.299027    Top1 48.750000    Top5 97.500000    
2022-12-29 10:46:48,997 - Epoch: [381][   13/   13]    Loss 1.253797    Top1 51.000000    Top5 98.000000    
2022-12-29 10:46:49,058 - ==> Top1: 51.000    Top5: 98.000    Loss: 1.254

2022-12-29 10:46:49,059 - ==> Confusion:
[[17  4  0  8  2  1]
 [ 3 16  6  6  4  0]
 [ 1  5  8  3  4  0]
 [ 2  8  4 49  4  2]
 [ 2  4  3 10 11  1]
 [ 1  5  1  2  2  1]]

2022-12-29 10:46:49,065 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:49,065 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:49,076 - 

2022-12-29 10:46:49,076 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:49,919 - Epoch: [382][   10/  113]    Overall Loss 1.339497    Objective Loss 1.339497                                        LR 0.000006    Time 0.084199    
2022-12-29 10:46:50,555 - Epoch: [382][   20/  113]    Overall Loss 1.335165    Objective Loss 1.335165                                        LR 0.000006    Time 0.073884    
2022-12-29 10:46:51,194 - Epoch: [382][   30/  113]    Overall Loss 1.311454    Objective Loss 1.311454                                        LR 0.000006    Time 0.070527    
2022-12-29 10:46:51,830 - Epoch: [382][   40/  113]    Overall Loss 1.304646    Objective Loss 1.304646                                        LR 0.000006    Time 0.068780    
2022-12-29 10:46:52,462 - Epoch: [382][   50/  113]    Overall Loss 1.283375    Objective Loss 1.283375                                        LR 0.000006    Time 0.067655    
2022-12-29 10:46:53,100 - Epoch: [382][   60/  113]    Overall Loss 1.282012    Objective Loss 1.282012                                        LR 0.000006    Time 0.067001    
2022-12-29 10:46:53,730 - Epoch: [382][   70/  113]    Overall Loss 1.284490    Objective Loss 1.284490                                        LR 0.000006    Time 0.066425    
2022-12-29 10:46:54,357 - Epoch: [382][   80/  113]    Overall Loss 1.273916    Objective Loss 1.273916                                        LR 0.000006    Time 0.065958    
2022-12-29 10:46:54,988 - Epoch: [382][   90/  113]    Overall Loss 1.271912    Objective Loss 1.271912                                        LR 0.000006    Time 0.065634    
2022-12-29 10:46:55,613 - Epoch: [382][  100/  113]    Overall Loss 1.283716    Objective Loss 1.283716                                        LR 0.000006    Time 0.065317    
2022-12-29 10:46:56,235 - Epoch: [382][  110/  113]    Overall Loss 1.285373    Objective Loss 1.285373                                        LR 0.000006    Time 0.065022    
2022-12-29 10:46:56,408 - Epoch: [382][  113/  113]    Overall Loss 1.286381    Objective Loss 1.286381    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064826    
2022-12-29 10:46:56,469 - --- validate (epoch=382)-----------
2022-12-29 10:46:56,470 - 200 samples (16 per mini-batch)
2022-12-29 10:46:57,025 - Epoch: [382][   10/   13]    Loss 1.332256    Top1 47.500000    Top5 96.875000    
2022-12-29 10:46:57,113 - Epoch: [382][   13/   13]    Loss 1.360438    Top1 46.500000    Top5 96.500000    
2022-12-29 10:46:57,167 - ==> Top1: 46.500    Top5: 96.500    Loss: 1.360

2022-12-29 10:46:57,168 - ==> Confusion:
[[14  7  2  2  2  0]
 [ 2 29  6  3  7  0]
 [ 1  8 14  3  7  0]
 [ 6  5  3 23  3  1]
 [ 5 12 11  1 11  0]
 [ 1  4  0  4  1  2]]

2022-12-29 10:46:57,171 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:46:57,171 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:46:57,193 - 

2022-12-29 10:46:57,194 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:46:58,036 - Epoch: [383][   10/  113]    Overall Loss 1.260688    Objective Loss 1.260688                                        LR 0.000006    Time 0.084075    
2022-12-29 10:46:58,670 - Epoch: [383][   20/  113]    Overall Loss 1.252980    Objective Loss 1.252980                                        LR 0.000006    Time 0.073745    
2022-12-29 10:46:59,310 - Epoch: [383][   30/  113]    Overall Loss 1.252376    Objective Loss 1.252376                                        LR 0.000006    Time 0.070473    
2022-12-29 10:46:59,941 - Epoch: [383][   40/  113]    Overall Loss 1.274758    Objective Loss 1.274758                                        LR 0.000006    Time 0.068620    
2022-12-29 10:47:00,572 - Epoch: [383][   50/  113]    Overall Loss 1.274123    Objective Loss 1.274123                                        LR 0.000006    Time 0.067500    
2022-12-29 10:47:01,200 - Epoch: [383][   60/  113]    Overall Loss 1.255007    Objective Loss 1.255007                                        LR 0.000006    Time 0.066719    
2022-12-29 10:47:01,829 - Epoch: [383][   70/  113]    Overall Loss 1.245579    Objective Loss 1.245579                                        LR 0.000006    Time 0.066163    
2022-12-29 10:47:02,462 - Epoch: [383][   80/  113]    Overall Loss 1.250201    Objective Loss 1.250201                                        LR 0.000006    Time 0.065793    
2022-12-29 10:47:03,097 - Epoch: [383][   90/  113]    Overall Loss 1.246368    Objective Loss 1.246368                                        LR 0.000006    Time 0.065535    
2022-12-29 10:47:03,727 - Epoch: [383][  100/  113]    Overall Loss 1.243643    Objective Loss 1.243643                                        LR 0.000006    Time 0.065283    
2022-12-29 10:47:04,358 - Epoch: [383][  110/  113]    Overall Loss 1.251344    Objective Loss 1.251344                                        LR 0.000006    Time 0.065076    
2022-12-29 10:47:04,531 - Epoch: [383][  113/  113]    Overall Loss 1.251184    Objective Loss 1.251184    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064876    
2022-12-29 10:47:04,589 - --- validate (epoch=383)-----------
2022-12-29 10:47:04,590 - 200 samples (16 per mini-batch)
2022-12-29 10:47:05,149 - Epoch: [383][   10/   13]    Loss 1.384253    Top1 49.375000    Top5 97.500000    
2022-12-29 10:47:05,235 - Epoch: [383][   13/   13]    Loss 1.372004    Top1 51.000000    Top5 97.500000    
2022-12-29 10:47:05,282 - ==> Top1: 51.000    Top5: 97.500    Loss: 1.372

2022-12-29 10:47:05,282 - ==> Confusion:
[[17  0  2  8  0  0]
 [ 1 23  2 15  6  0]
 [ 1  4 13  8  4  0]
 [ 1  1  4 31  4  0]
 [ 5  5  5 13 16  0]
 [ 2  4  0  2  1  2]]

2022-12-29 10:47:05,286 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:05,286 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:05,308 - 

2022-12-29 10:47:05,309 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:06,155 - Epoch: [384][   10/  113]    Overall Loss 1.317964    Objective Loss 1.317964                                        LR 0.000006    Time 0.084492    
2022-12-29 10:47:06,793 - Epoch: [384][   20/  113]    Overall Loss 1.333130    Objective Loss 1.333130                                        LR 0.000006    Time 0.074115    
2022-12-29 10:47:07,432 - Epoch: [384][   30/  113]    Overall Loss 1.326022    Objective Loss 1.326022                                        LR 0.000006    Time 0.070694    
2022-12-29 10:47:08,068 - Epoch: [384][   40/  113]    Overall Loss 1.300849    Objective Loss 1.300849                                        LR 0.000006    Time 0.068915    
2022-12-29 10:47:08,699 - Epoch: [384][   50/  113]    Overall Loss 1.305856    Objective Loss 1.305856                                        LR 0.000006    Time 0.067740    
2022-12-29 10:47:09,334 - Epoch: [384][   60/  113]    Overall Loss 1.299210    Objective Loss 1.299210                                        LR 0.000006    Time 0.067022    
2022-12-29 10:47:09,967 - Epoch: [384][   70/  113]    Overall Loss 1.314740    Objective Loss 1.314740                                        LR 0.000006    Time 0.066495    
2022-12-29 10:47:10,599 - Epoch: [384][   80/  113]    Overall Loss 1.303863    Objective Loss 1.303863                                        LR 0.000006    Time 0.066078    
2022-12-29 10:47:11,231 - Epoch: [384][   90/  113]    Overall Loss 1.298921    Objective Loss 1.298921                                        LR 0.000006    Time 0.065745    
2022-12-29 10:47:11,861 - Epoch: [384][  100/  113]    Overall Loss 1.299628    Objective Loss 1.299628                                        LR 0.000006    Time 0.065464    
2022-12-29 10:47:12,484 - Epoch: [384][  110/  113]    Overall Loss 1.294461    Objective Loss 1.294461                                        LR 0.000006    Time 0.065177    
2022-12-29 10:47:12,657 - Epoch: [384][  113/  113]    Overall Loss 1.297424    Objective Loss 1.297424    Top1 37.500000    Top5 100.000000    LR 0.000006    Time 0.064973    
2022-12-29 10:47:12,722 - --- validate (epoch=384)-----------
2022-12-29 10:47:12,722 - 200 samples (16 per mini-batch)
2022-12-29 10:47:13,269 - Epoch: [384][   10/   13]    Loss 1.247443    Top1 50.625000    Top5 95.625000    
2022-12-29 10:47:13,363 - Epoch: [384][   13/   13]    Loss 1.274234    Top1 51.000000    Top5 96.000000    
2022-12-29 10:47:13,423 - ==> Top1: 51.000    Top5: 96.000    Loss: 1.274

2022-12-29 10:47:13,423 - ==> Confusion:
[[18  7  0  4  2  1]
 [ 2 17  4  3  6  0]
 [ 0 11 12  4  7  0]
 [ 2  2  1 41  3  0]
 [ 0 15  3 12 13  0]
 [ 0  3  0  4  2  1]]

2022-12-29 10:47:13,427 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:13,427 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:13,453 - 

2022-12-29 10:47:13,454 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:14,279 - Epoch: [385][   10/  113]    Overall Loss 1.261452    Objective Loss 1.261452                                        LR 0.000006    Time 0.082443    
2022-12-29 10:47:14,918 - Epoch: [385][   20/  113]    Overall Loss 1.272291    Objective Loss 1.272291                                        LR 0.000006    Time 0.073129    
2022-12-29 10:47:15,554 - Epoch: [385][   30/  113]    Overall Loss 1.270451    Objective Loss 1.270451                                        LR 0.000006    Time 0.069959    
2022-12-29 10:47:16,190 - Epoch: [385][   40/  113]    Overall Loss 1.247104    Objective Loss 1.247104                                        LR 0.000006    Time 0.068358    
2022-12-29 10:47:16,821 - Epoch: [385][   50/  113]    Overall Loss 1.237355    Objective Loss 1.237355                                        LR 0.000006    Time 0.067278    
2022-12-29 10:47:17,457 - Epoch: [385][   60/  113]    Overall Loss 1.243985    Objective Loss 1.243985                                        LR 0.000006    Time 0.066673    
2022-12-29 10:47:18,094 - Epoch: [385][   70/  113]    Overall Loss 1.249154    Objective Loss 1.249154                                        LR 0.000006    Time 0.066238    
2022-12-29 10:47:18,727 - Epoch: [385][   80/  113]    Overall Loss 1.236076    Objective Loss 1.236076                                        LR 0.000006    Time 0.065869    
2022-12-29 10:47:19,359 - Epoch: [385][   90/  113]    Overall Loss 1.236419    Objective Loss 1.236419                                        LR 0.000006    Time 0.065558    
2022-12-29 10:47:19,992 - Epoch: [385][  100/  113]    Overall Loss 1.230570    Objective Loss 1.230570                                        LR 0.000006    Time 0.065329    
2022-12-29 10:47:20,622 - Epoch: [385][  110/  113]    Overall Loss 1.224890    Objective Loss 1.224890                                        LR 0.000006    Time 0.065114    
2022-12-29 10:47:20,794 - Epoch: [385][  113/  113]    Overall Loss 1.228420    Objective Loss 1.228420    Top1 50.000000    Top5 91.666667    LR 0.000006    Time 0.064903    
2022-12-29 10:47:20,848 - --- validate (epoch=385)-----------
2022-12-29 10:47:20,848 - 200 samples (16 per mini-batch)
2022-12-29 10:47:21,402 - Epoch: [385][   10/   13]    Loss 1.427589    Top1 43.750000    Top5 95.000000    
2022-12-29 10:47:21,488 - Epoch: [385][   13/   13]    Loss 1.428872    Top1 43.000000    Top5 94.000000    
2022-12-29 10:47:21,546 - ==> Top1: 43.000    Top5: 94.000    Loss: 1.429

2022-12-29 10:47:21,546 - ==> Confusion:
[[13  5  1  6  2  0]
 [ 3 22  2  5  5  0]
 [ 3 13  9  5  9  0]
 [ 6 10  2 29  1  0]
 [ 1  8  3  8 13  0]
 [ 3  7  1  3  2  0]]

2022-12-29 10:47:21,549 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:21,550 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:21,572 - 

2022-12-29 10:47:21,573 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:22,399 - Epoch: [386][   10/  113]    Overall Loss 1.209821    Objective Loss 1.209821                                        LR 0.000006    Time 0.082531    
2022-12-29 10:47:23,043 - Epoch: [386][   20/  113]    Overall Loss 1.265660    Objective Loss 1.265660                                        LR 0.000006    Time 0.073422    
2022-12-29 10:47:23,683 - Epoch: [386][   30/  113]    Overall Loss 1.265856    Objective Loss 1.265856                                        LR 0.000006    Time 0.070285    
2022-12-29 10:47:24,321 - Epoch: [386][   40/  113]    Overall Loss 1.276928    Objective Loss 1.276928                                        LR 0.000006    Time 0.068641    
2022-12-29 10:47:24,956 - Epoch: [386][   50/  113]    Overall Loss 1.280842    Objective Loss 1.280842                                        LR 0.000006    Time 0.067614    
2022-12-29 10:47:25,586 - Epoch: [386][   60/  113]    Overall Loss 1.300337    Objective Loss 1.300337                                        LR 0.000006    Time 0.066831    
2022-12-29 10:47:26,221 - Epoch: [386][   70/  113]    Overall Loss 1.286335    Objective Loss 1.286335                                        LR 0.000006    Time 0.066342    
2022-12-29 10:47:26,854 - Epoch: [386][   80/  113]    Overall Loss 1.273851    Objective Loss 1.273851                                        LR 0.000006    Time 0.065965    
2022-12-29 10:47:27,489 - Epoch: [386][   90/  113]    Overall Loss 1.283538    Objective Loss 1.283538                                        LR 0.000006    Time 0.065687    
2022-12-29 10:47:28,118 - Epoch: [386][  100/  113]    Overall Loss 1.286451    Objective Loss 1.286451                                        LR 0.000006    Time 0.065395    
2022-12-29 10:47:28,741 - Epoch: [386][  110/  113]    Overall Loss 1.282783    Objective Loss 1.282783                                        LR 0.000006    Time 0.065112    
2022-12-29 10:47:28,916 - Epoch: [386][  113/  113]    Overall Loss 1.284356    Objective Loss 1.284356    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064934    
2022-12-29 10:47:28,978 - --- validate (epoch=386)-----------
2022-12-29 10:47:28,978 - 200 samples (16 per mini-batch)
2022-12-29 10:47:29,523 - Epoch: [386][   10/   13]    Loss 1.245763    Top1 51.250000    Top5 99.375000    
2022-12-29 10:47:29,607 - Epoch: [386][   13/   13]    Loss 1.222948    Top1 51.000000    Top5 99.000000    
2022-12-29 10:47:29,663 - ==> Top1: 51.000    Top5: 99.000    Loss: 1.223

2022-12-29 10:47:29,663 - ==> Confusion:
[[24  4  0  6  2  1]
 [ 1 22  3  8  7  0]
 [ 2  8  9  4  6  0]
 [ 1  8  1 33  4  0]
 [ 1 11  6  7 14  0]
 [ 0  4  0  2  1  0]]

2022-12-29 10:47:29,668 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:29,668 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:29,689 - 

2022-12-29 10:47:29,690 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:30,523 - Epoch: [387][   10/  113]    Overall Loss 1.249648    Objective Loss 1.249648                                        LR 0.000006    Time 0.083149    
2022-12-29 10:47:31,159 - Epoch: [387][   20/  113]    Overall Loss 1.215315    Objective Loss 1.215315                                        LR 0.000006    Time 0.073365    
2022-12-29 10:47:31,791 - Epoch: [387][   30/  113]    Overall Loss 1.198165    Objective Loss 1.198165                                        LR 0.000006    Time 0.069951    
2022-12-29 10:47:32,425 - Epoch: [387][   40/  113]    Overall Loss 1.203055    Objective Loss 1.203055                                        LR 0.000006    Time 0.068303    
2022-12-29 10:47:33,058 - Epoch: [387][   50/  113]    Overall Loss 1.195795    Objective Loss 1.195795                                        LR 0.000006    Time 0.067288    
2022-12-29 10:47:33,693 - Epoch: [387][   60/  113]    Overall Loss 1.211128    Objective Loss 1.211128                                        LR 0.000006    Time 0.066659    
2022-12-29 10:47:34,328 - Epoch: [387][   70/  113]    Overall Loss 1.229687    Objective Loss 1.229687                                        LR 0.000006    Time 0.066189    
2022-12-29 10:47:34,962 - Epoch: [387][   80/  113]    Overall Loss 1.220098    Objective Loss 1.220098                                        LR 0.000006    Time 0.065846    
2022-12-29 10:47:35,598 - Epoch: [387][   90/  113]    Overall Loss 1.238119    Objective Loss 1.238119                                        LR 0.000006    Time 0.065591    
2022-12-29 10:47:36,232 - Epoch: [387][  100/  113]    Overall Loss 1.251338    Objective Loss 1.251338                                        LR 0.000006    Time 0.065361    
2022-12-29 10:47:36,859 - Epoch: [387][  110/  113]    Overall Loss 1.259829    Objective Loss 1.259829                                        LR 0.000006    Time 0.065116    
2022-12-29 10:47:37,032 - Epoch: [387][  113/  113]    Overall Loss 1.262636    Objective Loss 1.262636    Top1 54.166667    Top5 91.666667    LR 0.000006    Time 0.064912    
2022-12-29 10:47:37,085 - --- validate (epoch=387)-----------
2022-12-29 10:47:37,086 - 200 samples (16 per mini-batch)
2022-12-29 10:47:37,631 - Epoch: [387][   10/   13]    Loss 1.220377    Top1 53.125000    Top5 98.750000    
2022-12-29 10:47:37,716 - Epoch: [387][   13/   13]    Loss 1.258022    Top1 48.500000    Top5 98.000000    
2022-12-29 10:47:37,777 - ==> Top1: 48.500    Top5: 98.000    Loss: 1.258

2022-12-29 10:47:37,777 - ==> Confusion:
[[20  3  1  5  0  0]
 [ 6 29  3  5  8  1]
 [ 2  8 10  5  6  0]
 [ 4  7  3 24  4  0]
 [ 1 11  3  6 13  0]
 [ 0  3  3  0  5  1]]

2022-12-29 10:47:37,781 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:37,781 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:37,795 - 

2022-12-29 10:47:37,796 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:38,616 - Epoch: [388][   10/  113]    Overall Loss 1.345938    Objective Loss 1.345938                                        LR 0.000006    Time 0.081975    
2022-12-29 10:47:39,251 - Epoch: [388][   20/  113]    Overall Loss 1.286396    Objective Loss 1.286396                                        LR 0.000006    Time 0.072718    
2022-12-29 10:47:39,882 - Epoch: [388][   30/  113]    Overall Loss 1.258196    Objective Loss 1.258196                                        LR 0.000006    Time 0.069484    
2022-12-29 10:47:40,515 - Epoch: [388][   40/  113]    Overall Loss 1.269052    Objective Loss 1.269052                                        LR 0.000006    Time 0.067914    
2022-12-29 10:47:41,148 - Epoch: [388][   50/  113]    Overall Loss 1.270648    Objective Loss 1.270648                                        LR 0.000006    Time 0.066986    
2022-12-29 10:47:41,786 - Epoch: [388][   60/  113]    Overall Loss 1.279323    Objective Loss 1.279323                                        LR 0.000006    Time 0.066443    
2022-12-29 10:47:42,419 - Epoch: [388][   70/  113]    Overall Loss 1.268040    Objective Loss 1.268040                                        LR 0.000006    Time 0.065992    
2022-12-29 10:47:43,054 - Epoch: [388][   80/  113]    Overall Loss 1.268718    Objective Loss 1.268718                                        LR 0.000006    Time 0.065676    
2022-12-29 10:47:43,692 - Epoch: [388][   90/  113]    Overall Loss 1.263687    Objective Loss 1.263687                                        LR 0.000006    Time 0.065462    
2022-12-29 10:47:44,323 - Epoch: [388][  100/  113]    Overall Loss 1.263734    Objective Loss 1.263734                                        LR 0.000006    Time 0.065224    
2022-12-29 10:47:44,950 - Epoch: [388][  110/  113]    Overall Loss 1.262165    Objective Loss 1.262165                                        LR 0.000006    Time 0.064992    
2022-12-29 10:47:45,131 - Epoch: [388][  113/  113]    Overall Loss 1.262398    Objective Loss 1.262398    Top1 50.000000    Top5 91.666667    LR 0.000006    Time 0.064865    
2022-12-29 10:47:45,197 - --- validate (epoch=388)-----------
2022-12-29 10:47:45,197 - 200 samples (16 per mini-batch)
2022-12-29 10:47:45,751 - Epoch: [388][   10/   13]    Loss 1.237974    Top1 53.750000    Top5 95.625000    
2022-12-29 10:47:45,836 - Epoch: [388][   13/   13]    Loss 1.232273    Top1 53.500000    Top5 96.000000    
2022-12-29 10:47:45,900 - ==> Top1: 53.500    Top5: 96.000    Loss: 1.232

2022-12-29 10:47:45,900 - ==> Confusion:
[[23  5  3  3  2  0]
 [ 2 26  4  4  3  0]
 [ 0  7 14  3 10  0]
 [ 5  7  3 30  2  0]
 [ 1 11  4  5 13  0]
 [ 0  3  1  3  2  1]]

2022-12-29 10:47:45,902 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:45,902 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:45,914 - 

2022-12-29 10:47:45,914 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:46,754 - Epoch: [389][   10/  113]    Overall Loss 1.406307    Objective Loss 1.406307                                        LR 0.000006    Time 0.083902    
2022-12-29 10:47:47,391 - Epoch: [389][   20/  113]    Overall Loss 1.368923    Objective Loss 1.368923                                        LR 0.000006    Time 0.073782    
2022-12-29 10:47:48,032 - Epoch: [389][   30/  113]    Overall Loss 1.342564    Objective Loss 1.342564                                        LR 0.000006    Time 0.070533    
2022-12-29 10:47:48,672 - Epoch: [389][   40/  113]    Overall Loss 1.341061    Objective Loss 1.341061                                        LR 0.000006    Time 0.068879    
2022-12-29 10:47:49,308 - Epoch: [389][   50/  113]    Overall Loss 1.333685    Objective Loss 1.333685                                        LR 0.000006    Time 0.067808    
2022-12-29 10:47:49,937 - Epoch: [389][   60/  113]    Overall Loss 1.346998    Objective Loss 1.346998                                        LR 0.000006    Time 0.066994    
2022-12-29 10:47:50,566 - Epoch: [389][   70/  113]    Overall Loss 1.359546    Objective Loss 1.359546                                        LR 0.000006    Time 0.066406    
2022-12-29 10:47:51,200 - Epoch: [389][   80/  113]    Overall Loss 1.346932    Objective Loss 1.346932                                        LR 0.000006    Time 0.066014    
2022-12-29 10:47:51,829 - Epoch: [389][   90/  113]    Overall Loss 1.323568    Objective Loss 1.323568                                        LR 0.000006    Time 0.065670    
2022-12-29 10:47:52,459 - Epoch: [389][  100/  113]    Overall Loss 1.321874    Objective Loss 1.321874                                        LR 0.000006    Time 0.065400    
2022-12-29 10:47:53,085 - Epoch: [389][  110/  113]    Overall Loss 1.316584    Objective Loss 1.316584                                        LR 0.000006    Time 0.065136    
2022-12-29 10:47:53,265 - Epoch: [389][  113/  113]    Overall Loss 1.320730    Objective Loss 1.320730    Top1 37.500000    Top5 87.500000    LR 0.000006    Time 0.065002    
2022-12-29 10:47:53,330 - --- validate (epoch=389)-----------
2022-12-29 10:47:53,330 - 200 samples (16 per mini-batch)
2022-12-29 10:47:53,883 - Epoch: [389][   10/   13]    Loss 1.298401    Top1 50.625000    Top5 99.375000    
2022-12-29 10:47:53,969 - Epoch: [389][   13/   13]    Loss 1.318906    Top1 49.500000    Top5 98.000000    
2022-12-29 10:47:54,036 - ==> Top1: 49.500    Top5: 98.000    Loss: 1.319

2022-12-29 10:47:54,036 - ==> Confusion:
[[17  1  0  6  0  1]
 [ 1 24  3  7  8  0]
 [ 3  5 14  7 10  0]
 [ 3  2  3 31  4  0]
 [ 3  5  2 18 12  0]
 [ 2  0  2  4  1  1]]

2022-12-29 10:47:54,040 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:47:54,040 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:47:54,061 - 

2022-12-29 10:47:54,061 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:47:54,889 - Epoch: [390][   10/  113]    Overall Loss 1.178844    Objective Loss 1.178844                                        LR 0.000006    Time 0.082690    
2022-12-29 10:47:55,530 - Epoch: [390][   20/  113]    Overall Loss 1.183507    Objective Loss 1.183507                                        LR 0.000006    Time 0.073355    
2022-12-29 10:47:56,169 - Epoch: [390][   30/  113]    Overall Loss 1.190586    Objective Loss 1.190586                                        LR 0.000006    Time 0.070187    
2022-12-29 10:47:56,806 - Epoch: [390][   40/  113]    Overall Loss 1.197165    Objective Loss 1.197165                                        LR 0.000006    Time 0.068544    
2022-12-29 10:47:57,441 - Epoch: [390][   50/  113]    Overall Loss 1.203741    Objective Loss 1.203741                                        LR 0.000006    Time 0.067532    
2022-12-29 10:47:58,072 - Epoch: [390][   60/  113]    Overall Loss 1.206443    Objective Loss 1.206443                                        LR 0.000006    Time 0.066790    
2022-12-29 10:47:58,697 - Epoch: [390][   70/  113]    Overall Loss 1.234415    Objective Loss 1.234415                                        LR 0.000006    Time 0.066173    
2022-12-29 10:47:59,327 - Epoch: [390][   80/  113]    Overall Loss 1.229506    Objective Loss 1.229506                                        LR 0.000006    Time 0.065765    
2022-12-29 10:47:59,959 - Epoch: [390][   90/  113]    Overall Loss 1.236653    Objective Loss 1.236653                                        LR 0.000006    Time 0.065472    
2022-12-29 10:48:00,590 - Epoch: [390][  100/  113]    Overall Loss 1.239885    Objective Loss 1.239885                                        LR 0.000006    Time 0.065236    
2022-12-29 10:48:01,224 - Epoch: [390][  110/  113]    Overall Loss 1.246694    Objective Loss 1.246694                                        LR 0.000006    Time 0.065058    
2022-12-29 10:48:01,394 - Epoch: [390][  113/  113]    Overall Loss 1.247384    Objective Loss 1.247384    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.064836    
2022-12-29 10:48:01,447 - --- validate (epoch=390)-----------
2022-12-29 10:48:01,449 - 200 samples (16 per mini-batch)
2022-12-29 10:48:01,995 - Epoch: [390][   10/   13]    Loss 1.218691    Top1 55.000000    Top5 99.375000    
2022-12-29 10:48:02,080 - Epoch: [390][   13/   13]    Loss 1.277575    Top1 52.500000    Top5 98.000000    
2022-12-29 10:48:02,137 - ==> Top1: 52.500    Top5: 98.000    Loss: 1.278

2022-12-29 10:48:02,138 - ==> Confusion:
[[25  1  0  3  0  0]
 [ 0 19  2 10  6  0]
 [ 3  4 13  4 10  0]
 [ 3  7  0 30  4  1]
 [ 2 13  3 13 17  0]
 [ 3  1  0  0  2  1]]

2022-12-29 10:48:02,142 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:02,143 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:02,157 - 

2022-12-29 10:48:02,157 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:02,982 - Epoch: [391][   10/  113]    Overall Loss 1.236752    Objective Loss 1.236752                                        LR 0.000006    Time 0.082425    
2022-12-29 10:48:03,626 - Epoch: [391][   20/  113]    Overall Loss 1.224890    Objective Loss 1.224890                                        LR 0.000006    Time 0.073392    
2022-12-29 10:48:04,261 - Epoch: [391][   30/  113]    Overall Loss 1.265477    Objective Loss 1.265477                                        LR 0.000006    Time 0.070074    
2022-12-29 10:48:04,893 - Epoch: [391][   40/  113]    Overall Loss 1.244497    Objective Loss 1.244497                                        LR 0.000006    Time 0.068347    
2022-12-29 10:48:05,522 - Epoch: [391][   50/  113]    Overall Loss 1.250164    Objective Loss 1.250164                                        LR 0.000006    Time 0.067248    
2022-12-29 10:48:06,155 - Epoch: [391][   60/  113]    Overall Loss 1.242095    Objective Loss 1.242095                                        LR 0.000006    Time 0.066580    
2022-12-29 10:48:06,786 - Epoch: [391][   70/  113]    Overall Loss 1.247444    Objective Loss 1.247444                                        LR 0.000006    Time 0.066071    
2022-12-29 10:48:07,420 - Epoch: [391][   80/  113]    Overall Loss 1.247036    Objective Loss 1.247036                                        LR 0.000006    Time 0.065732    
2022-12-29 10:48:08,057 - Epoch: [391][   90/  113]    Overall Loss 1.251013    Objective Loss 1.251013                                        LR 0.000006    Time 0.065502    
2022-12-29 10:48:08,690 - Epoch: [391][  100/  113]    Overall Loss 1.254547    Objective Loss 1.254547                                        LR 0.000006    Time 0.065280    
2022-12-29 10:48:09,327 - Epoch: [391][  110/  113]    Overall Loss 1.258937    Objective Loss 1.258937                                        LR 0.000006    Time 0.065125    
2022-12-29 10:48:09,496 - Epoch: [391][  113/  113]    Overall Loss 1.257011    Objective Loss 1.257011    Top1 54.166667    Top5 100.000000    LR 0.000006    Time 0.064886    
2022-12-29 10:48:09,554 - --- validate (epoch=391)-----------
2022-12-29 10:48:09,555 - 200 samples (16 per mini-batch)
2022-12-29 10:48:10,107 - Epoch: [391][   10/   13]    Loss 1.227456    Top1 53.750000    Top5 99.375000    
2022-12-29 10:48:10,192 - Epoch: [391][   13/   13]    Loss 1.198510    Top1 55.500000    Top5 98.500000    
2022-12-29 10:48:10,243 - ==> Top1: 55.500    Top5: 98.500    Loss: 1.199

2022-12-29 10:48:10,243 - ==> Confusion:
[[23  3  0  5  0  0]
 [ 2 27  3  6  6  0]
 [ 2  4  7  4  5  0]
 [ 3  4  1 43  3  0]
 [ 3  6  5  8 10  0]
 [ 2  8  1  4  1  1]]

2022-12-29 10:48:10,248 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:10,249 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:10,275 - 

2022-12-29 10:48:10,275 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:11,114 - Epoch: [392][   10/  113]    Overall Loss 1.210111    Objective Loss 1.210111                                        LR 0.000006    Time 0.083778    
2022-12-29 10:48:11,752 - Epoch: [392][   20/  113]    Overall Loss 1.231652    Objective Loss 1.231652                                        LR 0.000006    Time 0.073736    
2022-12-29 10:48:12,387 - Epoch: [392][   30/  113]    Overall Loss 1.222155    Objective Loss 1.222155                                        LR 0.000006    Time 0.070310    
2022-12-29 10:48:13,020 - Epoch: [392][   40/  113]    Overall Loss 1.230762    Objective Loss 1.230762                                        LR 0.000006    Time 0.068561    
2022-12-29 10:48:13,655 - Epoch: [392][   50/  113]    Overall Loss 1.250805    Objective Loss 1.250805                                        LR 0.000006    Time 0.067538    
2022-12-29 10:48:14,286 - Epoch: [392][   60/  113]    Overall Loss 1.270573    Objective Loss 1.270573                                        LR 0.000006    Time 0.066785    
2022-12-29 10:48:14,920 - Epoch: [392][   70/  113]    Overall Loss 1.269546    Objective Loss 1.269546                                        LR 0.000006    Time 0.066294    
2022-12-29 10:48:15,550 - Epoch: [392][   80/  113]    Overall Loss 1.261522    Objective Loss 1.261522                                        LR 0.000006    Time 0.065885    
2022-12-29 10:48:16,189 - Epoch: [392][   90/  113]    Overall Loss 1.270370    Objective Loss 1.270370                                        LR 0.000006    Time 0.065651    
2022-12-29 10:48:16,824 - Epoch: [392][  100/  113]    Overall Loss 1.259986    Objective Loss 1.259986                                        LR 0.000006    Time 0.065434    
2022-12-29 10:48:17,454 - Epoch: [392][  110/  113]    Overall Loss 1.263242    Objective Loss 1.263242                                        LR 0.000006    Time 0.065209    
2022-12-29 10:48:17,626 - Epoch: [392][  113/  113]    Overall Loss 1.267282    Objective Loss 1.267282    Top1 41.666667    Top5 95.833333    LR 0.000006    Time 0.064996    
2022-12-29 10:48:17,686 - --- validate (epoch=392)-----------
2022-12-29 10:48:17,687 - 200 samples (16 per mini-batch)
2022-12-29 10:48:18,239 - Epoch: [392][   10/   13]    Loss 1.357540    Top1 44.375000    Top5 97.500000    
2022-12-29 10:48:18,323 - Epoch: [392][   13/   13]    Loss 1.316354    Top1 45.000000    Top5 97.500000    
2022-12-29 10:48:18,379 - ==> Top1: 45.000    Top5: 97.500    Loss: 1.316

2022-12-29 10:48:18,379 - ==> Confusion:
[[18  2  1 11  3  1]
 [ 1 26  5  5  3  0]
 [ 2  6  4  4  7  0]
 [ 3  3  3 29 10  0]
 [ 2  9  5  8 12  0]
 [ 2  1  3  2  8  1]]

2022-12-29 10:48:18,382 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:18,383 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:18,394 - 

2022-12-29 10:48:18,395 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:19,223 - Epoch: [393][   10/  113]    Overall Loss 1.213038    Objective Loss 1.213038                                        LR 0.000006    Time 0.082759    
2022-12-29 10:48:19,862 - Epoch: [393][   20/  113]    Overall Loss 1.221780    Objective Loss 1.221780                                        LR 0.000006    Time 0.073302    
2022-12-29 10:48:20,498 - Epoch: [393][   30/  113]    Overall Loss 1.229537    Objective Loss 1.229537                                        LR 0.000006    Time 0.070055    
2022-12-29 10:48:21,131 - Epoch: [393][   40/  113]    Overall Loss 1.220916    Objective Loss 1.220916                                        LR 0.000006    Time 0.068331    
2022-12-29 10:48:21,762 - Epoch: [393][   50/  113]    Overall Loss 1.220759    Objective Loss 1.220759                                        LR 0.000006    Time 0.067290    
2022-12-29 10:48:22,391 - Epoch: [393][   60/  113]    Overall Loss 1.216920    Objective Loss 1.216920                                        LR 0.000006    Time 0.066545    
2022-12-29 10:48:23,019 - Epoch: [393][   70/  113]    Overall Loss 1.224043    Objective Loss 1.224043                                        LR 0.000006    Time 0.066002    
2022-12-29 10:48:23,653 - Epoch: [393][   80/  113]    Overall Loss 1.224520    Objective Loss 1.224520                                        LR 0.000006    Time 0.065672    
2022-12-29 10:48:24,284 - Epoch: [393][   90/  113]    Overall Loss 1.227607    Objective Loss 1.227607                                        LR 0.000006    Time 0.065388    
2022-12-29 10:48:24,912 - Epoch: [393][  100/  113]    Overall Loss 1.236436    Objective Loss 1.236436                                        LR 0.000006    Time 0.065120    
2022-12-29 10:48:25,539 - Epoch: [393][  110/  113]    Overall Loss 1.237005    Objective Loss 1.237005                                        LR 0.000006    Time 0.064895    
2022-12-29 10:48:25,712 - Epoch: [393][  113/  113]    Overall Loss 1.244963    Objective Loss 1.244963    Top1 41.666667    Top5 100.000000    LR 0.000006    Time 0.064699    
2022-12-29 10:48:25,769 - --- validate (epoch=393)-----------
2022-12-29 10:48:25,770 - 200 samples (16 per mini-batch)
2022-12-29 10:48:26,331 - Epoch: [393][   10/   13]    Loss 1.338288    Top1 50.000000    Top5 98.125000    
2022-12-29 10:48:26,416 - Epoch: [393][   13/   13]    Loss 1.350818    Top1 47.000000    Top5 98.500000    
2022-12-29 10:48:26,486 - ==> Top1: 47.000    Top5: 98.500    Loss: 1.351

2022-12-29 10:48:26,486 - ==> Confusion:
[[19  2  2  5  1  0]
 [ 3 16  6 11  7  0]
 [ 3  7  6  6  5  0]
 [ 4  7  1 36  3  0]
 [ 3  4  0 15 14  0]
 [ 3  2  3  1  2  3]]

2022-12-29 10:48:26,489 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:26,489 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:26,508 - 

2022-12-29 10:48:26,508 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:27,349 - Epoch: [394][   10/  113]    Overall Loss 1.213443    Objective Loss 1.213443                                        LR 0.000006    Time 0.083982    
2022-12-29 10:48:27,984 - Epoch: [394][   20/  113]    Overall Loss 1.220964    Objective Loss 1.220964                                        LR 0.000006    Time 0.073712    
2022-12-29 10:48:28,621 - Epoch: [394][   30/  113]    Overall Loss 1.223852    Objective Loss 1.223852                                        LR 0.000006    Time 0.070361    
2022-12-29 10:48:29,252 - Epoch: [394][   40/  113]    Overall Loss 1.225852    Objective Loss 1.225852                                        LR 0.000006    Time 0.068540    
2022-12-29 10:48:29,890 - Epoch: [394][   50/  113]    Overall Loss 1.216423    Objective Loss 1.216423                                        LR 0.000006    Time 0.067577    
2022-12-29 10:48:30,527 - Epoch: [394][   60/  113]    Overall Loss 1.225468    Objective Loss 1.225468                                        LR 0.000006    Time 0.066920    
2022-12-29 10:48:31,160 - Epoch: [394][   70/  113]    Overall Loss 1.237551    Objective Loss 1.237551                                        LR 0.000006    Time 0.066393    
2022-12-29 10:48:31,790 - Epoch: [394][   80/  113]    Overall Loss 1.237453    Objective Loss 1.237453                                        LR 0.000006    Time 0.065966    
2022-12-29 10:48:32,420 - Epoch: [394][   90/  113]    Overall Loss 1.225843    Objective Loss 1.225843                                        LR 0.000006    Time 0.065627    
2022-12-29 10:48:33,048 - Epoch: [394][  100/  113]    Overall Loss 1.237016    Objective Loss 1.237016                                        LR 0.000006    Time 0.065344    
2022-12-29 10:48:33,679 - Epoch: [394][  110/  113]    Overall Loss 1.241784    Objective Loss 1.241784                                        LR 0.000006    Time 0.065130    
2022-12-29 10:48:33,853 - Epoch: [394][  113/  113]    Overall Loss 1.245257    Objective Loss 1.245257    Top1 45.833333    Top5 95.833333    LR 0.000006    Time 0.064938    
2022-12-29 10:48:33,902 - --- validate (epoch=394)-----------
2022-12-29 10:48:33,903 - 200 samples (16 per mini-batch)
2022-12-29 10:48:34,453 - Epoch: [394][   10/   13]    Loss 1.242466    Top1 50.625000    Top5 96.250000    
2022-12-29 10:48:34,537 - Epoch: [394][   13/   13]    Loss 1.287258    Top1 47.000000    Top5 96.000000    
2022-12-29 10:48:34,587 - ==> Top1: 47.000    Top5: 96.000    Loss: 1.287

2022-12-29 10:48:34,588 - ==> Confusion:
[[19  0  0  5  3  0]
 [ 3 19  6 11 10  0]
 [ 1  7 11  2 12  0]
 [ 4  1  2 31  6  0]
 [ 2  9  0 10 13  0]
 [ 3  2  2  3  2  1]]

2022-12-29 10:48:34,590 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:34,590 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:34,613 - 

2022-12-29 10:48:34,613 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:35,437 - Epoch: [395][   10/  113]    Overall Loss 1.318685    Objective Loss 1.318685                                        LR 0.000006    Time 0.082258    
2022-12-29 10:48:36,074 - Epoch: [395][   20/  113]    Overall Loss 1.243191    Objective Loss 1.243191                                        LR 0.000006    Time 0.072946    
2022-12-29 10:48:36,716 - Epoch: [395][   30/  113]    Overall Loss 1.255127    Objective Loss 1.255127                                        LR 0.000006    Time 0.070018    
2022-12-29 10:48:37,351 - Epoch: [395][   40/  113]    Overall Loss 1.261602    Objective Loss 1.261602                                        LR 0.000006    Time 0.068372    
2022-12-29 10:48:37,986 - Epoch: [395][   50/  113]    Overall Loss 1.274512    Objective Loss 1.274512                                        LR 0.000006    Time 0.067380    
2022-12-29 10:48:38,620 - Epoch: [395][   60/  113]    Overall Loss 1.267767    Objective Loss 1.267767                                        LR 0.000006    Time 0.066717    
2022-12-29 10:48:39,257 - Epoch: [395][   70/  113]    Overall Loss 1.268634    Objective Loss 1.268634                                        LR 0.000006    Time 0.066282    
2022-12-29 10:48:39,893 - Epoch: [395][   80/  113]    Overall Loss 1.267089    Objective Loss 1.267089                                        LR 0.000006    Time 0.065934    
2022-12-29 10:48:40,530 - Epoch: [395][   90/  113]    Overall Loss 1.267749    Objective Loss 1.267749                                        LR 0.000006    Time 0.065684    
2022-12-29 10:48:41,165 - Epoch: [395][  100/  113]    Overall Loss 1.265386    Objective Loss 1.265386                                        LR 0.000006    Time 0.065464    
2022-12-29 10:48:41,795 - Epoch: [395][  110/  113]    Overall Loss 1.265973    Objective Loss 1.265973                                        LR 0.000006    Time 0.065236    
2022-12-29 10:48:41,967 - Epoch: [395][  113/  113]    Overall Loss 1.273532    Objective Loss 1.273532    Top1 41.666667    Top5 87.500000    LR 0.000006    Time 0.065021    
2022-12-29 10:48:42,020 - --- validate (epoch=395)-----------
2022-12-29 10:48:42,020 - 200 samples (16 per mini-batch)
2022-12-29 10:48:42,568 - Epoch: [395][   10/   13]    Loss 1.265746    Top1 50.000000    Top5 98.125000    
2022-12-29 10:48:42,653 - Epoch: [395][   13/   13]    Loss 1.244066    Top1 53.000000    Top5 98.000000    
2022-12-29 10:48:42,699 - ==> Top1: 53.000    Top5: 98.000    Loss: 1.244

2022-12-29 10:48:42,699 - ==> Confusion:
[[22  6  0  7  4  0]
 [ 4 26  1  2  6  0]
 [ 0 10 13  4  7  1]
 [ 4  3  1 28  5  0]
 [ 1 11  3  4 17  1]
 [ 0  4  0  2  3  0]]

2022-12-29 10:48:42,702 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:42,702 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:42,717 - 

2022-12-29 10:48:42,717 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:43,571 - Epoch: [396][   10/  113]    Overall Loss 1.234304    Objective Loss 1.234304                                        LR 0.000006    Time 0.085285    
2022-12-29 10:48:44,208 - Epoch: [396][   20/  113]    Overall Loss 1.243366    Objective Loss 1.243366                                        LR 0.000006    Time 0.074467    
2022-12-29 10:48:44,843 - Epoch: [396][   30/  113]    Overall Loss 1.244451    Objective Loss 1.244451                                        LR 0.000006    Time 0.070800    
2022-12-29 10:48:45,478 - Epoch: [396][   40/  113]    Overall Loss 1.279520    Objective Loss 1.279520                                        LR 0.000006    Time 0.068974    
2022-12-29 10:48:46,113 - Epoch: [396][   50/  113]    Overall Loss 1.273301    Objective Loss 1.273301                                        LR 0.000006    Time 0.067871    
2022-12-29 10:48:46,748 - Epoch: [396][   60/  113]    Overall Loss 1.273161    Objective Loss 1.273161                                        LR 0.000006    Time 0.067124    
2022-12-29 10:48:47,381 - Epoch: [396][   70/  113]    Overall Loss 1.276783    Objective Loss 1.276783                                        LR 0.000006    Time 0.066582    
2022-12-29 10:48:48,016 - Epoch: [396][   80/  113]    Overall Loss 1.278157    Objective Loss 1.278157                                        LR 0.000006    Time 0.066182    
2022-12-29 10:48:48,651 - Epoch: [396][   90/  113]    Overall Loss 1.286708    Objective Loss 1.286708                                        LR 0.000006    Time 0.065887    
2022-12-29 10:48:49,290 - Epoch: [396][  100/  113]    Overall Loss 1.285256    Objective Loss 1.285256                                        LR 0.000006    Time 0.065679    
2022-12-29 10:48:49,921 - Epoch: [396][  110/  113]    Overall Loss 1.276658    Objective Loss 1.276658                                        LR 0.000006    Time 0.065438    
2022-12-29 10:48:50,096 - Epoch: [396][  113/  113]    Overall Loss 1.271506    Objective Loss 1.271506    Top1 54.166667    Top5 91.666667    LR 0.000006    Time 0.065252    
2022-12-29 10:48:50,154 - --- validate (epoch=396)-----------
2022-12-29 10:48:50,154 - 200 samples (16 per mini-batch)
2022-12-29 10:48:50,703 - Epoch: [396][   10/   13]    Loss 1.253118    Top1 48.125000    Top5 98.125000    
2022-12-29 10:48:50,788 - Epoch: [396][   13/   13]    Loss 1.249820    Top1 46.500000    Top5 97.500000    
2022-12-29 10:48:50,832 - ==> Top1: 46.500    Top5: 97.500    Loss: 1.250

2022-12-29 10:48:50,832 - ==> Confusion:
[[27  8  0  6  1  0]
 [ 5 23  0  3  6  0]
 [ 1  4  5  4 10  0]
 [ 2  9  5 24  3  0]
 [ 5 13  5  9 14  0]
 [ 0  5  1  0  2  0]]

2022-12-29 10:48:50,835 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:50,836 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:50,857 - 

2022-12-29 10:48:50,858 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:51,689 - Epoch: [397][   10/  113]    Overall Loss 1.369898    Objective Loss 1.369898                                        LR 0.000006    Time 0.083014    
2022-12-29 10:48:52,330 - Epoch: [397][   20/  113]    Overall Loss 1.331797    Objective Loss 1.331797                                        LR 0.000006    Time 0.073512    
2022-12-29 10:48:52,962 - Epoch: [397][   30/  113]    Overall Loss 1.295532    Objective Loss 1.295532                                        LR 0.000006    Time 0.070088    
2022-12-29 10:48:53,601 - Epoch: [397][   40/  113]    Overall Loss 1.321784    Objective Loss 1.321784                                        LR 0.000006    Time 0.068517    
2022-12-29 10:48:54,231 - Epoch: [397][   50/  113]    Overall Loss 1.303435    Objective Loss 1.303435                                        LR 0.000006    Time 0.067407    
2022-12-29 10:48:54,863 - Epoch: [397][   60/  113]    Overall Loss 1.303702    Objective Loss 1.303702                                        LR 0.000006    Time 0.066699    
2022-12-29 10:48:55,497 - Epoch: [397][   70/  113]    Overall Loss 1.292453    Objective Loss 1.292453                                        LR 0.000006    Time 0.066225    
2022-12-29 10:48:56,130 - Epoch: [397][   80/  113]    Overall Loss 1.276683    Objective Loss 1.276683                                        LR 0.000006    Time 0.065846    
2022-12-29 10:48:56,767 - Epoch: [397][   90/  113]    Overall Loss 1.269843    Objective Loss 1.269843                                        LR 0.000006    Time 0.065610    
2022-12-29 10:48:57,398 - Epoch: [397][  100/  113]    Overall Loss 1.264111    Objective Loss 1.264111                                        LR 0.000006    Time 0.065344    
2022-12-29 10:48:58,020 - Epoch: [397][  110/  113]    Overall Loss 1.265424    Objective Loss 1.265424                                        LR 0.000006    Time 0.065058    
2022-12-29 10:48:58,195 - Epoch: [397][  113/  113]    Overall Loss 1.262873    Objective Loss 1.262873    Top1 54.166667    Top5 95.833333    LR 0.000006    Time 0.064877    
2022-12-29 10:48:58,245 - --- validate (epoch=397)-----------
2022-12-29 10:48:58,246 - 200 samples (16 per mini-batch)
2022-12-29 10:48:58,793 - Epoch: [397][   10/   13]    Loss 1.234050    Top1 55.000000    Top5 98.125000    
2022-12-29 10:48:58,879 - Epoch: [397][   13/   13]    Loss 1.232096    Top1 53.000000    Top5 97.500000    
2022-12-29 10:48:58,925 - ==> Top1: 53.000    Top5: 97.500    Loss: 1.232

2022-12-29 10:48:58,926 - ==> Confusion:
[[28  1  0  4  2  0]
 [ 8 17  2  7  9  0]
 [ 1  6 10  9  9  0]
 [ 4  3  2 38  3  1]
 [ 1  9  0 10 11  0]
 [ 1  0  0  1  1  2]]

2022-12-29 10:48:58,929 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:48:58,929 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:48:58,955 - 

2022-12-29 10:48:58,955 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:48:59,784 - Epoch: [398][   10/  113]    Overall Loss 1.231269    Objective Loss 1.231269                                        LR 0.000006    Time 0.082781    
2022-12-29 10:49:00,419 - Epoch: [398][   20/  113]    Overall Loss 1.213734    Objective Loss 1.213734                                        LR 0.000006    Time 0.073091    
2022-12-29 10:49:01,054 - Epoch: [398][   30/  113]    Overall Loss 1.252775    Objective Loss 1.252775                                        LR 0.000006    Time 0.069899    
2022-12-29 10:49:01,694 - Epoch: [398][   40/  113]    Overall Loss 1.219749    Objective Loss 1.219749                                        LR 0.000006    Time 0.068395    
2022-12-29 10:49:02,325 - Epoch: [398][   50/  113]    Overall Loss 1.227424    Objective Loss 1.227424                                        LR 0.000006    Time 0.067334    
2022-12-29 10:49:02,951 - Epoch: [398][   60/  113]    Overall Loss 1.243319    Objective Loss 1.243319                                        LR 0.000006    Time 0.066538    
2022-12-29 10:49:03,582 - Epoch: [398][   70/  113]    Overall Loss 1.249882    Objective Loss 1.249882                                        LR 0.000006    Time 0.066039    
2022-12-29 10:49:04,211 - Epoch: [398][   80/  113]    Overall Loss 1.244145    Objective Loss 1.244145                                        LR 0.000006    Time 0.065643    
2022-12-29 10:49:04,841 - Epoch: [398][   90/  113]    Overall Loss 1.242272    Objective Loss 1.242272                                        LR 0.000006    Time 0.065344    
2022-12-29 10:49:05,466 - Epoch: [398][  100/  113]    Overall Loss 1.256405    Objective Loss 1.256405                                        LR 0.000006    Time 0.065053    
2022-12-29 10:49:06,092 - Epoch: [398][  110/  113]    Overall Loss 1.259548    Objective Loss 1.259548                                        LR 0.000006    Time 0.064822    
2022-12-29 10:49:06,269 - Epoch: [398][  113/  113]    Overall Loss 1.265239    Objective Loss 1.265239    Top1 33.333333    Top5 100.000000    LR 0.000006    Time 0.064666    
2022-12-29 10:49:06,324 - --- validate (epoch=398)-----------
2022-12-29 10:49:06,325 - 200 samples (16 per mini-batch)
2022-12-29 10:49:06,871 - Epoch: [398][   10/   13]    Loss 1.346760    Top1 43.125000    Top5 98.750000    
2022-12-29 10:49:06,957 - Epoch: [398][   13/   13]    Loss 1.379201    Top1 43.000000    Top5 98.000000    
2022-12-29 10:49:07,005 - ==> Top1: 43.000    Top5: 98.000    Loss: 1.379

2022-12-29 10:49:07,005 - ==> Confusion:
[[21  4  0  8  2  1]
 [ 7 20  4  8  4  0]
 [ 1 17  7  4  4  0]
 [ 8  9  4 28  4  0]
 [ 0  5  4  5 10  0]
 [ 2  4  0  3  2  0]]

2022-12-29 10:49:07,009 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:49:07,010 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:49:07,030 - 

2022-12-29 10:49:07,031 - Training epoch: 1800 samples (16 per mini-batch)
2022-12-29 10:49:07,865 - Epoch: [399][   10/  113]    Overall Loss 1.282346    Objective Loss 1.282346                                        LR 0.000006    Time 0.083189    
2022-12-29 10:49:08,497 - Epoch: [399][   20/  113]    Overall Loss 1.254355    Objective Loss 1.254355                                        LR 0.000006    Time 0.073173    
2022-12-29 10:49:09,133 - Epoch: [399][   30/  113]    Overall Loss 1.271802    Objective Loss 1.271802                                        LR 0.000006    Time 0.069981    
2022-12-29 10:49:09,768 - Epoch: [399][   40/  113]    Overall Loss 1.248460    Objective Loss 1.248460                                        LR 0.000006    Time 0.068334    
2022-12-29 10:49:10,396 - Epoch: [399][   50/  113]    Overall Loss 1.257980    Objective Loss 1.257980                                        LR 0.000006    Time 0.067233    
2022-12-29 10:49:11,024 - Epoch: [399][   60/  113]    Overall Loss 1.260183    Objective Loss 1.260183                                        LR 0.000006    Time 0.066475    
2022-12-29 10:49:11,655 - Epoch: [399][   70/  113]    Overall Loss 1.265302    Objective Loss 1.265302                                        LR 0.000006    Time 0.065998    
2022-12-29 10:49:12,285 - Epoch: [399][   80/  113]    Overall Loss 1.262069    Objective Loss 1.262069                                        LR 0.000006    Time 0.065616    
2022-12-29 10:49:12,920 - Epoch: [399][   90/  113]    Overall Loss 1.276285    Objective Loss 1.276285                                        LR 0.000006    Time 0.065375    
2022-12-29 10:49:13,559 - Epoch: [399][  100/  113]    Overall Loss 1.273735    Objective Loss 1.273735                                        LR 0.000006    Time 0.065216    
2022-12-29 10:49:14,186 - Epoch: [399][  110/  113]    Overall Loss 1.274017    Objective Loss 1.274017                                        LR 0.000006    Time 0.064984    
2022-12-29 10:49:14,359 - Epoch: [399][  113/  113]    Overall Loss 1.268969    Objective Loss 1.268969    Top1 62.500000    Top5 100.000000    LR 0.000006    Time 0.064788    
2022-12-29 10:49:14,420 - --- validate (epoch=399)-----------
2022-12-29 10:49:14,421 - 200 samples (16 per mini-batch)
2022-12-29 10:49:14,972 - Epoch: [399][   10/   13]    Loss 1.237377    Top1 50.000000    Top5 95.625000    
2022-12-29 10:49:15,057 - Epoch: [399][   13/   13]    Loss 1.228443    Top1 51.000000    Top5 96.000000    
2022-12-29 10:49:15,109 - ==> Top1: 51.000    Top5: 96.000    Loss: 1.228

2022-12-29 10:49:15,110 - ==> Confusion:
[[14  4  1  5  2  0]
 [ 3 31  1  4  8  0]
 [ 1 10 18  5  7  0]
 [ 4  3  0 20  8  1]
 [ 1 13  1  6 16  0]
 [ 1  5  0  3  1  3]]

2022-12-29 10:49:15,113 - ==> Best [Top1: 59.000   Top5: 97.500   Sparsity:0.00   Params: 289216 on epoch: 305]
2022-12-29 10:49:15,113 - Saving checkpoint to: logs/2022.12.29-095525/qat_checkpoint.pth.tar
2022-12-29 10:49:15,129 - --- test ---------------------
2022-12-29 10:49:15,130 - 527 samples (16 per mini-batch)
2022-12-29 10:49:15,615 - Test: [   10/   33]    Loss 0.987474    Top1 60.625000    Top5 100.000000    
2022-12-29 10:49:15,944 - Test: [   20/   33]    Loss 0.986372    Top1 61.875000    Top5 100.000000    
2022-12-29 10:49:16,262 - Test: [   30/   33]    Loss 0.972209    Top1 61.666667    Top5 100.000000    
2022-12-29 10:49:16,356 - Test: [   33/   33]    Loss 0.979504    Top1 61.480076    Top5 100.000000    
2022-12-29 10:49:16,405 - ==> Top1: 61.480    Top5: 100.000    Loss: 0.980

2022-12-29 10:49:16,406 - ==> Confusion:
[[82  2  0  5  0  0]
 [ 2 60  6  4  9  9]
 [ 2 39 45  8 12  7]
 [15  3  2 90  0  8]
 [ 1 17  8 11 40 10]
 [ 4  5  3  6  5  7]]

2022-12-29 10:49:16,407 - 
2022-12-29 10:49:16,408 - Log file for this run: /home/lyl/MAX78000/ai8x-training/logs/2022.12.29-095525/2022.12.29-095525.log
